# ğŸ‡¹ğŸ‡· FAZ 2: TÃ¼rkÃ§e Uzman - LoRA AdaptÃ¶r #1

**Durum:** âœ… TamamlandÄ±  
**BaÅŸlangÄ±Ã§:** 2 AralÄ±k 2024  
**BitiÅŸ:** 2 AralÄ±k 2024  
**Ã–ncelik:** ğŸŸ  YÃ¼ksek  
**BaÄŸÄ±mlÄ±lÄ±k:** âœ… Faz 0 ve Faz 1 tamamlandÄ±

---

## ğŸ“Š SonuÃ§lar

### Veri Seti
- **Aya Dataset (TR):** 4046 Ã¶rnek
- **Manuel veriler:** 119 Ã¶rnek (selamlaÅŸma, kÃ¼ltÃ¼r, atasÃ¶zleri, gÃ¼nlÃ¼k sohbet)
- **Toplam:** 4147 Ã¶rnek (train: 3732, val: 415)

### EÄŸitim (V2 - Final)
- **Parametreler:** batch=4, lr=5e-5, 3000 iter, max_seq=768
- **Best Val Loss:** 1.77 (iter 1500)
- **Peak Memory:** 7GB
- **Adapter Size:** 26.6MB

### âš ï¸ Bilinen Problemler
- Base model (Qwen-2.5-3B) TÃ¼rkÃ§e'de zayÄ±f
- Tekrarlama (repetition) problemi gÃ¶rÃ¼lÃ¼yor
- BazÄ± faktÃ¼el bilgiler yanlÄ±ÅŸ olabiliyor
- Ä°lerde daha kaliteli veri ve/veya daha gÃ¼Ã§lÃ¼ base model Ã¶nerilir

---

## ğŸ¯ Faz Hedefi

Qwen-2.5-3B-Instruct base modeli Ã¼zerine TÃ¼rkÃ§e iletiÅŸim, kÃ¼ltÃ¼r ve doÄŸal sohbet yeteneklerini geliÅŸtiren bir LoRA adaptÃ¶rÃ¼ eÄŸitmek. Bu adaptÃ¶r TÃ¼rkÃ§e konuÅŸmalarda daha doÄŸal, kÃ¼ltÃ¼rel olarak uygun yanÄ±tlar Ã¼retecek.

---

## ğŸ—ï¸ Mimari Genel BakÄ±ÅŸ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     BASE MODEL (Frozen)                      â”‚
â”‚                  Qwen-2.5-3B-Instruct                        â”‚
â”‚                    (~2GB, 4-bit)                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LoRA ADAPTER (Bu Faz)                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚           adapter_tr_chat.safetensors                  â”‚  â”‚
â”‚  â”‚                  (~50-100MB)                           â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  EÄŸitim Verisi:                                        â”‚  â”‚
â”‚  â”‚  - Aya Dataset (TR)                                    â”‚  â”‚
â”‚  â”‚  - Turkish Instructions                                â”‚  â”‚
â”‚  â”‚  - Manuel TÃ¼rkÃ§e sohbet Ã¶rnekleri                      â”‚  â”‚
â”‚  â”‚                                                        â”‚  â”‚
â”‚  â”‚  LoRA Parametreleri:                                   â”‚  â”‚
â”‚  â”‚  - Rank (r): 8                                         â”‚  â”‚
â”‚  â”‚  - Alpha: 16                                           â”‚  â”‚
â”‚  â”‚  - Target: q_proj, v_proj                              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
                          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MERGED OUTPUT                            â”‚
â”‚           TÃ¼rkÃ§e konuÅŸan, kÃ¼ltÃ¼rel bilgili model             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ DetaylÄ± GÃ¶rev Listesi

### 2.1 Veri Seti AraÅŸtÄ±rma ve Ä°ndirme

#### 2.1.1 Aya Dataset (TÃ¼rkÃ§e) Ä°nceleme
- [ ] Hugging Face'de `CohereForAI/aya_dataset` incele
- [ ] TÃ¼rkÃ§e subset boyutunu Ã¶ÄŸren
- [ ] Veri formatÄ±nÄ± incele (instruction/response pairs)
- [ ] Kalite Ã¶rnekleri kontrol et
- [ ] Lisans kontrolÃ¼ yap (Apache 2.0)

#### 2.1.2 Aya Dataset Ä°ndirme
- [ ] `scripts/download_aya_tr.py` oluÅŸtur:
  ```python
  #!/usr/bin/env python3
  """Aya Dataset TÃ¼rkÃ§e subset'ini indir"""
  
  from datasets import load_dataset
  from pathlib import Path
  import json
  
  OUTPUT_DIR = Path("data/training/aya_tr")
  OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
  
  def main():
      print("ğŸ“¥ Aya Dataset yÃ¼kleniyor...")
      
      # TÃ¼m dataset'i yÃ¼kle
      dataset = load_dataset("CohereForAI/aya_dataset")
      
      # TÃ¼rkÃ§e filtrele
      tr_data = dataset.filter(lambda x: x["language"] == "Turkish")
      
      print(f"âœ… TÃ¼rkÃ§e Ã¶rnek sayÄ±sÄ±: {len(tr_data['train'])}")
      
      # JSONL formatÄ±nda kaydet
      output_file = OUTPUT_DIR / "aya_tr.jsonl"
      with open(output_file, "w", encoding="utf-8") as f:
          for item in tr_data["train"]:
              entry = {
                  "instruction": item["inputs"],
                  "input": "",
                  "output": item["targets"]
              }
              f.write(json.dumps(entry, ensure_ascii=False) + "\n")
      
      print(f"ğŸ’¾ Kaydedildi: {output_file}")
      print(f"ğŸ“Š Dosya boyutu: {output_file.stat().st_size / 1024 / 1024:.2f} MB")
  
  if __name__ == "__main__":
      main()
  ```
- [ ] Script'i Ã§alÄ±ÅŸtÄ±r
- [ ] Ä°ndirilen veri sayÄ±sÄ±nÄ± not et

#### 2.1.3 Turkish Instructions Veri Seti
- [ ] Hugging Face'de TÃ¼rkÃ§e instruction dataset'leri ara:
  - `merve/turkish_instructions`
  - `umarbutler/open-australian-legal-qa` (alternatif format)
  - DiÄŸer TÃ¼rkÃ§e QA dataset'leri
- [ ] En uygun olanlarÄ± indir
- [ ] FormatÄ± Alpaca formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r

#### 2.1.4 Manuel TÃ¼rkÃ§e Sohbet Verileri
- [ ] `data/training/manual_tr/` dizini oluÅŸtur
- [ ] AÅŸaÄŸÄ±daki kategorilerde manuel Ã¶rnekler yaz:

##### SelamlaÅŸma Ã–rnekleri (20+)
- [ ] `data/training/manual_tr/greetings.jsonl`:
  ```json
  {"instruction": "Merhaba!", "input": "", "output": "Merhaba! Size nasÄ±l yardÄ±mcÄ± olabilirim?"}
  {"instruction": "GÃ¼naydÄ±n", "input": "", "output": "GÃ¼naydÄ±n! GÃ¼zel bir gÃ¼n geÃ§irmenizi dilerim. BugÃ¼n size nasÄ±l yardÄ±mcÄ± olabilirim?"}
  {"instruction": "Ä°yi akÅŸamlar", "input": "", "output": "Ä°yi akÅŸamlar! UmarÄ±m gÃ¼nÃ¼nÃ¼z gÃ¼zel geÃ§miÅŸtir. Size nasÄ±l yardÄ±mcÄ± olabilirim?"}
  ```

##### TÃ¼rk KÃ¼ltÃ¼rÃ¼ Ã–rnekleri (30+)
- [ ] `data/training/manual_tr/culture.jsonl`:
  ```json
  {"instruction": "AtatÃ¼rk hakkÄ±nda bilgi ver", "input": "", "output": "Mustafa Kemal AtatÃ¼rk (1881-1938), TÃ¼rkiye Cumhuriyeti'nin kurucusu ve ilk CumhurbaÅŸkanÄ±'dÄ±r. KurtuluÅŸ SavaÅŸÄ±'nÄ±n Ã¶nderi olan AtatÃ¼rk, modern TÃ¼rkiye'nin temellerini atmÄ±ÅŸ ve birÃ§ok devrimle Ã¼lkeyi Ã§aÄŸdaÅŸ uygarlÄ±k seviyesine taÅŸÄ±mÄ±ÅŸtÄ±r."}
  {"instruction": "TÃ¼rk kahvesi nasÄ±l yapÄ±lÄ±r?", "input": "", "output": "TÃ¼rk kahvesi yapmak iÃ§in: 1) Bir fincan soÄŸuk su ve bir tatlÄ± kaÅŸÄ±ÄŸÄ± Ã¶ÄŸÃ¼tÃ¼lmÃ¼ÅŸ TÃ¼rk kahvesini cezveye koyun. 2) Ä°steÄŸe gÃ¶re ÅŸeker ekleyin. 3) KÄ±sÄ±k ateÅŸte, karÄ±ÅŸtÄ±rarak kÃ¶pÃ¼rene kadar Ä±sÄ±tÄ±n. 4) KÃ¶pÃ¼rmeye baÅŸlayÄ±nca ateÅŸten alÄ±n, kÃ¶pÃ¼ÄŸÃ¼ fincanlara paylaÅŸtÄ±rÄ±n. 5) Tekrar Ä±sÄ±tÄ±p servis edin. Afiyet olsun!"}
  ```

##### Deyim ve AtasÃ¶zleri (30+)
- [ ] `data/training/manual_tr/proverbs.jsonl`:
  ```json
  {"instruction": "'Damlaya damlaya gÃ¶l olur' ne demek?", "input": "", "output": "Bu atasÃ¶zÃ¼, kÃ¼Ã§Ã¼k birikimlerin zamanla bÃ¼yÃ¼k sonuÃ§lar doÄŸuracaÄŸÄ±nÄ± anlatÄ±r. SabÄ±rla ve istikrarla yapÄ±lan kÃ¼Ã§Ã¼k Ã§abalar, uzun vadede Ã¶nemli baÅŸarÄ±lara dÃ¶nÃ¼ÅŸÃ¼r. Tasarruf, Ã¶ÄŸrenme veya beceri geliÅŸtirme gibi konularda sÄ±kÃ§a kullanÄ±lÄ±r."}
  {"instruction": "'TaÅŸÄ±ma su ile deÄŸirmen dÃ¶nmez' atasÃ¶zÃ¼nÃ¼ aÃ§Ä±kla", "input": "", "output": "Bu atasÃ¶zÃ¼, dÄ±ÅŸarÄ±dan getirilen veya geÃ§ici kaynaklarla sÃ¼rdÃ¼rÃ¼lebilir bir iÅŸ yapÄ±lamayacaÄŸÄ±nÄ± anlatÄ±r. BaÅŸarÄ±lÄ± olmak iÃ§in kendi kaynaklarÄ±nÄ±zÄ±, kendi gÃ¼cÃ¼nÃ¼zÃ¼ kullanmanÄ±z gerekir. BaÅŸkalarÄ±na baÄŸÄ±mlÄ± kalarak kalÄ±cÄ± bir baÅŸarÄ± elde etmek zordur."}
  ```

##### GÃ¼nlÃ¼k KonuÅŸma (30+)
- [ ] `data/training/manual_tr/daily_chat.jsonl`:
  ```json
  {"instruction": "BugÃ¼n hava nasÄ±l?", "input": "", "output": "Maalesef gerÃ§ek zamanlÄ± hava durumu bilgisine eriÅŸimim yok. Ancak bulunduÄŸunuz ÅŸehri sÃ¶ylerseniz, genel iklim Ã¶zellikleri hakkÄ±nda bilgi verebilirim. Veya bir hava durumu uygulamasÄ± kullanmanÄ±zÄ± Ã¶neririm."}
  {"instruction": "CanÄ±m sÄ±kÄ±lÄ±yor, ne yapmalÄ±yÄ±m?", "input": "", "output": "Can sÄ±kÄ±ntÄ±sÄ±nÄ± gidermek iÃ§in birkaÃ§ Ã¶neri: 1) Yeni bir hobi deneyin (resim, mÃ¼zik, yemek yapma). 2) Bir kitap okuyun veya podcast dinleyin. 3) DÄ±ÅŸarÄ± Ã§Ä±kÄ±p yÃ¼rÃ¼yÃ¼ÅŸ yapÄ±n. 4) ArkadaÅŸlarÄ±nÄ±zla iletiÅŸime geÃ§in. 5) Yeni bir beceri Ã¶ÄŸrenin. Siz ne tÃ¼r aktivitelerden hoÅŸlanÄ±rsÄ±nÄ±z?"}
  ```

---

### 2.2 Veri Seti Temizleme ve HazÄ±rlama

#### 2.2.1 Veri Temizleme Script'i
- [ ] `scripts/clean_training_data.py` oluÅŸtur:
  ```python
  #!/usr/bin/env python3
  """EÄŸitim verisini temizle ve birleÅŸtir"""
  
  import json
  import re
  from pathlib import Path
  from typing import List, Dict
  from collections import Counter
  
  INPUT_DIRS = [
      Path("data/training/aya_tr"),
      Path("data/training/manual_tr"),
  ]
  OUTPUT_FILE = Path("data/training/tr_chat_combined.jsonl")
  
  def clean_text(text: str) -> str:
      """Metni temizle"""
      # Fazla boÅŸluklarÄ± kaldÄ±r
      text = re.sub(r'\s+', ' ', text)
      # BaÅŸta ve sondaki boÅŸluklarÄ± kaldÄ±r
      text = text.strip()
      return text
  
  def is_valid_sample(sample: Dict) -> bool:
      """Ã–rneÄŸin geÃ§erli olup olmadÄ±ÄŸÄ±nÄ± kontrol et"""
      instruction = sample.get("instruction", "")
      output = sample.get("output", "")
      
      # BoÅŸ kontrolleri
      if not instruction or not output:
          return False
      
      # Minimum uzunluk
      if len(instruction) < 5 or len(output) < 10:
          return False
      
      # Maximum uzunluk (token limiti iÃ§in)
      if len(instruction) > 2000 or len(output) > 4000:
          return False
      
      return True
  
  def remove_duplicates(samples: List[Dict]) -> List[Dict]:
      """Duplicate'larÄ± kaldÄ±r"""
      seen = set()
      unique = []
      
      for sample in samples:
          key = (sample["instruction"], sample["output"])
          if key not in seen:
              seen.add(key)
              unique.append(sample)
      
      return unique
  
  def main():
      all_samples = []
      
      for input_dir in INPUT_DIRS:
          if not input_dir.exists():
              print(f"âš ï¸ Dizin bulunamadÄ±: {input_dir}")
              continue
          
          for file in input_dir.glob("*.jsonl"):
              print(f"ğŸ“– Okunuyor: {file}")
              with open(file, "r", encoding="utf-8") as f:
                  for line in f:
                      try:
                          sample = json.loads(line)
                          sample["instruction"] = clean_text(sample.get("instruction", ""))
                          sample["input"] = clean_text(sample.get("input", ""))
                          sample["output"] = clean_text(sample.get("output", ""))
                          
                          if is_valid_sample(sample):
                              all_samples.append(sample)
                      except json.JSONDecodeError:
                          continue
      
      print(f"\nğŸ“Š Toplam Ã¶rnek (temizleme Ã¶ncesi): {len(all_samples)}")
      
      # Duplicate kaldÄ±r
      all_samples = remove_duplicates(all_samples)
      print(f"ğŸ“Š Toplam Ã¶rnek (duplicate sonrasÄ±): {len(all_samples)}")
      
      # Kaydet
      OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)
      with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
          for sample in all_samples:
              f.write(json.dumps(sample, ensure_ascii=False) + "\n")
      
      print(f"\nğŸ’¾ Kaydedildi: {OUTPUT_FILE}")
      print(f"ğŸ“Š Dosya boyutu: {OUTPUT_FILE.stat().st_size / 1024 / 1024:.2f} MB")
      
      # Ä°statistikler
      instruction_lengths = [len(s["instruction"]) for s in all_samples]
      output_lengths = [len(s["output"]) for s in all_samples]
      
      print(f"\nğŸ“ˆ Ä°statistikler:")
      print(f"   Instruction uzunluÄŸu: min={min(instruction_lengths)}, max={max(instruction_lengths)}, avg={sum(instruction_lengths)/len(instruction_lengths):.0f}")
      print(f"   Output uzunluÄŸu: min={min(output_lengths)}, max={max(output_lengths)}, avg={sum(output_lengths)/len(output_lengths):.0f}")
  
  if __name__ == "__main__":
      main()
  ```

#### 2.2.2 Alpaca Format DÃ¶nÃ¼ÅŸÃ¼mÃ¼
- [ ] TÃ¼m verilerin ÅŸu formatta olduÄŸunu doÄŸrula:
  ```json
  {
    "instruction": "KullanÄ±cÄ± talimatÄ±/sorusu",
    "input": "Opsiyonel ek baÄŸlam",
    "output": "Model yanÄ±tÄ±"
  }
  ```

#### 2.2.3 Train/Validation BÃ¶lme
- [ ] `scripts/split_dataset.py` oluÅŸtur:
  ```python
  #!/usr/bin/env python3
  """Veri setini train/val olarak bÃ¶l"""
  
  import json
  import random
  from pathlib import Path
  
  INPUT_FILE = Path("data/training/tr_chat_combined.jsonl")
  TRAIN_FILE = Path("data/training/tr_chat_train.jsonl")
  VAL_FILE = Path("data/training/tr_chat_val.jsonl")
  
  TRAIN_RATIO = 0.9
  RANDOM_SEED = 42
  
  def main():
      # Veriyi yÃ¼kle
      samples = []
      with open(INPUT_FILE, "r", encoding="utf-8") as f:
          for line in f:
              samples.append(json.loads(line))
      
      # KarÄ±ÅŸtÄ±r
      random.seed(RANDOM_SEED)
      random.shuffle(samples)
      
      # BÃ¶l
      split_idx = int(len(samples) * TRAIN_RATIO)
      train_samples = samples[:split_idx]
      val_samples = samples[split_idx:]
      
      # Kaydet
      with open(TRAIN_FILE, "w", encoding="utf-8") as f:
          for sample in train_samples:
              f.write(json.dumps(sample, ensure_ascii=False) + "\n")
      
      with open(VAL_FILE, "w", encoding="utf-8") as f:
          for sample in val_samples:
              f.write(json.dumps(sample, ensure_ascii=False) + "\n")
      
      print(f"âœ… Train: {len(train_samples)} Ã¶rnek -> {TRAIN_FILE}")
      print(f"âœ… Val: {len(val_samples)} Ã¶rnek -> {VAL_FILE}")
  
  if __name__ == "__main__":
      main()
  ```

---

### 2.3 LoRA EÄŸitim KonfigÃ¼rasyonu

#### 2.3.1 MLX LoRA Config DosyasÄ±
- [ ] `configs/lora_tr_config.yaml` oluÅŸtur:
  ```yaml
  # EVO-TR TÃ¼rkÃ§e Uzman LoRA KonfigÃ¼rasyonu
  
  # Model
  model: "./models/base/qwen-2.5-3b-instruct"
  
  # LoRA Parametreleri
  lora_parameters:
    rank: 8                    # LoRA rank (dÃ¼ÅŸÃ¼k = daha az parametre)
    alpha: 16                  # Scaling factor (genelde 2*rank)
    dropout: 0.05              # Dropout oranÄ±
    scale: 1.0                 # LoRA scale
  
  # Target ModÃ¼ller
  lora_layers: 16              # Son 16 layer'a LoRA uygula
  
  # Training Parametreleri
  training:
    batch_size: 1              # Mac M4 iÃ§in gÃ¼venli
    learning_rate: 1.0e-4      # Ã–ÄŸrenme hÄ±zÄ±
    epochs: 3                  # Epoch sayÄ±sÄ±
    warmup_steps: 100          # Warmup adÄ±mlarÄ±
    gradient_accumulation: 4   # Gradient biriktirme
    max_seq_length: 2048       # Maximum sequence uzunluÄŸu
    
  # Veri
  data:
    train: "./data/training/tr_chat_train.jsonl"
    valid: "./data/training/tr_chat_val.jsonl"
  
  # Ã‡Ä±ktÄ±
  output:
    adapter_path: "./adapters/tr_chat"
    save_every: 500            # Her N adÄ±mda kaydet
  
  # Logging
  logging:
    log_level: "INFO"
    report_to: "tensorboard"
    log_dir: "./logs/training/tr_chat"
  ```

#### 2.3.2 EÄŸitim Script'i
- [ ] `scripts/train_lora_tr.py` oluÅŸtur:
  ```python
  #!/usr/bin/env python3
  """
  EVO-TR TÃ¼rkÃ§e LoRA EÄŸitim Script'i
  
  Mac Mini M4 iÃ§in optimize edilmiÅŸ MLX-LM tabanlÄ± eÄŸitim.
  """
  
  import argparse
  import json
  import yaml
  from pathlib import Path
  from datetime import datetime
  
  import mlx.core as mx
  from mlx_lm import load, generate
  from mlx_lm.tuner import train as lora_train
  from mlx_lm.tuner.trainer import TrainingArgs
  from mlx_lm.tuner.datasets import Dataset
  
  
  def load_config(config_path: str) -> dict:
      """YAML config dosyasÄ±nÄ± yÃ¼kle"""
      with open(config_path, "r") as f:
          return yaml.safe_load(f)
  
  
  def prepare_dataset(data_path: str) -> list:
      """JSONL veri setini yÃ¼kle"""
      samples = []
      with open(data_path, "r", encoding="utf-8") as f:
          for line in f:
              item = json.loads(line)
              # Qwen chat formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
              if item.get("input"):
                  prompt = f"{item['instruction']}\n\n{item['input']}"
              else:
                  prompt = item["instruction"]
              
              samples.append({
                  "prompt": prompt,
                  "response": item["output"]
              })
      return samples
  
  
  def main():
      parser = argparse.ArgumentParser(description="TÃ¼rkÃ§e LoRA EÄŸitimi")
      parser.add_argument(
          "--config", 
          type=str, 
          default="configs/lora_tr_config.yaml",
          help="KonfigÃ¼rasyon dosyasÄ±"
      )
      parser.add_argument(
          "--resume",
          type=str,
          default=None,
          help="Checkpoint'tan devam et"
      )
      args = parser.parse_args()
      
      # Config yÃ¼kle
      print(f"ğŸ“– Config yÃ¼kleniyor: {args.config}")
      config = load_config(args.config)
      
      # Device kontrolÃ¼
      print(f"ğŸ–¥ï¸ Device: {mx.default_device()}")
      
      # Model yÃ¼kle
      print(f"ğŸ¤– Model yÃ¼kleniyor: {config['model']}")
      model, tokenizer = load(config["model"])
      
      # Veri yÃ¼kle
      print(f"ğŸ“š EÄŸitim verisi yÃ¼kleniyor...")
      train_data = prepare_dataset(config["data"]["train"])
      val_data = prepare_dataset(config["data"]["valid"])
      print(f"   Train: {len(train_data)} Ã¶rnek")
      print(f"   Val: {len(val_data)} Ã¶rnek")
      
      # EÄŸitim argÃ¼manlarÄ±
      training_args = TrainingArgs(
          batch_size=config["training"]["batch_size"],
          iters=len(train_data) * config["training"]["epochs"],
          learning_rate=config["training"]["learning_rate"],
          steps_per_report=50,
          steps_per_eval=config["output"]["save_every"],
          adapter_path=config["output"]["adapter_path"],
          lora_layers=config.get("lora_layers", 16),
          lora_rank=config["lora_parameters"]["rank"],
          lora_scale=config["lora_parameters"]["scale"],
      )
      
      # Output dizinini oluÅŸtur
      Path(config["output"]["adapter_path"]).mkdir(parents=True, exist_ok=True)
      
      # EÄŸitimi baÅŸlat
      print(f"\nğŸš€ EÄŸitim baÅŸlÄ±yor...")
      print(f"   Epochs: {config['training']['epochs']}")
      print(f"   Batch size: {config['training']['batch_size']}")
      print(f"   Learning rate: {config['training']['learning_rate']}")
      print(f"   LoRA rank: {config['lora_parameters']['rank']}")
      
      start_time = datetime.now()
      
      lora_train(
          model=model,
          tokenizer=tokenizer,
          args=training_args,
          train_dataset=train_data,
          val_dataset=val_data,
      )
      
      elapsed = datetime.now() - start_time
      print(f"\nâœ… EÄŸitim tamamlandÄ±! SÃ¼re: {elapsed}")
      print(f"ğŸ’¾ Adapter kaydedildi: {config['output']['adapter_path']}")
  
  
  if __name__ == "__main__":
      main()
  ```

---

### 2.4 LoRA EÄŸitimini Ã‡alÄ±ÅŸtÄ±rma

#### 2.4.1 EÄŸitim Ã–ncesi Kontroller
- [ ] Yeterli disk alanÄ± var mÄ±? (en az 5GB boÅŸ)
- [ ] Activity Monitor'da bellek durumu uygun mu?
- [ ] Gereksiz uygulamalarÄ± kapat
- [ ] MacBook'taysan ÅŸarjda olduÄŸundan emin ol

#### 2.4.2 EÄŸitimi BaÅŸlatma
- [ ] Terminal'de virtual environment aktif et:
  ```bash
  source .venv/bin/activate
  cd /Users/kaan/Desktop/Kaan/Personal/agÄ±-llm
  ```
- [ ] EÄŸitimi baÅŸlat:
  ```bash
  python scripts/train_lora_tr.py --config configs/lora_tr_config.yaml
  ```
- [ ] Loss deÄŸerlerini takip et
- [ ] Ä°lerlemeyi not et

#### 2.4.3 EÄŸitim SÄ±rasÄ±nda Ä°zleme
- [ ] Loss'un dÃ¼ÅŸtÃ¼ÄŸÃ¼nÃ¼ doÄŸrula
- [ ] Overfitting belirtilerine dikkat et (val_loss artarken train_loss dÃ¼ÅŸÃ¼yorsa)
- [ ] Memory kullanÄ±mÄ±nÄ± izle (Activity Monitor)
- [ ] Checkpoint'lerin kaydedildiÄŸini kontrol et

#### 2.4.4 EÄŸitim TamamlandÄ±ÄŸÄ±nda
- [ ] Final adapter dosyalarÄ±nÄ± kontrol et:
  ```bash
  ls -la adapters/tr_chat/
  ```
- [ ] Beklenen dosyalar:
  - `adapter_config.json`
  - `adapters.safetensors`
- [ ] Dosya boyutunu not et (~50-100MB)

---

### 2.5 Adapter Test ve DeÄŸerlendirme

#### 2.5.1 HÄ±zlÄ± Test Script'i
- [ ] `scripts/test_adapter_tr.py` oluÅŸtur:
  ```python
  #!/usr/bin/env python3
  """TÃ¼rkÃ§e adapter hÄ±zlÄ± test"""
  
  from mlx_lm import load, generate
  from rich.console import Console
  from rich.panel import Panel
  
  console = Console()
  
  MODEL_PATH = "./models/base/qwen-2.5-3b-instruct"
  ADAPTER_PATH = "./adapters/tr_chat"
  
  TEST_PROMPTS = [
      "Merhaba! NasÄ±lsÄ±n?",
      "TÃ¼rk kahvesi nasÄ±l yapÄ±lÄ±r?",
      "'Damlaya damlaya gÃ¶l olur' ne demek?",
      "AtatÃ¼rk hakkÄ±nda kÄ±sa bilgi ver.",
      "Bana bir TÃ¼rk atasÃ¶zÃ¼ sÃ¶yle ve anlamÄ±nÄ± aÃ§Ä±kla.",
      "Ä°stanbul'un tarihi Ã¶nemi nedir?",
      "Ramazan ayÄ±nda neler yapÄ±lÄ±r?",
      "TÃ¼rk misafirperverliÄŸi hakkÄ±nda ne sÃ¶ylersin?",
  ]
  
  def main():
      console.print("\n[bold blue]ğŸ§ª TÃ¼rkÃ§e Adapter Testi[/bold blue]\n")
      
      # Base model yÃ¼kle
      console.print("ğŸ“¥ Model yÃ¼kleniyor (base)...")
      base_model, tokenizer = load(MODEL_PATH)
      
      # Adapter ile yÃ¼kle
      console.print("ğŸ“¥ Model yÃ¼kleniyor (adapter)...")
      adapter_model, _ = load(MODEL_PATH, adapter_path=ADAPTER_PATH)
      
      for prompt in TEST_PROMPTS:
          console.print(Panel(f"[cyan]Prompt:[/cyan] {prompt}", expand=False))
          
          # Base model yanÄ±tÄ±
          console.print("\n[yellow]Base Model:[/yellow]")
          base_response = generate(
              base_model, tokenizer, 
              prompt=prompt, 
              max_tokens=150,
              verbose=False
          )
          console.print(base_response)
          
          # Adapter yanÄ±tÄ±
          console.print("\n[green]Adapter (TR):[/green]")
          adapter_response = generate(
              adapter_model, tokenizer, 
              prompt=prompt, 
              max_tokens=150,
              verbose=False
          )
          console.print(adapter_response)
          
          console.print("\n" + "="*60 + "\n")
          
          input("Enter'a basarak devam et...")
  
  if __name__ == "__main__":
      main()
  ```

#### 2.5.2 KarÅŸÄ±laÅŸtÄ±rmalÄ± Test
- [ ] Her test prompt iÃ§in:
  - Base model yanÄ±tÄ±nÄ± not et
  - Adapter yanÄ±tÄ±nÄ± not et
  - Hangisinin daha iyi olduÄŸunu deÄŸerlendir
- [ ] DeÄŸerlendirme kriterleri:
  - [ ] TÃ¼rkÃ§e dilbilgisi doÄŸruluÄŸu
  - [ ] KÃ¼ltÃ¼rel uygunluk
  - [ ] DoÄŸallÄ±k
  - [ ] Bilgi doÄŸruluÄŸu

#### 2.5.3 Nicel DeÄŸerlendirme
- [ ] Perplexity hesapla (dÃ¼ÅŸÃ¼k = daha iyi)
- [ ] TÃ¼rkÃ§e benchmark varsa kullan
- [ ] Manuel puanlama (1-5 Ã¶lÃ§eÄŸi)

---

### 2.6 Adapter Optimizasyonu (Opsiyonel)

#### 2.6.1 Hyperparameter Tuning
- [ ] EÄŸer sonuÃ§lar yetersizse, ÅŸu parametreleri deÄŸiÅŸtirerek tekrar dene:
  - [ ] `rank`: 8 -> 16
  - [ ] `learning_rate`: 1e-4 -> 5e-5 veya 2e-4
  - [ ] `epochs`: 3 -> 5

#### 2.6.2 Veri ArtÄ±rma
- [ ] Daha fazla manuel Ã¶rnek ekle
- [ ] Parafraz ile veri Ã§oÄŸaltma dene
- [ ] DÃ¼ÅŸÃ¼k kaliteli Ã¶rnekleri temizle

#### 2.6.3 Final Adapter Kaydetme
- [ ] En iyi checkpoint'Ä± seÃ§
- [ ] `adapters/tr_chat/` dizinine kopyala
- [ ] Metadata dosyasÄ± oluÅŸtur:
  ```json
  {
    "name": "adapter_tr_chat",
    "version": "1.0",
    "created": "2024-12-02",
    "base_model": "Qwen/Qwen2.5-3B-Instruct",
    "training_samples": 5000,
    "epochs": 3,
    "lora_rank": 8
  }
  ```

---

## âœ… Faz Tamamlanma Kriterleri

Bu faz tamamlanmÄ±ÅŸ sayÄ±lmasÄ± iÃ§in:

1. [ ] EÄŸitim verisi hazÄ±r (3000+ Ã¶rnek)
2. [ ] `data/training/tr_chat_train.jsonl` oluÅŸturuldu
3. [ ] LoRA eÄŸitimi tamamlandÄ±
4. [ ] `adapters/tr_chat/` dizininde adapter var
5. [ ] Base vs Adapter karÅŸÄ±laÅŸtÄ±rmasÄ± yapÄ±ldÄ±
6. [ ] TÃ¼rkÃ§e yanÄ±tlarda gÃ¶zle gÃ¶rÃ¼lÃ¼r iyileÅŸme var
7. [ ] Adapter boyutu makul (<200MB)

---

## â­ï¸ Sonraki Faz

Faz 2 tamamlandÄ±ktan sonra â†’ **FAZ-3-PYTHON-UZMAN.md** dosyasÄ±na geÃ§.

---

## ğŸ› OlasÄ± Sorunlar ve Ã‡Ã¶zÃ¼mleri

### Out of Memory HatasÄ±
```
RuntimeError: MPS backend out of memory
```
**Ã‡Ã¶zÃ¼m:**
- `batch_size: 1` yap
- `max_seq_length: 1024` dÃ¼ÅŸÃ¼r
- Gereksiz uygulamalarÄ± kapat
- `gradient_accumulation` artÄ±r

### Loss DÃ¼ÅŸmÃ¼yor
**Ã‡Ã¶zÃ¼m:**
- Learning rate'i artÄ±r (2e-4)
- Daha fazla epoch dene
- Veri kalitesini kontrol et

### Overfitting
**Ã‡Ã¶zÃ¼m:**
- Early stopping uygula
- Dropout artÄ±r (0.1)
- Daha fazla veri ekle

### Adapter YÃ¼klenmiyor
**Ã‡Ã¶zÃ¼m:**
- Dosya yollarÄ±nÄ± kontrol et
- `adapter_config.json` formatÄ±nÄ± kontrol et
- Model versiyonu uyumluluÄŸunu kontrol et

---

## ğŸ“Š Zaman Takibi

| GÃ¶rev | BaÅŸlangÄ±Ã§ | BitiÅŸ | SÃ¼re |
|-------|-----------|-------|------|
| 2.1 Veri Ä°ndirme | | | |
| 2.2 Veri Temizleme | | | |
| 2.3 Config HazÄ±rlama | | | |
| 2.4 EÄŸitim | | | |
| 2.5 Test | | | |
| 2.6 Optimizasyon | | | |
| **TOPLAM** | | | |

---

*Bu faz tamamlandÄ±ÄŸÄ±nda, "âœ… FAZ 2 TAMAMLANDI" olarak iÅŸaretle.*
