# ğŸ”— FAZ 5: Sistem Entegrasyonu (The Orchestration)

**Durum:** â¬œ BaÅŸlanmadÄ±  
**Tahmini SÃ¼re:** 2-3 gÃ¼n  
**Ã–ncelik:** ğŸŸ  YÃ¼ksek  
**BaÄŸÄ±mlÄ±lÄ±k:** Faz 0, 1, 2, 3, 4 tamamlanmÄ±ÅŸ olmalÄ±

---

## ğŸ¯ Faz Hedefi

TÃ¼m bileÅŸenleri (Router, LoRA AdaptÃ¶rleri, HafÄ±za Sistemi) birleÅŸtirerek Ã§alÄ±ÅŸan bir uÃ§tan uca sistem oluÅŸturmak. KullanÄ±cÄ± mesajÄ± girdiÄŸinde otomatik olarak doÄŸru adaptÃ¶re yÃ¶nlendirilecek ve hafÄ±za ile zenginleÅŸtirilmiÅŸ yanÄ±t Ã¼retilecek.

---

## ğŸ—ï¸ Mimari Genel BakÄ±ÅŸ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           EVO-TR ORCHESTRATOR                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  USER INPUT     â”‚â”€â”€â”€â–¶â”‚     ROUTER      â”‚â”€â”€â”€â–¶â”‚   ADAPTER SELECTION     â”‚  â”‚
â”‚  â”‚                 â”‚    â”‚  (Intent Class) â”‚    â”‚                         â”‚  â”‚
â”‚  â”‚  "Python'da    â”‚    â”‚                 â”‚    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚   liste nasÄ±l  â”‚    â”‚  Intent:        â”‚    â”‚  â”‚ adapter_tr_chat     â”‚ â”‚  â”‚
â”‚  â”‚   sÄ±ralarÄ±m?"  â”‚    â”‚  code_python    â”‚    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚  â”‚
â”‚  â”‚                 â”‚    â”‚  Confidence:   â”‚â”€â”€â”€â–¶â”‚  â”‚ adapter_python_coderâ”‚â—€â”‚  â”‚
â”‚  â”‚                 â”‚    â”‚  0.94          â”‚    â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚ base_model          â”‚ â”‚  â”‚
â”‚                                                â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚                                                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                           â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                        INFERENCE ENGINE                                 â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚â”‚
â”‚  â”‚  â”‚                    MLX-LM Server                                  â”‚  â”‚â”‚
â”‚  â”‚  â”‚                                                                   â”‚  â”‚â”‚
â”‚  â”‚  â”‚  Base Model: Qwen-2.5-3B-Instruct                                â”‚  â”‚â”‚
â”‚  â”‚  â”‚  Active Adapter: adapter_python_coder                            â”‚  â”‚â”‚
â”‚  â”‚  â”‚  Device: MPS (Apple Silicon)                                     â”‚  â”‚â”‚
â”‚  â”‚  â”‚                                                                   â”‚  â”‚â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚â”‚
â”‚  â”‚  â”‚  â”‚ Augmented Prompt:                                           â”‚ â”‚  â”‚â”‚
â”‚  â”‚  â”‚  â”‚                                                             â”‚ â”‚  â”‚â”‚
â”‚  â”‚  â”‚  â”‚ [HafÄ±zadan]: KullanÄ±cÄ± Python projesi yapÄ±yor              â”‚ â”‚  â”‚â”‚
â”‚  â”‚  â”‚  â”‚ [GeÃ§miÅŸ]: Son 3 soru da kod ile ilgiliydi                  â”‚ â”‚  â”‚â”‚
â”‚  â”‚  â”‚  â”‚ [Soru]: Python'da liste nasÄ±l sÄ±ralarÄ±m?                   â”‚ â”‚  â”‚â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                           â”‚                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                         RESPONSE + LOGGING                              â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚  Response: "Python'da listeyi sÄ±ralamak iÃ§in sorted() veya .sort()..." â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚â”‚
â”‚  â”‚  â”‚ Context Buffer   â”‚  â”‚ ChromaDB Memory  â”‚  â”‚ Log File         â”‚       â”‚â”‚
â”‚  â”‚  â”‚ (GÃ¼ncel mesaj    â”‚  â”‚ (Ã–nemli bilgiler â”‚  â”‚ (TÃ¼m sohbet      â”‚       â”‚â”‚
â”‚  â”‚  â”‚  eklendi)        â”‚  â”‚  kaydedildi)     â”‚  â”‚  kaydedildi)     â”‚       â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ DetaylÄ± GÃ¶rev Listesi

### 5.1 LoRA Manager GeliÅŸtirme

#### 5.1.1 Adapter Registry
- [ ] `configs/adapters.json` oluÅŸtur:
  ```json
  {
    "adapters": {
      "adapter_tr_chat": {
        "path": "./adapters/tr_chat",
        "description": "TÃ¼rkÃ§e sohbet ve kÃ¼ltÃ¼r uzmanÄ±",
        "intents": ["general_chat", "turkish_culture"],
        "priority": 1
      },
      "adapter_python_coder": {
        "path": "./adapters/python_coder",
        "description": "Python programlama uzmanÄ±",
        "intents": ["code_python", "code_debug", "code_explain"],
        "priority": 1
      },
      "base_model": {
        "path": null,
        "description": "Temel model (adapter yok)",
        "intents": ["general_knowledge", "memory_recall"],
        "priority": 0
      }
    },
    "default_adapter": "base_model",
    "cache_adapters": true,
    "max_cached_adapters": 2
  }
  ```

#### 5.1.2 LoRA Manager Class
- [ ] `src/experts/lora_manager.py` oluÅŸtur:
  ```python
  """
  EVO-TR LoRA YÃ¶neticisi
  
  AdaptÃ¶rlerin yÃ¼klenmesi, deÄŸiÅŸtirilmesi ve cache yÃ¶netimi.
  """
  
  import json
  from pathlib import Path
  from typing import Dict, Optional, Tuple, Any
  from collections import OrderedDict
  import time
  
  import mlx.core as mx
  from mlx_lm import load
  
  
  class LoRAManager:
      """LoRA adaptÃ¶r yÃ¶neticisi"""
      
      def __init__(
          self,
          base_model_path: str = "./models/base/qwen-2.5-3b-instruct",
          adapters_config_path: str = "./configs/adapters.json",
          max_cache_size: int = 2
      ):
          self.base_model_path = base_model_path
          self.max_cache_size = max_cache_size
          
          # Config yÃ¼kle
          with open(adapters_config_path, "r") as f:
              self.config = json.load(f)
          
          # Adapter cache (LRU-style)
          self._adapter_cache: OrderedDict = OrderedDict()
          
          # Base model ve tokenizer (her zaman yÃ¼klÃ¼)
          self._base_model = None
          self._tokenizer = None
          self._current_adapter: Optional[str] = None
          
          print(f"âœ… LoRAManager baÅŸlatÄ±ldÄ±")
          print(f"   KayÄ±tlÄ± adapter sayÄ±sÄ±: {len(self.config['adapters'])}")
      
      def _ensure_base_model_loaded(self) -> None:
          """Base model'in yÃ¼klÃ¼ olduÄŸundan emin ol"""
          if self._base_model is None:
              print(f"ğŸ“¥ Base model yÃ¼kleniyor: {self.base_model_path}")
              start = time.time()
              self._base_model, self._tokenizer = load(self.base_model_path)
              elapsed = time.time() - start
              print(f"âœ… Base model yÃ¼klendi ({elapsed:.1f}s)")
      
      def get_model(
          self, 
          adapter_id: str
      ) -> Tuple[Any, Any, str]:
          """
          Belirtilen adapter ile model dÃ¶ndÃ¼r.
          
          Args:
              adapter_id: Adapter ID (Ã¶rn: "adapter_python_coder")
              
          Returns:
              (model, tokenizer, adapter_id)
          """
          self._ensure_base_model_loaded()
          
          # Base model isteniyorsa direkt dÃ¶ndÃ¼r
          if adapter_id == "base_model" or adapter_id is None:
              self._current_adapter = None
              return self._base_model, self._tokenizer, "base_model"
          
          # Adapter config kontrolÃ¼
          if adapter_id not in self.config["adapters"]:
              print(f"âš ï¸ Bilinmeyen adapter: {adapter_id}, base_model kullanÄ±lÄ±yor")
              return self._base_model, self._tokenizer, "base_model"
          
          adapter_config = self.config["adapters"][adapter_id]
          adapter_path = adapter_config.get("path")
          
          if not adapter_path:
              return self._base_model, self._tokenizer, "base_model"
          
          # Cache'de var mÄ±?
          if adapter_id in self._adapter_cache:
              # LRU: En son eriÅŸileni sona taÅŸÄ±
              self._adapter_cache.move_to_end(adapter_id)
              model = self._adapter_cache[adapter_id]
              self._current_adapter = adapter_id
              return model, self._tokenizer, adapter_id
          
          # Cache'de yok, yÃ¼kle
          print(f"ğŸ“¥ Adapter yÃ¼kleniyor: {adapter_id}")
          start = time.time()
          
          model, _ = load(
              self.base_model_path,
              adapter_path=adapter_path
          )
          
          elapsed = time.time() - start
          print(f"âœ… Adapter yÃ¼klendi ({elapsed:.1f}s)")
          
          # Cache'e ekle
          self._adapter_cache[adapter_id] = model
          
          # Cache boyutu aÅŸÄ±ldÄ±ysa en eskiyi kaldÄ±r
          while len(self._adapter_cache) > self.max_cache_size:
              oldest = next(iter(self._adapter_cache))
              del self._adapter_cache[oldest]
              print(f"ğŸ—‘ï¸ Cache'den kaldÄ±rÄ±ldÄ±: {oldest}")
          
          self._current_adapter = adapter_id
          return model, self._tokenizer, adapter_id
      
      def get_adapter_for_intent(self, intent: str) -> str:
          """Intent'e gÃ¶re adapter ID dÃ¶ndÃ¼r"""
          for adapter_id, config in self.config["adapters"].items():
              if intent in config.get("intents", []):
                  return adapter_id
          
          return self.config.get("default_adapter", "base_model")
      
      def list_adapters(self) -> Dict:
          """TÃ¼m adaptÃ¶rleri listele"""
          return {
              adapter_id: {
                  "description": config.get("description"),
                  "intents": config.get("intents", []),
                  "cached": adapter_id in self._adapter_cache
              }
              for adapter_id, config in self.config["adapters"].items()
          }
      
      def get_current_adapter(self) -> Optional[str]:
          """Åu an aktif adapter'Ä± dÃ¶ndÃ¼r"""
          return self._current_adapter
      
      def clear_cache(self) -> int:
          """Cache'i temizle"""
          count = len(self._adapter_cache)
          self._adapter_cache.clear()
          return count
      
      def get_stats(self) -> Dict:
          """Manager istatistikleri"""
          return {
              "base_model_loaded": self._base_model is not None,
              "current_adapter": self._current_adapter,
              "cached_adapters": list(self._adapter_cache.keys()),
              "cache_size": len(self._adapter_cache),
              "max_cache_size": self.max_cache_size
          }
  
  
  # Singleton
  _lora_manager: Optional[LoRAManager] = None
  
  
  def get_lora_manager() -> LoRAManager:
      """Global LoRAManager instance"""
      global _lora_manager
      if _lora_manager is None:
          _lora_manager = LoRAManager()
      return _lora_manager
  ```

---

### 5.2 Inference Engine GeliÅŸtirme

#### 5.2.1 MLX Inference Wrapper
- [ ] `src/inference/mlx_inference.py` oluÅŸtur:
  ```python
  """
  EVO-TR Inference Engine
  
  MLX-LM tabanlÄ± text generation.
  """
  
  from typing import Optional, Generator, Dict, Any
  from dataclasses import dataclass
  import time
  
  from mlx_lm import generate
  from mlx_lm.utils import generate_step
  
  
  @dataclass
  class GenerationConfig:
      """Generation parametreleri"""
      max_tokens: int = 512
      temperature: float = 0.7
      top_p: float = 0.9
      repetition_penalty: float = 1.1
      stop_tokens: tuple = ("<|im_end|>", "\n\n\n")
  
  
  class InferenceEngine:
      """MLX tabanlÄ± inference engine"""
      
      def __init__(self, default_config: Optional[GenerationConfig] = None):
          self.default_config = default_config or GenerationConfig()
      
      def generate(
          self,
          model: Any,
          tokenizer: Any,
          prompt: str,
          config: Optional[GenerationConfig] = None,
          stream: bool = False
      ) -> str | Generator[str, None, None]:
          """
          Text generation.
          
          Args:
              model: MLX model
              tokenizer: Tokenizer
              prompt: Input prompt
              config: Generation config
              stream: Streaming output
              
          Returns:
              Generated text or generator
          """
          cfg = config or self.default_config
          
          if stream:
              return self._generate_stream(model, tokenizer, prompt, cfg)
          else:
              return self._generate_full(model, tokenizer, prompt, cfg)
      
      def _generate_full(
          self,
          model: Any,
          tokenizer: Any,
          prompt: str,
          config: GenerationConfig
      ) -> str:
          """Tam yanÄ±t Ã¼ret"""
          start_time = time.time()
          
          response = generate(
              model,
              tokenizer,
              prompt=prompt,
              max_tokens=config.max_tokens,
              temp=config.temperature,
              top_p=config.top_p,
              repetition_penalty=config.repetition_penalty,
              verbose=False
          )
          
          # Stop token'larÄ± temizle
          for stop in config.stop_tokens:
              if stop in response:
                  response = response.split(stop)[0]
          
          elapsed = time.time() - start_time
          tokens = len(tokenizer.encode(response))
          
          # Debug bilgisi
          print(f"   â±ï¸ {elapsed:.2f}s, {tokens} tokens, {tokens/elapsed:.1f} t/s")
          
          return response.strip()
      
      def _generate_stream(
          self,
          model: Any,
          tokenizer: Any,
          prompt: str,
          config: GenerationConfig
      ) -> Generator[str, None, None]:
          """Streaming generation"""
          # Token encode
          input_ids = tokenizer.encode(prompt, return_tensors="np")
          
          generated_tokens = []
          
          for token in generate_step(
              model,
              input_ids,
              max_tokens=config.max_tokens,
              temp=config.temperature,
              top_p=config.top_p
          ):
              generated_tokens.append(token)
              
              # Token'Ä± decode et
              text = tokenizer.decode(generated_tokens)
              
              # Stop token kontrolÃ¼
              should_stop = False
              for stop in config.stop_tokens:
                  if stop in text:
                      text = text.split(stop)[0]
                      should_stop = True
                      break
              
              yield text
              
              if should_stop:
                  break
      
      def format_chat_prompt(
          self,
          user_message: str,
          system_prompt: Optional[str] = None,
          context: Optional[str] = None
      ) -> str:
          """
          Qwen chat formatÄ±nda prompt oluÅŸtur.
          
          Args:
              user_message: KullanÄ±cÄ± mesajÄ±
              system_prompt: System prompt
              context: Ek context (hafÄ±za, geÃ§miÅŸ)
          """
          parts = []
          
          # System prompt
          if system_prompt:
              parts.append(f"<|im_start|>system\n{system_prompt}<|im_end|>")
          
          # Context (hafÄ±zadan gelen)
          if context:
              parts.append(f"<|im_start|>system\n{context}<|im_end|>")
          
          # User message
          parts.append(f"<|im_start|>user\n{user_message}<|im_end|>")
          
          # Assistant prefix
          parts.append("<|im_start|>assistant\n")
          
          return "\n".join(parts)
  
  
  # Singleton
  _inference_engine: Optional[InferenceEngine] = None
  
  
  def get_inference_engine() -> InferenceEngine:
      """Global InferenceEngine instance"""
      global _inference_engine
      if _inference_engine is None:
          _inference_engine = InferenceEngine()
      return _inference_engine
  ```

---

### 5.3 Ana Orchestrator GeliÅŸtirme

#### 5.3.1 Orchestrator Class
- [ ] `src/orchestrator.py` oluÅŸtur:
  ```python
  """
  EVO-TR Orchestrator
  
  TÃ¼m bileÅŸenleri birleÅŸtiren ana orkestrasyon katmanÄ±.
  """
  
  from typing import Optional, Dict, Any
  from dataclasses import dataclass
  from datetime import datetime
  import json
  
  from .router.classifier import get_classifier
  from .experts.lora_manager import get_lora_manager
  from .memory.rag_pipeline import get_rag_pipeline
  from .inference.mlx_inference import get_inference_engine, GenerationConfig
  
  
  @dataclass
  class Response:
      """Orchestrator yanÄ±tÄ±"""
      text: str
      intent: str
      confidence: float
      adapter_used: str
      processing_time: float
      memory_results: int
  
  
  class Orchestrator:
      """
      EVO-TR Ana OrkestratÃ¶rÃ¼
      
      KullanÄ±cÄ± mesajÄ±nÄ± alÄ±r, router ile yÃ¶nlendirir,
      hafÄ±zadan context alÄ±r, uygun adapter ile yanÄ±t Ã¼retir.
      """
      
      DEFAULT_SYSTEM_PROMPT = """Sen EVO-TR, modÃ¼ler ve sÃ¼rekli Ã¶ÄŸrenen bir yapay zeka asistanÄ±sÄ±n.
  TÃ¼rkÃ§e konuÅŸuyorsun ve kullanÄ±cÄ±ya yardÄ±mcÄ± olmak iÃ§in eÄŸitildin.
  YanÄ±tlarÄ±n doÄŸal, samimi ve bilgilendirici olmalÄ±."""
      
      def __init__(
          self,
          system_prompt: Optional[str] = None,
          generation_config: Optional[GenerationConfig] = None,
          use_memory: bool = True,
          use_router: bool = True
      ):
          self.system_prompt = system_prompt or self.DEFAULT_SYSTEM_PROMPT
          self.generation_config = generation_config or GenerationConfig()
          self.use_memory = use_memory
          self.use_router = use_router
          
          # BileÅŸenler
          self.router = get_classifier() if use_router else None
          self.lora_manager = get_lora_manager()
          self.rag = get_rag_pipeline() if use_memory else None
          self.inference = get_inference_engine()
          
          print("âœ… Orchestrator baÅŸlatÄ±ldÄ±")
      
      def chat(
          self,
          user_message: str,
          force_adapter: Optional[str] = None,
          save_to_memory: bool = True
      ) -> Response:
          """
          KullanÄ±cÄ± mesajÄ±na yanÄ±t Ã¼ret.
          
          Args:
              user_message: KullanÄ±cÄ± mesajÄ±
              force_adapter: Zorla belirli adapter kullan
              save_to_memory: HafÄ±zaya kaydet
              
          Returns:
              Response objesi
          """
          import time
          start_time = time.time()
          
          # 1. Router ile intent belirleme
          if force_adapter:
              intent = "forced"
              confidence = 1.0
              adapter_id = force_adapter
          elif self.use_router:
              route_result = self.router.predict(user_message)
              intent = route_result["intent"]
              confidence = route_result["confidence"]
              adapter_id = route_result["adapter_id"]
          else:
              intent = "unknown"
              confidence = 0.0
              adapter_id = "base_model"
          
          print(f"ğŸ¯ Intent: {intent} (conf: {confidence:.2f}) â†’ {adapter_id}")
          
          # 2. HafÄ±zadan context al
          memory_context = ""
          memory_count = 0
          
          if self.use_memory and self.rag:
              memory_results = self.rag.retrieve(user_message)
              memory_count = len(memory_results)
              
              if memory_results:
                  memory_context = self.rag.format_retrieved_context(memory_results)
                  print(f"ğŸ“š HafÄ±zadan {memory_count} sonuÃ§ bulundu")
          
          # 3. Uygun model/adapter yÃ¼kle
          model, tokenizer, used_adapter = self.lora_manager.get_model(adapter_id)
          
          # 4. Prompt oluÅŸtur
          full_context = memory_context if memory_context else None
          prompt = self.inference.format_chat_prompt(
              user_message=user_message,
              system_prompt=self.system_prompt,
              context=full_context
          )
          
          # 5. YanÄ±t Ã¼ret
          print("ğŸ¤– YanÄ±t Ã¼retiliyor...")
          response_text = self.inference.generate(
              model=model,
              tokenizer=tokenizer,
              prompt=prompt,
              config=self.generation_config
          )
          
          # 6. HafÄ±zaya kaydet
          if self.use_memory and self.rag and save_to_memory:
              self.rag.process_response(
                  user_query=user_message,
                  model_response=response_text,
                  save_to_memory=True
              )
          
          processing_time = time.time() - start_time
          
          return Response(
              text=response_text,
              intent=intent,
              confidence=confidence,
              adapter_used=used_adapter,
              processing_time=processing_time,
              memory_results=memory_count
          )
      
      def get_status(self) -> Dict:
          """Sistem durumu"""
          return {
              "router": "active" if self.use_router else "disabled",
              "memory": "active" if self.use_memory else "disabled",
              "lora_manager": self.lora_manager.get_stats(),
              "rag": self.rag.get_stats() if self.rag else None
          }
      
      def reset_session(self) -> None:
          """Session sÄ±fÄ±rla (kÄ±sa sÃ¼reli hafÄ±zayÄ± temizle)"""
          if self.rag:
              self.rag.buffer.clear()
          print("âœ… Session sÄ±fÄ±rlandÄ±")
  
  
  # Singleton
  _orchestrator: Optional[Orchestrator] = None
  
  
  def get_orchestrator() -> Orchestrator:
      """Global Orchestrator instance"""
      global _orchestrator
      if _orchestrator is None:
          _orchestrator = Orchestrator()
      return _orchestrator
  ```

---

### 5.4 CLI Interface GeliÅŸtirme

#### 5.4.1 Interactive Chat CLI
- [ ] `src/cli.py` oluÅŸtur:
  ```python
  #!/usr/bin/env python3
  """
  EVO-TR Interactive Chat CLI
  """
  
  import sys
  from rich.console import Console
  from rich.panel import Panel
  from rich.markdown import Markdown
  from rich.table import Table
  
  from orchestrator import get_orchestrator
  
  console = Console()
  
  
  def print_help():
      """YardÄ±m mesajÄ±"""
      help_text = """
  **EVO-TR KomutlarÄ±:**
  
  - `/help` - Bu yardÄ±m mesajÄ±nÄ± gÃ¶ster
  - `/status` - Sistem durumunu gÃ¶ster
  - `/adapters` - Mevcut adaptÃ¶rleri listele
  - `/adapter <id>` - Adapter deÄŸiÅŸtir (Ã¶rn: /adapter adapter_python_coder)
  - `/memory` - HafÄ±za istatistikleri
  - `/clear` - KÄ±sa sÃ¼reli hafÄ±zayÄ± temizle
  - `/exit` veya `/quit` - Ã‡Ä±kÄ±ÅŸ
      """
      console.print(Markdown(help_text))
  
  
  def print_status(orchestrator):
      """Sistem durumu"""
      status = orchestrator.get_status()
      
      table = Table(title="Sistem Durumu")
      table.add_column("BileÅŸen", style="cyan")
      table.add_column("Durum", style="green")
      
      table.add_row("Router", status["router"])
      table.add_row("Memory", status["memory"])
      table.add_row("Aktif Adapter", status["lora_manager"]["current_adapter"] or "base_model")
      table.add_row("Cache", f"{status['lora_manager']['cache_size']} adapter")
      
      console.print(table)
  
  
  def print_adapters(orchestrator):
      """Adapter listesi"""
      adapters = orchestrator.lora_manager.list_adapters()
      
      table = Table(title="Mevcut AdaptÃ¶rler")
      table.add_column("ID", style="cyan")
      table.add_column("AÃ§Ä±klama", style="white")
      table.add_column("Intent'ler", style="yellow")
      table.add_column("Cache", style="green")
      
      for adapter_id, info in adapters.items():
          table.add_row(
              adapter_id,
              info["description"],
              ", ".join(info["intents"][:3]),
              "âœ…" if info["cached"] else "âŒ"
          )
      
      console.print(table)
  
  
  def main():
      console.print(Panel(
          "[bold blue]ğŸ¤– EVO-TR Interactive Chat[/bold blue]\n"
          "ModÃ¼ler ve SÃ¼rekli Ã–ÄŸrenen YZ AsistanÄ±\n\n"
          "[dim]YardÄ±m iÃ§in /help yazÄ±n. Ã‡Ä±kmak iÃ§in /exit yazÄ±n.[/dim]",
          expand=False
      ))
      
      # Orchestrator'Ä± baÅŸlat
      console.print("\n[yellow]â³ Sistem baÅŸlatÄ±lÄ±yor...[/yellow]\n")
      orchestrator = get_orchestrator()
      console.print("[green]âœ… Sistem hazÄ±r![/green]\n")
      
      force_adapter = None
      
      while True:
          try:
              # Prompt
              adapter_label = f"[{force_adapter}]" if force_adapter else ""
              user_input = console.input(f"[bold cyan]Sen{adapter_label}:[/bold cyan] ")
              
              if not user_input.strip():
                  continue
              
              # Komut kontrolÃ¼
              if user_input.startswith("/"):
                  cmd = user_input.lower().split()
                  
                  if cmd[0] in ["/exit", "/quit"]:
                      console.print("\n[yellow]ğŸ‘‹ GÃ¶rÃ¼ÅŸÃ¼rÃ¼z![/yellow]\n")
                      break
                  
                  elif cmd[0] == "/help":
                      print_help()
                  
                  elif cmd[0] == "/status":
                      print_status(orchestrator)
                  
                  elif cmd[0] == "/adapters":
                      print_adapters(orchestrator)
                  
                  elif cmd[0] == "/adapter":
                      if len(cmd) > 1:
                          force_adapter = cmd[1] if cmd[1] != "auto" else None
                          console.print(f"[green]âœ… Adapter: {force_adapter or 'auto'}[/green]")
                      else:
                          console.print("[yellow]KullanÄ±m: /adapter <id> veya /adapter auto[/yellow]")
                  
                  elif cmd[0] == "/memory":
                      if orchestrator.rag:
                          stats = orchestrator.rag.get_stats()
                          console.print(f"HafÄ±za: {stats['memory_stats']['total_memories']} kayÄ±t")
                          console.print(f"Buffer: {stats['buffer_stats']['message_count']} mesaj")
                  
                  elif cmd[0] == "/clear":
                      orchestrator.reset_session()
                      console.print("[green]âœ… Session temizlendi[/green]")
                  
                  else:
                      console.print(f"[red]âŒ Bilinmeyen komut: {cmd[0]}[/red]")
                  
                  continue
              
              # Normal mesaj - yanÄ±t Ã¼ret
              response = orchestrator.chat(
                  user_message=user_input,
                  force_adapter=force_adapter
              )
              
              # YanÄ±tÄ± gÃ¶ster
              console.print()
              console.print(Panel(
                  response.text,
                  title=f"[bold green]EVO-TR[/bold green] "
                        f"[dim]({response.adapter_used}, {response.processing_time:.1f}s)[/dim]",
                  expand=False
              ))
              console.print()
              
          except KeyboardInterrupt:
              console.print("\n[yellow]Ctrl+C - Ã‡Ä±kmak iÃ§in /exit yazÄ±n[/yellow]")
              continue
          except Exception as e:
              console.print(f"[red]âŒ Hata: {e}[/red]")
              continue
  
  
  if __name__ == "__main__":
      main()
  ```

---

### 5.5 Entegrasyon Testleri

#### 5.5.1 UÃ§tan Uca Test Script
- [ ] `scripts/test_e2e.py` oluÅŸtur:
  ```python
  #!/usr/bin/env python3
  """UÃ§tan uca entegrasyon testi"""
  
  from rich.console import Console
  from rich.table import Table
  
  from src.orchestrator import get_orchestrator
  
  console = Console()
  
  TEST_CASES = [
      # (mesaj, beklenen_intent, aÃ§Ä±klama)
      ("Merhaba, nasÄ±lsÄ±n?", "general_chat", "SelamlaÅŸma"),
      ("Python'da liste nasÄ±l sÄ±ralarÄ±m?", "code_python", "Kod sorusu"),
      ("Bu hata ne demek: IndexError", "code_debug", "Debug sorusu"),
      ("AtatÃ¼rk hakkÄ±nda bilgi ver", "turkish_culture", "KÃ¼ltÃ¼r sorusu"),
      ("Daha Ã¶nce sana ne sÃ¶yledim?", "memory_recall", "HafÄ±za sorusu"),
  ]
  
  
  def main():
      console.print("\n[bold blue]ğŸ§ª EVO-TR UÃ§tan Uca Test[/bold blue]\n")
      
      orchestrator = get_orchestrator()
      
      results = []
      
      for message, expected_intent, description in TEST_CASES:
          console.print(f"\n[cyan]Test:[/cyan] {description}")
          console.print(f"[dim]Mesaj: {message}[/dim]")
          
          try:
              response = orchestrator.chat(message, save_to_memory=False)
              
              intent_match = response.intent == expected_intent
              has_response = len(response.text) > 10
              
              results.append({
                  "description": description,
                  "intent_match": intent_match,
                  "has_response": has_response,
                  "adapter": response.adapter_used,
                  "time": response.processing_time
              })
              
              status = "âœ…" if (intent_match and has_response) else "âŒ"
              console.print(f"{status} Intent: {response.intent} (beklenen: {expected_intent})")
              console.print(f"   Adapter: {response.adapter_used}")
              console.print(f"   SÃ¼re: {response.processing_time:.2f}s")
              console.print(f"   YanÄ±t: {response.text[:100]}...")
              
          except Exception as e:
              results.append({
                  "description": description,
                  "intent_match": False,
                  "has_response": False,
                  "error": str(e)
              })
              console.print(f"[red]âŒ Hata: {e}[/red]")
      
      # Ã–zet
      console.print("\n" + "="*60 + "\n")
      
      table = Table(title="Test Ã–zeti")
      table.add_column("Test", style="cyan")
      table.add_column("Intent", style="yellow")
      table.add_column("YanÄ±t", style="green")
      table.add_column("SÃ¼re", style="magenta")
      
      success = 0
      for r in results:
          intent = "âœ…" if r.get("intent_match") else "âŒ"
          response = "âœ…" if r.get("has_response") else "âŒ"
          time_str = f"{r.get('time', 0):.1f}s" if "time" in r else "N/A"
          
          table.add_row(r["description"], intent, response, time_str)
          
          if r.get("intent_match") and r.get("has_response"):
              success += 1
      
      console.print(table)
      console.print(f"\n[bold]BaÅŸarÄ±: {success}/{len(results)} ({100*success/len(results):.0f}%)[/bold]\n")
  
  
  if __name__ == "__main__":
      main()
  ```

#### 5.5.2 Performance Test
- [ ] `scripts/test_performance.py` oluÅŸtur
- [ ] Latency Ã¶lÃ§Ã¼mleri
- [ ] Memory kullanÄ±mÄ± Ã¶lÃ§Ã¼mleri
- [ ] Throughput testleri

---

### 5.6 Logging Sistemi

#### 5.6.1 Structured Logger
- [ ] `src/lifecycle/logger.py` oluÅŸtur:
  ```python
  """
  EVO-TR Logging Sistemi
  """
  
  import json
  import logging
  from datetime import datetime
  from pathlib import Path
  from typing import Optional, Dict, Any
  
  
  class ConversationLogger:
      """KonuÅŸma loglama"""
      
      def __init__(self, log_dir: str = "./logs/conversations"):
          self.log_dir = Path(log_dir)
          self.log_dir.mkdir(parents=True, exist_ok=True)
          
          # GÃ¼nlÃ¼k dosya adÄ±
          self.current_date = datetime.now().strftime("%Y-%m-%d")
          self.log_file = self.log_dir / f"{self.current_date}.jsonl"
      
      def log_interaction(
          self,
          user_message: str,
          response_text: str,
          intent: str,
          confidence: float,
          adapter_used: str,
          processing_time: float,
          metadata: Optional[Dict] = None
      ) -> None:
          """EtkileÅŸimi logla"""
          entry = {
              "timestamp": datetime.now().isoformat(),
              "user_message": user_message,
              "response": response_text,
              "intent": intent,
              "confidence": confidence,
              "adapter": adapter_used,
              "processing_time": processing_time,
              "metadata": metadata or {}
          }
          
          with open(self.log_file, "a", encoding="utf-8") as f:
              f.write(json.dumps(entry, ensure_ascii=False) + "\n")
      
      def get_daily_logs(self, date: Optional[str] = None) -> list:
          """GÃ¼nlÃ¼k loglarÄ± getir"""
          target_date = date or self.current_date
          log_file = self.log_dir / f"{target_date}.jsonl"
          
          if not log_file.exists():
              return []
          
          logs = []
          with open(log_file, "r", encoding="utf-8") as f:
              for line in f:
                  logs.append(json.loads(line))
          
          return logs
  
  
  # Singleton
  _logger: Optional[ConversationLogger] = None
  
  
  def get_conversation_logger() -> ConversationLogger:
      global _logger
      if _logger is None:
          _logger = ConversationLogger()
      return _logger
  ```

---

## âœ… Faz Tamamlanma Kriterleri

1. [ ] LoRA Manager adapter'larÄ± yÃ¶netebiliyor
2. [ ] Inference Engine yanÄ±t Ã¼retebiliyor
3. [ ] Orchestrator tÃ¼m bileÅŸenleri birleÅŸtiriyor
4. [ ] CLI interface Ã§alÄ±ÅŸÄ±yor
5. [ ] Router â†’ Adapter â†’ Response akÄ±ÅŸÄ± Ã§alÄ±ÅŸÄ±yor
6. [ ] HafÄ±za entegrasyonu aktif
7. [ ] E2E testler %80+ baÅŸarÄ±lÄ±
8. [ ] Logging sistemi Ã§alÄ±ÅŸÄ±yor

---

## â­ï¸ Sonraki Faz

Faz 5 tamamlandÄ±ktan sonra â†’ **FAZ-6-YASAM-DONGUSU.md** dosyasÄ±na geÃ§.

---

## ğŸ› OlasÄ± Sorunlar ve Ã‡Ã¶zÃ¼mleri

### Adapter DeÄŸiÅŸtirme YavaÅŸ
**Ã‡Ã¶zÃ¼m:** Cache mekanizmasÄ±nÄ± aktif et, sÄ±k kullanÄ±lan adapter'larÄ± Ã¶nbellekte tut

### Memory Overflow
**Ã‡Ã¶zÃ¼m:** Generation config'de max_tokens dÃ¼ÅŸÃ¼r, batch size=1 kullan

### Intent YanlÄ±ÅŸ Tahmin
**Ã‡Ã¶zÃ¼m:** /adapter komutu ile manuel override yap, router eÄŸitim verisini gÃ¼ncelle

---

## ğŸ“Š Zaman Takibi

| GÃ¶rev | BaÅŸlangÄ±Ã§ | BitiÅŸ | SÃ¼re |
|-------|-----------|-------|------|
| 5.1 LoRA Manager | | | |
| 5.2 Inference Engine | | | |
| 5.3 Orchestrator | | | |
| 5.4 CLI Interface | | | |
| 5.5 E2E Testler | | | |
| 5.6 Logging | | | |
| **TOPLAM** | | | |

---

*Bu faz tamamlandÄ±ÄŸÄ±nda, "âœ… FAZ 5 TAMAMLANDI" olarak iÅŸaretle.*
