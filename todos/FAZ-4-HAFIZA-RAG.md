# ğŸ§  FAZ 4: HafÄ±za ve RAG Sistemi (The Hippocampus)

**Durum:** â¬œ BaÅŸlanmadÄ±  
**Tahmini SÃ¼re:** 2-3 gÃ¼n  
**Ã–ncelik:** ğŸŸ  YÃ¼ksek  
**BaÄŸÄ±mlÄ±lÄ±k:** Faz 0, 1, 2, 3 tamamlanmÄ±ÅŸ olmalÄ±

---

## ğŸ¯ Faz Hedefi

Modelin geÃ§miÅŸ konuÅŸmalarÄ± hatÄ±rlamasÄ±nÄ± ve kullanÄ±cÄ± hakkÄ±nda bilgi biriktirmesini saÄŸlayan hafÄ±za sistemi oluÅŸturmak. Bu sistem RAG (Retrieval-Augmented Generation) kullanarak uzun sÃ¼reli hafÄ±za saÄŸlar.

---

## ğŸ—ï¸ Mimari Genel BakÄ±ÅŸ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     KULLANICI MESAJI                         â”‚
â”‚              "DÃ¼n sana sÃ¶ylediÄŸim proje adÄ± neydi?"          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   KISA SÃœRELÄ° HAFIZA    â”‚   â”‚     UZUN SÃœRELÄ° HAFIZA (RAG)    â”‚
â”‚  (Context Buffer)       â”‚   â”‚                                 â”‚
â”‚                         â”‚   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â€¢ Son 10-20 mesaj      â”‚   â”‚  â”‚       ChromaDB              â”‚â”‚
â”‚  â€¢ Session-based        â”‚   â”‚  â”‚    (Vector Database)        â”‚â”‚
â”‚  â€¢ RAM'de tutulur       â”‚   â”‚  â”‚                             â”‚â”‚
â”‚                         â”‚   â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚â”‚
â”‚  [Usr] Merhaba          â”‚   â”‚  â”‚  â”‚  Turkish Embeddings   â”‚  â”‚â”‚
â”‚  [Bot] Selam!           â”‚   â”‚  â”‚  â”‚  (Sentence-BERT-TR)   â”‚  â”‚â”‚
â”‚  [Usr] Proje adÄ± X      â”‚   â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚â”‚
â”‚  [Bot] Tamam, not ettim â”‚   â”‚  â”‚                             â”‚â”‚
â”‚                         â”‚   â”‚  â”‚  Persistent Storage         â”‚â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚  â”‚  ./data/chromadb/           â”‚â”‚
                              â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                            â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚         RETRIEVAL               â”‚
                              â”‚                                 â”‚
                              â”‚  Query: "proje adÄ±"             â”‚
                              â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€           â”‚
                              â”‚  Top-3 Results:                 â”‚
                              â”‚  1. "Proje adÄ±: EVO-TR" (0.92)  â”‚
                              â”‚  2. "Proje kapsamÄ±..." (0.78)   â”‚
                              â”‚  3. "Kaan'Ä±n projesi" (0.71)    â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                            â”‚
                                            â–¼
                              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                              â”‚       AUGMENTED PROMPT          â”‚
                              â”‚                                 â”‚
                              â”‚  [Context from memory]          â”‚
                              â”‚  Proje adÄ±: EVO-TR              â”‚
                              â”‚                                 â”‚
                              â”‚  [User question]                â”‚
                              â”‚  DÃ¼n sÃ¶ylediÄŸim proje adÄ±?      â”‚
                              â”‚                                 â”‚
                              â”‚  [Model response]               â”‚
                              â”‚  EVO-TR projesinden bahsetmiÅŸtiniz.â”‚
                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ DetaylÄ± GÃ¶rev Listesi

### 4.1 ChromaDB Kurulumu

#### 4.1.1 BaÄŸÄ±mlÄ±lÄ±k KontrolÃ¼
- [ ] ChromaDB'nin requirements.txt'te olduÄŸunu doÄŸrula
- [ ] Kurulumu test et:
  ```bash
  python -c "import chromadb; print(chromadb.__version__)"
  ```

#### 4.1.2 Persistent Storage Dizini
- [ ] `data/chromadb/` dizinini oluÅŸtur
- [ ] Yazma izinlerini kontrol et
- [ ] .gitignore'da olduÄŸundan emin ol

#### 4.1.3 ChromaDB Client Test
- [ ] `scripts/test_chromadb.py` oluÅŸtur:
  ```python
  #!/usr/bin/env python3
  """ChromaDB baÄŸlantÄ± testi"""
  
  import chromadb
  from chromadb.config import Settings
  
  # Persistent client oluÅŸtur
  client = chromadb.PersistentClient(
      path="./data/chromadb",
      settings=Settings(anonymized_telemetry=False)
  )
  
  # Test collection oluÅŸtur
  collection = client.get_or_create_collection(
      name="test_collection",
      metadata={"description": "Test collection"}
  )
  
  # Veri ekle
  collection.add(
      documents=["Bu bir test dokÃ¼manÄ±dÄ±r.", "Merhaba dÃ¼nya!"],
      metadatas=[{"type": "test"}, {"type": "greeting"}],
      ids=["doc1", "doc2"]
  )
  
  # Sorgula
  results = collection.query(
      query_texts=["selamlama"],
      n_results=1
  )
  
  print("âœ… ChromaDB Ã§alÄ±ÅŸÄ±yor!")
  print(f"Query sonucu: {results['documents']}")
  
  # Temizle
  client.delete_collection("test_collection")
  print("âœ… Test collection silindi")
  ```
- [ ] Testi Ã§alÄ±ÅŸtÄ±r ve doÄŸrula

---

### 4.2 Embedding Model Kurulumu

#### 4.2.1 TÃ¼rkÃ§e Embedding Model SeÃ§imi
- [ ] AÅŸaÄŸÄ±daki modelleri deÄŸerlendir:

| Model | Boyut | TÃ¼rkÃ§e PerformansÄ± | Ã–nerilen |
|-------|-------|-------------------|----------|
| `emrecan/bert-base-turkish-cased-mean-nli-stsb-tr` | 438MB | â­â­â­â­â­ | âœ… En iyi |
| `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` | 420MB | â­â­â­â­ | Alternatif |
| `intfloat/multilingual-e5-small` | 117MB | â­â­â­ | Hafif alternatif |

#### 4.2.2 Model Ä°ndirme
- [ ] `scripts/download_embedding_model.py` oluÅŸtur:
  ```python
  #!/usr/bin/env python3
  """TÃ¼rkÃ§e embedding modelini indir"""
  
  from sentence_transformers import SentenceTransformer
  from pathlib import Path
  
  MODEL_NAME = "emrecan/bert-base-turkish-cased-mean-nli-stsb-tr"
  OUTPUT_PATH = Path("./models/embeddings/turkish-sbert")
  
  def main():
      print(f"ğŸ“¥ Model indiriliyor: {MODEL_NAME}")
      
      model = SentenceTransformer(MODEL_NAME)
      
      OUTPUT_PATH.mkdir(parents=True, exist_ok=True)
      model.save(str(OUTPUT_PATH))
      
      print(f"âœ… Kaydedildi: {OUTPUT_PATH}")
      
      # Test
      test_sentences = [
          "Merhaba, nasÄ±lsÄ±n?",
          "Selam, ne haber?",
          "Python programlama dili"
      ]
      
      embeddings = model.encode(test_sentences)
      print(f"âœ… Embedding boyutu: {embeddings.shape}")
  
  if __name__ == "__main__":
      main()
  ```
- [ ] Script'i Ã§alÄ±ÅŸtÄ±r
- [ ] Model boyutunu not et

#### 4.2.3 Embedding Test
- [ ] Benzerlik testi yap:
  ```python
  from sentence_transformers import SentenceTransformer, util
  
  model = SentenceTransformer("./models/embeddings/turkish-sbert")
  
  sentences = [
      "Merhaba, nasÄ±lsÄ±n?",
      "Selam, ne haber?",
      "Python'da liste nasÄ±l oluÅŸturulur?"
  ]
  
  embeddings = model.encode(sentences)
  
  # Benzerlik matrisi
  for i, s1 in enumerate(sentences):
      for j, s2 in enumerate(sentences):
          sim = util.cos_sim(embeddings[i], embeddings[j])
          print(f"'{s1[:30]}' <-> '{s2[:30]}': {sim.item():.3f}")
  ```

---

### 4.3 ChromaDB Handler GeliÅŸtirme

#### 4.3.1 Memory Handler Class
- [ ] `src/memory/chromadb_handler.py` oluÅŸtur:
  ```python
  """
  EVO-TR HafÄ±za Sistemi: ChromaDB Handler
  
  Uzun sÃ¼reli hafÄ±za yÃ¶netimi iÃ§in vektÃ¶r veritabanÄ± iÅŸlemleri.
  """
  
  import json
  import uuid
  from datetime import datetime
  from pathlib import Path
  from typing import List, Dict, Optional, Any
  
  import chromadb
  from chromadb.config import Settings
  from sentence_transformers import SentenceTransformer
  
  
  class MemoryHandler:
      """ChromaDB tabanlÄ± uzun sÃ¼reli hafÄ±za yÃ¶neticisi"""
      
      def __init__(
          self,
          persist_dir: str = "./data/chromadb",
          embedding_model_path: str = "./models/embeddings/turkish-sbert",
          collection_name: str = "evo_tr_memory"
      ):
          """
          Args:
              persist_dir: ChromaDB kalÄ±cÄ± depolama dizini
              embedding_model_path: Embedding model yolu
              collection_name: Collection adÄ±
          """
          self.persist_dir = Path(persist_dir)
          self.persist_dir.mkdir(parents=True, exist_ok=True)
          
          # ChromaDB client
          self.client = chromadb.PersistentClient(
              path=str(self.persist_dir),
              settings=Settings(anonymized_telemetry=False)
          )
          
          # Embedding model
          self.embedding_model = SentenceTransformer(embedding_model_path)
          
          # Collection
          self.collection = self.client.get_or_create_collection(
              name=collection_name,
              metadata={"description": "EVO-TR Long-term Memory"}
          )
          
          print(f"âœ… MemoryHandler baÅŸlatÄ±ldÄ±. Collection: {collection_name}")
          print(f"   Mevcut kayÄ±t sayÄ±sÄ±: {self.collection.count()}")
      
      def add_memory(
          self,
          text: str,
          metadata: Optional[Dict[str, Any]] = None,
          memory_type: str = "general"
      ) -> str:
          """
          HafÄ±zaya yeni bilgi ekle.
          
          Args:
              text: Kaydedilecek metin
              metadata: Ek bilgiler (tarih, kategori, vb.)
              memory_type: HafÄ±za tipi (general, user_info, conversation, fact)
              
          Returns:
              OluÅŸturulan belge ID'si
          """
          doc_id = str(uuid.uuid4())
          
          # VarsayÄ±lan metadata
          default_metadata = {
              "type": memory_type,
              "created_at": datetime.now().isoformat(),
              "text_length": len(text)
          }
          
          if metadata:
              default_metadata.update(metadata)
          
          # Embedding oluÅŸtur ve ekle
          embedding = self.embedding_model.encode([text])[0].tolist()
          
          self.collection.add(
              documents=[text],
              embeddings=[embedding],
              metadatas=[default_metadata],
              ids=[doc_id]
          )
          
          return doc_id
      
      def search(
          self,
          query: str,
          top_k: int = 3,
          min_score: float = 0.5,
          filter_metadata: Optional[Dict] = None
      ) -> List[Dict]:
          """
          HafÄ±zada arama yap.
          
          Args:
              query: Arama sorgusu
              top_k: DÃ¶ndÃ¼rÃ¼lecek maksimum sonuÃ§ sayÄ±sÄ±
              min_score: Minimum benzerlik skoru
              filter_metadata: Metadata filtresi
              
          Returns:
              Bulunan belgelerin listesi
          """
          # Query embedding
          query_embedding = self.embedding_model.encode([query])[0].tolist()
          
          # Arama
          results = self.collection.query(
              query_embeddings=[query_embedding],
              n_results=top_k,
              where=filter_metadata
          )
          
          # SonuÃ§larÄ± formatla
          formatted_results = []
          
          if results and results['documents'] and results['documents'][0]:
              for i, doc in enumerate(results['documents'][0]):
                  # Distance'Ä± similarity'ye Ã§evir (ChromaDB L2 distance kullanÄ±r)
                  distance = results['distances'][0][i] if results['distances'] else 0
                  # L2 distance -> cosine similarity yaklaÅŸÄ±mÄ±
                  similarity = 1 / (1 + distance)
                  
                  if similarity >= min_score:
                      formatted_results.append({
                          "id": results['ids'][0][i],
                          "text": doc,
                          "score": similarity,
                          "metadata": results['metadatas'][0][i] if results['metadatas'] else {}
                      })
          
          return formatted_results
      
      def get_by_id(self, doc_id: str) -> Optional[Dict]:
          """ID ile belge getir"""
          result = self.collection.get(ids=[doc_id])
          
          if result and result['documents']:
              return {
                  "id": doc_id,
                  "text": result['documents'][0],
                  "metadata": result['metadatas'][0] if result['metadatas'] else {}
              }
          return None
      
      def delete(self, doc_id: str) -> bool:
          """Belge sil"""
          try:
              self.collection.delete(ids=[doc_id])
              return True
          except Exception:
              return False
      
      def update(self, doc_id: str, new_text: str, new_metadata: Optional[Dict] = None) -> bool:
          """Belge gÃ¼ncelle"""
          try:
              existing = self.get_by_id(doc_id)
              if not existing:
                  return False
              
              # Yeni embedding
              embedding = self.embedding_model.encode([new_text])[0].tolist()
              
              # Metadata gÃ¼ncelle
              metadata = existing.get("metadata", {})
              metadata["updated_at"] = datetime.now().isoformat()
              if new_metadata:
                  metadata.update(new_metadata)
              
              self.collection.update(
                  ids=[doc_id],
                  documents=[new_text],
                  embeddings=[embedding],
                  metadatas=[metadata]
              )
              return True
          except Exception:
              return False
      
      def get_stats(self) -> Dict:
          """HafÄ±za istatistikleri"""
          count = self.collection.count()
          
          # Tip bazlÄ± daÄŸÄ±lÄ±m
          type_counts = {}
          if count > 0:
              all_docs = self.collection.get()
              for meta in all_docs.get('metadatas', []):
                  mem_type = meta.get('type', 'unknown')
                  type_counts[mem_type] = type_counts.get(mem_type, 0) + 1
          
          return {
              "total_memories": count,
              "type_distribution": type_counts,
              "persist_dir": str(self.persist_dir)
          }
      
      def clear_all(self) -> int:
          """TÃ¼m hafÄ±zayÄ± temizle (DÄ°KKAT!)"""
          count = self.collection.count()
          
          # Collection'Ä± sil ve yeniden oluÅŸtur
          collection_name = self.collection.name
          self.client.delete_collection(collection_name)
          self.collection = self.client.create_collection(
              name=collection_name,
              metadata={"description": "EVO-TR Long-term Memory"}
          )
          
          return count
  
  
  # Singleton instance
  _memory_handler: Optional[MemoryHandler] = None
  
  
  def get_memory_handler() -> MemoryHandler:
      """Global MemoryHandler instance dÃ¶ndÃ¼r"""
      global _memory_handler
      if _memory_handler is None:
          _memory_handler = MemoryHandler()
      return _memory_handler
  ```

#### 4.3.2 Memory Handler Testleri
- [ ] `tests/test_memory.py` oluÅŸtur:
  ```python
  """Memory Handler testleri"""
  
  import pytest
  from src.memory.chromadb_handler import MemoryHandler
  
  
  @pytest.fixture
  def memory_handler():
      """Test iÃ§in geÃ§ici memory handler"""
      handler = MemoryHandler(
          persist_dir="./data/chromadb_test",
          collection_name="test_memory"
      )
      yield handler
      # Temizlik
      handler.clear_all()
  
  
  class TestMemoryHandler:
      
      def test_add_memory(self, memory_handler):
          """HafÄ±zaya ekleme testi"""
          doc_id = memory_handler.add_memory(
              "KullanÄ±cÄ±nÄ±n adÄ± Kaan.",
              memory_type="user_info"
          )
          assert doc_id is not None
          assert len(doc_id) > 0
      
      def test_search(self, memory_handler):
          """Arama testi"""
          # Veri ekle
          memory_handler.add_memory("Proje adÄ± EVO-TR.", memory_type="fact")
          memory_handler.add_memory("Kaan Python seviyor.", memory_type="user_info")
          
          # Ara
          results = memory_handler.search("proje adÄ± ne?", top_k=1)
          assert len(results) > 0
          assert "EVO-TR" in results[0]["text"]
      
      def test_get_by_id(self, memory_handler):
          """ID ile getirme testi"""
          doc_id = memory_handler.add_memory("Test verisi")
          result = memory_handler.get_by_id(doc_id)
          assert result is not None
          assert result["text"] == "Test verisi"
      
      def test_delete(self, memory_handler):
          """Silme testi"""
          doc_id = memory_handler.add_memory("Silinecek veri")
          assert memory_handler.delete(doc_id) == True
          assert memory_handler.get_by_id(doc_id) is None
      
      def test_stats(self, memory_handler):
          """Ä°statistik testi"""
          memory_handler.add_memory("Veri 1", memory_type="fact")
          memory_handler.add_memory("Veri 2", memory_type="user_info")
          
          stats = memory_handler.get_stats()
          assert stats["total_memories"] == 2
  ```

---

### 4.4 KÄ±sa SÃ¼reli HafÄ±za (Context Buffer)

#### 4.4.1 Context Buffer Class
- [ ] `src/memory/context_buffer.py` oluÅŸtur:
  ```python
  """
  EVO-TR KÄ±sa SÃ¼reli HafÄ±za: Context Buffer
  
  Son N mesajÄ± bellekte tutar ve context olarak saÄŸlar.
  """
  
  from collections import deque
  from dataclasses import dataclass, field
  from datetime import datetime
  from typing import List, Optional, Dict
  
  
  @dataclass
  class Message:
      """Tek bir mesaj"""
      role: str  # "user" veya "assistant"
      content: str
      timestamp: datetime = field(default_factory=datetime.now)
      metadata: Dict = field(default_factory=dict)
  
  
  class ContextBuffer:
      """
      Son N mesajÄ± tutan kÄ±sa sÃ¼reli hafÄ±za.
      
      Session-based Ã§alÄ±ÅŸÄ±r, yeniden baÅŸlatÄ±ldÄ±ÄŸÄ±nda sÄ±fÄ±rlanÄ±r.
      """
      
      def __init__(
          self,
          max_messages: int = 20,
          max_tokens: int = 4096
      ):
          """
          Args:
              max_messages: Maksimum mesaj sayÄ±sÄ±
              max_tokens: Maksimum token sayÄ±sÄ± (tahmini)
          """
          self.max_messages = max_messages
          self.max_tokens = max_tokens
          self.messages: deque = deque(maxlen=max_messages)
          self.session_id: str = datetime.now().strftime("%Y%m%d_%H%M%S")
      
      def add_message(self, role: str, content: str, metadata: Optional[Dict] = None) -> None:
          """Yeni mesaj ekle"""
          message = Message(
              role=role,
              content=content,
              metadata=metadata or {}
          )
          self.messages.append(message)
          
          # Token kontrolÃ¼ (basit yaklaÅŸÄ±m: 4 karakter = 1 token)
          self._trim_to_token_limit()
      
      def _trim_to_token_limit(self) -> None:
          """Token limitini aÅŸarsa eski mesajlarÄ± kaldÄ±r"""
          total_chars = sum(len(m.content) for m in self.messages)
          estimated_tokens = total_chars / 4
          
          while estimated_tokens > self.max_tokens and len(self.messages) > 2:
              self.messages.popleft()
              total_chars = sum(len(m.content) for m in self.messages)
              estimated_tokens = total_chars / 4
      
      def get_messages(self, last_n: Optional[int] = None) -> List[Message]:
          """MesajlarÄ± getir"""
          if last_n:
              return list(self.messages)[-last_n:]
          return list(self.messages)
      
      def get_formatted_context(self, format_type: str = "chat") -> str:
          """
          FormatlanmÄ±ÅŸ context dÃ¶ndÃ¼r.
          
          Args:
              format_type: "chat", "qwen", "simple"
          """
          if format_type == "chat":
              lines = []
              for msg in self.messages:
                  role_label = "KullanÄ±cÄ±" if msg.role == "user" else "Asistan"
                  lines.append(f"{role_label}: {msg.content}")
              return "\n".join(lines)
          
          elif format_type == "qwen":
              # Qwen chat format
              formatted = []
              for msg in self.messages:
                  if msg.role == "user":
                      formatted.append(f"<|im_start|>user\n{msg.content}<|im_end|>")
                  else:
                      formatted.append(f"<|im_start|>assistant\n{msg.content}<|im_end|>")
              return "\n".join(formatted)
          
          else:  # simple
              return "\n\n".join(m.content for m in self.messages)
      
      def get_last_user_message(self) -> Optional[str]:
          """Son kullanÄ±cÄ± mesajÄ±nÄ± dÃ¶ndÃ¼r"""
          for msg in reversed(self.messages):
              if msg.role == "user":
                  return msg.content
          return None
      
      def get_last_assistant_message(self) -> Optional[str]:
          """Son asistan mesajÄ±nÄ± dÃ¶ndÃ¼r"""
          for msg in reversed(self.messages):
              if msg.role == "assistant":
                  return msg.content
          return None
      
      def clear(self) -> None:
          """Buffer'Ä± temizle"""
          self.messages.clear()
          self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
      
      def get_stats(self) -> Dict:
          """Buffer istatistikleri"""
          total_chars = sum(len(m.content) for m in self.messages)
          return {
              "session_id": self.session_id,
              "message_count": len(self.messages),
              "max_messages": self.max_messages,
              "estimated_tokens": total_chars / 4,
              "max_tokens": self.max_tokens
          }
      
      def export_session(self) -> List[Dict]:
          """Session'Ä± export et"""
          return [
              {
                  "role": msg.role,
                  "content": msg.content,
                  "timestamp": msg.timestamp.isoformat(),
                  "metadata": msg.metadata
              }
              for msg in self.messages
          ]
  
  
  # Singleton
  _context_buffer: Optional[ContextBuffer] = None
  
  
  def get_context_buffer() -> ContextBuffer:
      """Global ContextBuffer instance"""
      global _context_buffer
      if _context_buffer is None:
          _context_buffer = ContextBuffer()
      return _context_buffer
  ```

---

### 4.5 RAG Pipeline GeliÅŸtirme

#### 4.5.1 RAG Orchestrator
- [ ] `src/memory/rag_pipeline.py` oluÅŸtur:
  ```python
  """
  EVO-TR RAG Pipeline
  
  KullanÄ±cÄ± sorusunu alÄ±r, hafÄ±zada arama yapar ve zenginleÅŸtirilmiÅŸ context dÃ¶ndÃ¼rÃ¼r.
  """
  
  from typing import List, Dict, Optional, Tuple
  from .chromadb_handler import get_memory_handler, MemoryHandler
  from .context_buffer import get_context_buffer, ContextBuffer
  
  
  class RAGPipeline:
      """Retrieval-Augmented Generation Pipeline"""
      
      def __init__(
          self,
          memory_handler: Optional[MemoryHandler] = None,
          context_buffer: Optional[ContextBuffer] = None,
          top_k: int = 3,
          min_relevance: float = 0.5,
          max_context_tokens: int = 1024
      ):
          self.memory = memory_handler or get_memory_handler()
          self.buffer = context_buffer or get_context_buffer()
          self.top_k = top_k
          self.min_relevance = min_relevance
          self.max_context_tokens = max_context_tokens
      
      def retrieve(self, query: str) -> List[Dict]:
          """
          HafÄ±zadan ilgili bilgileri getir.
          
          Args:
              query: KullanÄ±cÄ± sorusu
              
          Returns:
              Ä°lgili belgelerin listesi
          """
          results = self.memory.search(
              query=query,
              top_k=self.top_k,
              min_score=self.min_relevance
          )
          return results
      
      def format_retrieved_context(self, results: List[Dict]) -> str:
          """Getirilen sonuÃ§larÄ± context string'e Ã§evir"""
          if not results:
              return ""
          
          lines = ["[HafÄ±zadan hatÄ±rlanan bilgiler:]"]
          for i, result in enumerate(results, 1):
              lines.append(f"{i}. {result['text']}")
          
          return "\n".join(lines)
      
      def build_augmented_prompt(
          self,
          user_query: str,
          include_memory: bool = True,
          include_history: bool = True,
          system_prompt: Optional[str] = None
      ) -> str:
          """
          ZenginleÅŸtirilmiÅŸ prompt oluÅŸtur.
          
          Args:
              user_query: KullanÄ±cÄ± sorusu
              include_memory: Uzun sÃ¼reli hafÄ±zayÄ± dahil et
              include_history: KÄ±sa sÃ¼reli geÃ§miÅŸi dahil et
              system_prompt: Opsiyonel system prompt
              
          Returns:
              Model'e gÃ¶nderilecek tam prompt
          """
          parts = []
          
          # System prompt
          if system_prompt:
              parts.append(f"[System]: {system_prompt}")
          
          # Uzun sÃ¼reli hafÄ±za (RAG)
          if include_memory:
              retrieved = self.retrieve(user_query)
              if retrieved:
                  memory_context = self.format_retrieved_context(retrieved)
                  parts.append(memory_context)
          
          # KÄ±sa sÃ¼reli geÃ§miÅŸ
          if include_history and len(self.buffer.messages) > 0:
              history = self.buffer.get_formatted_context(format_type="chat")
              if history:
                  parts.append(f"[Sohbet geÃ§miÅŸi:]\n{history}")
          
          # KullanÄ±cÄ± sorusu
          parts.append(f"[KullanÄ±cÄ±]: {user_query}")
          
          return "\n\n".join(parts)
      
      def process_response(
          self,
          user_query: str,
          model_response: str,
          save_to_memory: bool = True
      ) -> None:
          """
          YanÄ±tÄ± iÅŸle ve hafÄ±zaya kaydet.
          
          Args:
              user_query: KullanÄ±cÄ± sorusu
              model_response: Model yanÄ±tÄ±
              save_to_memory: Uzun sÃ¼reli hafÄ±zaya kaydet
          """
          # KÄ±sa sÃ¼reli hafÄ±zaya ekle
          self.buffer.add_message("user", user_query)
          self.buffer.add_message("assistant", model_response)
          
          # Uzun sÃ¼reli hafÄ±zaya Ã¶nemli bilgileri kaydet
          if save_to_memory:
              self._extract_and_save_info(user_query, model_response)
      
      def _extract_and_save_info(self, query: str, response: str) -> None:
          """
          KonuÅŸmadan Ã¶nemli bilgileri Ã§Ä±kar ve kaydet.
          
          TODO: NER, keyword extraction eklenebilir
          """
          # Basit yaklaÅŸÄ±m: "benim adÄ±m X" gibi pattern'larÄ± yakala
          import re
          
          # Ä°sim pattern'Ä±
          name_patterns = [
              r"benim ad[Ä±i]m (\w+)",
              r"ben (\w+)",
              r"ad[Ä±i]m (\w+)"
          ]
          
          combined_text = f"{query} {response}".lower()
          
          for pattern in name_patterns:
              match = re.search(pattern, combined_text)
              if match:
                  name = match.group(1).title()
                  self.memory.add_memory(
                      f"KullanÄ±cÄ±nÄ±n adÄ±: {name}",
                      metadata={"extracted_type": "user_name"},
                      memory_type="user_info"
                  )
                  break
          
          # Proje adÄ± pattern'Ä±
          project_patterns = [
              r"proje(?:nin)? ad[Ä±i] (\w+)",
              r"(\w+) projes[Ä±i]"
          ]
          
          for pattern in project_patterns:
              match = re.search(pattern, combined_text)
              if match:
                  project = match.group(1).upper()
                  self.memory.add_memory(
                      f"Proje adÄ±: {project}",
                      metadata={"extracted_type": "project_name"},
                      memory_type="fact"
                  )
                  break
      
      def get_stats(self) -> Dict:
          """Pipeline istatistikleri"""
          return {
              "memory_stats": self.memory.get_stats(),
              "buffer_stats": self.buffer.get_stats(),
              "config": {
                  "top_k": self.top_k,
                  "min_relevance": self.min_relevance,
                  "max_context_tokens": self.max_context_tokens
              }
          }
  
  
  # Singleton
  _rag_pipeline: Optional[RAGPipeline] = None
  
  
  def get_rag_pipeline() -> RAGPipeline:
      """Global RAGPipeline instance"""
      global _rag_pipeline
      if _rag_pipeline is None:
          _rag_pipeline = RAGPipeline()
      return _rag_pipeline
  ```

---

### 4.6 Entegrasyon ve Test

#### 4.6.1 Memory Demo Script
- [ ] `scripts/demo_memory.py` oluÅŸtur:
  ```python
  #!/usr/bin/env python3
  """HafÄ±za sistemi demo"""
  
  from rich.console import Console
  from rich.panel import Panel
  from rich.table import Table
  
  from src.memory.rag_pipeline import get_rag_pipeline
  from src.memory.chromadb_handler import get_memory_handler
  
  console = Console()
  
  def main():
      console.print("\n[bold blue]ğŸ§  EVO-TR HafÄ±za Sistemi Demo[/bold blue]\n")
      
      # RAG Pipeline al
      rag = get_rag_pipeline()
      memory = get_memory_handler()
      
      # BaÅŸlangÄ±Ã§ durumu
      stats = rag.get_stats()
      console.print(f"Mevcut hafÄ±za: {stats['memory_stats']['total_memories']} kayÄ±t\n")
      
      # Demo konuÅŸma
      conversations = [
          ("Merhaba, benim adÄ±m Kaan.", "Merhaba Kaan! Size nasÄ±l yardÄ±mcÄ± olabilirim?"),
          ("EVO-TR projesi Ã¼zerinde Ã§alÄ±ÅŸÄ±yorum.", "EVO-TR projesi hakkÄ±nda not aldÄ±m. Proje ile ilgili nasÄ±l yardÄ±mcÄ± olabilirim?"),
          ("Python'da LoRA eÄŸitimi yapacaÄŸÄ±m.", "LoRA eÄŸitimi iÃ§in size yardÄ±mcÄ± olabilirim. Hangi model Ã¼zerinde Ã§alÄ±ÅŸÄ±yorsunuz?"),
      ]
      
      console.print("[yellow]ğŸ“ Demo konuÅŸmalar ekleniyor...[/yellow]\n")
      
      for user_msg, assistant_msg in conversations:
          console.print(f"[cyan]KullanÄ±cÄ±:[/cyan] {user_msg}")
          console.print(f"[green]Asistan:[/green] {assistant_msg}\n")
          
          rag.process_response(user_msg, assistant_msg, save_to_memory=True)
      
      # HafÄ±za aramasÄ±
      console.print("\n[yellow]ğŸ” HafÄ±za aramasÄ± yapÄ±lÄ±yor...[/yellow]\n")
      
      test_queries = [
          "KullanÄ±cÄ±nÄ±n adÄ± ne?",
          "Hangi proje Ã¼zerinde Ã§alÄ±ÅŸÄ±yoruz?",
          "Ne yapmak istiyor?"
      ]
      
      for query in test_queries:
          console.print(f"[cyan]Soru:[/cyan] {query}")
          
          results = memory.search(query, top_k=2)
          
          if results:
              for r in results:
                  console.print(f"   â†’ {r['text']} (skor: {r['score']:.2f})")
          else:
              console.print("   â†’ SonuÃ§ bulunamadÄ±")
          
          console.print()
      
      # Augmented prompt Ã¶rneÄŸi
      console.print("\n[yellow]ğŸ“„ Augmented Prompt Ã¶rneÄŸi:[/yellow]\n")
      
      test_question = "Daha Ã¶nce sana sÃ¶ylediÄŸim proje adÄ± neydi?"
      augmented = rag.build_augmented_prompt(test_question)
      
      console.print(Panel(augmented, title="ZenginleÅŸtirilmiÅŸ Prompt", expand=False))
      
      # Final stats
      console.print("\n[yellow]ğŸ“Š Final Ä°statistikler:[/yellow]\n")
      
      final_stats = rag.get_stats()
      
      table = Table(title="HafÄ±za Durumu")
      table.add_column("Metrik", style="cyan")
      table.add_column("DeÄŸer", style="green")
      
      table.add_row("Toplam KayÄ±t", str(final_stats['memory_stats']['total_memories']))
      table.add_row("Buffer Mesaj", str(final_stats['buffer_stats']['message_count']))
      table.add_row("Session ID", final_stats['buffer_stats']['session_id'])
      
      console.print(table)
  
  
  if __name__ == "__main__":
      main()
  ```

#### 4.6.2 Entegrasyon Testi
- [ ] `tests/test_rag_pipeline.py` oluÅŸtur
- [ ] UÃ§tan uca test senaryolarÄ± yaz
- [ ] Latency testleri ekle

---

## âœ… Faz Tamamlanma Kriterleri

1. [ ] ChromaDB persistent storage Ã§alÄ±ÅŸÄ±yor
2. [ ] TÃ¼rkÃ§e embedding model yÃ¼klendi
3. [ ] MemoryHandler tÃ¼m CRUD operasyonlarÄ±nÄ± destekliyor
4. [ ] ContextBuffer son N mesajÄ± tutuyor
5. [ ] RAG Pipeline hafÄ±zadan retrieval yapabiliyor
6. [ ] Augmented prompt oluÅŸturuluyor
7. [ ] Demo script baÅŸarÄ±yla Ã§alÄ±ÅŸÄ±yor
8. [ ] Search latency < 100ms

---

## â­ï¸ Sonraki Faz

Faz 4 tamamlandÄ±ktan sonra â†’ **FAZ-5-ENTEGRASYON.md** dosyasÄ±na geÃ§.

---

## ğŸ› OlasÄ± Sorunlar ve Ã‡Ã¶zÃ¼mleri

### ChromaDB Yazma HatasÄ±
**Ã‡Ã¶zÃ¼m:** Dizin izinlerini kontrol et, disk alanÄ±nÄ± kontrol et

### Embedding Model YavaÅŸ
**Ã‡Ã¶zÃ¼m:** Batch processing kullan, daha kÃ¼Ã§Ã¼k model seÃ§

### Search SonuÃ§larÄ± Ä°lgisiz
**Ã‡Ã¶zÃ¼m:** min_score threshold'u artÄ±r, TÃ¼rkÃ§e-spesifik model kullan

---

## ğŸ“Š Zaman Takibi

| GÃ¶rev | BaÅŸlangÄ±Ã§ | BitiÅŸ | SÃ¼re |
|-------|-----------|-------|------|
| 4.1 ChromaDB Kurulum | | | |
| 4.2 Embedding Model | | | |
| 4.3 Memory Handler | | | |
| 4.4 Context Buffer | | | |
| 4.5 RAG Pipeline | | | |
| 4.6 Test & Demo | | | |
| **TOPLAM** | | | |

---

*Bu faz tamamlandÄ±ÄŸÄ±nda, "âœ… FAZ 4 TAMAMLANDI" olarak iÅŸaretle.*
