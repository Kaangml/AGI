#!/usr/bin/env python3
"""
EVO-TR: TÃ¼rkÃ§e Adapter Testi

Base model ve adapter ile yanÄ±tlarÄ± karÅŸÄ±laÅŸtÄ±rÄ±r.
"""

from mlx_lm import load, generate
from rich.console import Console
from rich.panel import Panel
from rich.table import Table

console = Console()

MODEL_PATH = "./models/base/qwen-2.5-3b-instruct"
ADAPTER_PATH = "./adapters/tr_chat"

TEST_PROMPTS = [
    "Merhaba! NasÄ±lsÄ±n?",
    "TÃ¼rk kahvesi nasÄ±l yapÄ±lÄ±r?",
    "'Damlaya damlaya gÃ¶l olur' ne demek?",
    "AtatÃ¼rk hakkÄ±nda kÄ±sa bilgi ver.",
    "Bana bir TÃ¼rk atasÃ¶zÃ¼ sÃ¶yle ve anlamÄ±nÄ± aÃ§Ä±kla.",
    "Ä°stanbul'un tarihi Ã¶nemi nedir?",
    "TÃ¼rk misafirperverliÄŸi hakkÄ±nda ne sÃ¶ylersin?",
    "CanÄ±m sÄ±kÄ±lÄ±yor, ne yapmalÄ±yÄ±m?",
]


def main():
    console.print("\n[bold blue]ğŸ§ª TÃ¼rkÃ§e Adapter Testi[/bold blue]\n")
    
    # Adapter ile model yÃ¼kle
    console.print("ğŸ“¥ Model yÃ¼kleniyor (adapter ile)...")
    model, tokenizer = load(MODEL_PATH, adapter_path=ADAPTER_PATH)
    console.print("âœ… Model hazÄ±r!\n")
    
    for i, prompt in enumerate(TEST_PROMPTS, 1):
        console.print(Panel(f"[cyan]Soru {i}:[/cyan] {prompt}", expand=False))
        
        # Chat formatÄ±nda prompt oluÅŸtur
        messages = [{"role": "user", "content": prompt}]
        formatted_prompt = tokenizer.apply_chat_template(
            messages, 
            tokenize=False, 
            add_generation_prompt=True
        )
        
        # YanÄ±t Ã¼ret
        response = generate(
            model, 
            tokenizer, 
            prompt=formatted_prompt, 
            max_tokens=200,
            verbose=False
        )
        
        console.print(f"\n[green]YanÄ±t:[/green] {response}\n")
        console.print("â”€" * 60 + "\n")
        
        if i < len(TEST_PROMPTS):
            try:
                input("Enter'a basarak devam et...")
            except EOFError:
                pass
    
    console.print("\nâœ… [bold green]Test tamamlandÄ±![/bold green]")


if __name__ == "__main__":
    main()
