"""
EVO-TR V2: Gemini Data Generator
==================================
Gemini 2.5 Flash ile kaliteli eÄŸitim verisi Ã¼retimi.

KullanÄ±m:
    python scripts/gemini_data_generator.py --domain turkish_chat --count 100
    python scripts/gemini_data_generator.py --domain python_code --count 50
"""

import os
import json
import asyncio
import aiohttp
import argparse
import random
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, asdict
from dotenv import load_dotenv

# .env'den API key yÃ¼kle
load_dotenv()


def get_api_keys() -> list:
    """TÃ¼m mevcut API key'leri dÃ¶ndÃ¼r."""
    keys = []
    primary = os.getenv("GOOGLE_API_KEY")
    secondary = os.getenv("GOOGLE_API_KEY_2")
    
    if primary:
        keys.append(primary)
    if secondary:
        keys.append(secondary)
    
    return keys


class APIKeyRotator:
    """Rate limit'e takÄ±lÄ±nca key deÄŸiÅŸtiren rotator."""
    
    def __init__(self):
        self.keys = get_api_keys()
        self.current_index = 0
        self.rate_limited_until = {}  # key -> datetime
        
    def get_current_key(self) -> str:
        """Aktif key'i dÃ¶ndÃ¼r, rate limited ise diÄŸerine geÃ§."""
        if not self.keys:
            raise ValueError("HiÃ§ API key bulunamadÄ±!")
        
        now = datetime.now()
        
        # TÃ¼m key'leri dene
        for _ in range(len(self.keys)):
            key = self.keys[self.current_index]
            
            # Bu key rate limited mÄ±?
            if key in self.rate_limited_until:
                if now < self.rate_limited_until[key]:
                    # Hala rate limited, sonraki key'e geÃ§
                    self.current_index = (self.current_index + 1) % len(self.keys)
                    continue
                else:
                    # Rate limit sÃ¼resi dolmuÅŸ
                    del self.rate_limited_until[key]
            
            return key
        
        # TÃ¼m key'ler rate limited - en kÄ±sa sÃ¼rede aÃ§Ä±lacak olanÄ± bekle
        wait_times = {k: v for k, v in self.rate_limited_until.items()}
        if wait_times:
            min_wait = min(wait_times.values())
            wait_seconds = (min_wait - now).total_seconds()
            print(f"â³ TÃ¼m API key'ler rate limited. {wait_seconds:.0f} saniye bekleniyor...")
            return None
        
        return self.keys[0]
    
    def mark_rate_limited(self, key: str, wait_seconds: int = 60):
        """Bir key'i rate limited olarak iÅŸaretle."""
        self.rate_limited_until[key] = datetime.now() + timedelta(seconds=wait_seconds)
        print(f"ğŸ”„ API key rate limited, {wait_seconds}s sonra tekrar denenecek")
        self.current_index = (self.current_index + 1) % len(self.keys)


@dataclass
class GenerationConfig:
    """Veri Ã¼retim konfigÃ¼rasyonu."""
    domain: str
    count: int
    output_dir: str = "./data/generated"
    batch_size: int = 1  # SÄ±ralÄ± Ã¼retim iÃ§in 1
    delay_between_batches: float = 4.0  # RPM=30 -> 60/30=2s, +2s buffer = 4s
    max_retries: int = 3
    
    # Token limitleri (Qwen 2.5 3B eÄŸitimi iÃ§in optimize)
    # Hedef: Toplam konuÅŸma ~600-800 token (eÄŸitim iÃ§in ideal)
    # Gemma 3 27B: input=131K, output=8192
    # TPM=15K -> istek baÅŸÄ±na ~1K token gÃ¼venli
    max_input_tokens: int = 400   # Prompt token limiti
    max_output_tokens: int = 600  # Response token limiti
    
    # Gemma 3 27B Rate limits
    rpm_limit: int = 30    # Dakikada max istek
    tpm_limit: int = 15000 # Dakikada max token
    rpd_limit: int = 14400 # GÃ¼nde max istek


class GeminiClient:
    """Gemini/Gemma API async client."""
    
    BASE_URL = "https://generativelanguage.googleapis.com/v1beta/models"
    MODEL = "gemma-3-27b-it"  # Gemma 3 27B Instruct - RPM=30, TPM=15K, RPD=14.4K
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.session: Optional[aiohttp.ClientSession] = None
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, *args):
        if self.session:
            await self.session.close()
    
    async def generate(
        self,
        prompt: str,
        temperature: float = 0.8,
        max_tokens: int = 800  # Output token limiti
    ) -> tuple[Optional[str], dict]:
        """Tek bir prompt iÃ§in yanÄ±t Ã¼ret. Token kullanÄ±mÄ±nÄ± da dÃ¶ndÃ¼r."""
        url = f"{self.BASE_URL}/{self.MODEL}:generateContent?key={self.api_key}"
        
        token_usage = {"input": 0, "output": 0, "total": 0}
        
        payload = {
            "contents": [{"parts": [{"text": prompt}]}],
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_tokens,
            }
        }
        
        try:
            async with self.session.post(url, json=payload) as response:
                if response.status == 200:
                    data = await response.json()
                    
                    # Token kullanÄ±mÄ±nÄ± Ã§Ä±kar
                    usage = data.get("usageMetadata", {})
                    token_usage["input"] = usage.get("promptTokenCount", 0)
                    token_usage["output"] = usage.get("candidatesTokenCount", 0)
                    token_usage["total"] = usage.get("totalTokenCount", 0)
                    
                    # Gemini response parsing
                    if "candidates" in data and data["candidates"]:
                        content = data["candidates"][0].get("content", {})
                        parts = content.get("parts", [])
                        if parts:
                            return parts[0].get("text", ""), token_usage
                elif response.status == 429:
                    # Rate limit - Ã¶zel iÅŸaretle
                    token_usage["rate_limited"] = True
                    print(f"âš ï¸ Rate limit aÅŸÄ±ldÄ±!")
                    return None, token_usage
                else:
                    error_text = await response.text()
                    print(f"âš ï¸ API Error {response.status}: {error_text[:100]}")
                    return None, token_usage
        except Exception as e:
            print(f"âš ï¸ Request error: {e}")
            return None, token_usage
        
        return None, token_usage


class TurkishChatGenerator:
    """TÃ¼rkÃ§e sohbet verisi Ã¼retici."""
    
    # Konu kategorileri
    TOPICS = {
        "selamlama": [
            "Sabah selamlaÅŸmasÄ±",
            "AkÅŸam selamlaÅŸmasÄ±", 
            "Resmi selamlama",
            "Samimi selamlama",
            "VedalaÅŸma",
        ],
        "gunluk_sohbet": [
            "Hava durumu hakkÄ±nda sohbet",
            "Hafta sonu planlarÄ±",
            "Ä°ÅŸ/okul stresi",
            "Yemek Ã¶nerileri",
            "Film/dizi tavsiyeleri",
            "MÃ¼zik sohbeti",
            "Spor haberleri",
            "Tatil planlarÄ±",
        ],
        "turk_kulturu": [
            "TÃ¼rk mutfaÄŸÄ± (yemekler, tarifler)",
            "TÃ¼rk kahvesi ve Ã§ay kÃ¼ltÃ¼rÃ¼",
            "Bayramlar ve kutlamalar",
            "TÃ¼rk mÃ¼ziÄŸi",
            "Tarihi yerler",
            "Gelenekler ve gÃ¶renekler",
            "TÃ¼rk edebiyatÄ±",
        ],
        "duygusal_destek": [
            "Motivasyon ve cesaret verme",
            "Stres yÃ¶netimi tavsiyeleri",
            "Empati gÃ¶sterme",
            "BaÅŸarÄ±sÄ±zlÄ±kla baÅŸa Ã§Ä±kma",
            "Olumlu dÃ¼ÅŸÃ¼nce",
        ],
        "bilgi_soru": [
            "Genel kÃ¼ltÃ¼r sorularÄ±",
            "GÃ¼ncel olaylar hakkÄ±nda",
            "NasÄ±l yapÄ±lÄ±r sorularÄ±",
            "Tavsiye isteme",
            "AÃ§Ä±klama isteme",
        ],
        "atasozleri": [
            "AtasÃ¶zÃ¼ aÃ§Ä±klamasÄ±",
            "Deyim kullanÄ±mÄ±",
            "TÃ¼rkÃ§e dil bilgisi",
        ]
    }
    
    def generate_prompt(self, topic_category: str, specific_topic: str) -> str:
        """Veri Ã¼retim promptu oluÅŸtur - Qwen 2.5 3B eÄŸitimi iÃ§in optimize."""
        return f"""Sen bir TÃ¼rkÃ§e sohbet veri seti oluÅŸturucususun.

HEDEF MODEL: Qwen 2.5 3B (kÃ¼Ã§Ã¼k dil modeli)
TOKEN LÄ°MÄ°TÄ°: Toplam konuÅŸma ~400-600 token olmalÄ±

Konu: {topic_category} - {specific_topic}

Bir kullanÄ±cÄ± ve yardÄ±mcÄ± asistan (EVO-TR) arasÄ±nda doÄŸal bir TÃ¼rkÃ§e sohbet oluÅŸtur.

KURALLAR:
1. YanÄ±tlar KISA ve Ã–Z olmalÄ± (her yanÄ±t max 2-3 cÃ¼mle)
2. DoÄŸal TÃ¼rkÃ§e kullan, Ã§eviri gibi olmasÄ±n
3. Samimi ama profesyonel ton
4. 2 tur yeterli (user-assistant-user-assistant)
5. Asistan "EVO-TR" olarak TÃ¼rkÃ§e konuÅŸan yardÄ±mcÄ± bir AI

Sistem promptu dahil et:
- system: "Sen EVO-TR, TÃ¼rkÃ§e konuÅŸan yardÄ±mcÄ± bir yapay zeka asistanÄ±sÄ±n."

SADECE aÅŸaÄŸÄ±daki JSON formatÄ±nda yanÄ±t ver:

{{
  "messages": [
    {{"role": "system", "content": "Sen EVO-TR, TÃ¼rkÃ§e konuÅŸan yardÄ±mcÄ± bir yapay zeka asistanÄ±sÄ±n. KÄ±sa, net ve yararlÄ± yanÄ±tlar verirsin."}},
    {{"role": "user", "content": "..."}},
    {{"role": "assistant", "content": "..."}},
    {{"role": "user", "content": "..."}},
    {{"role": "assistant", "content": "..."}}
  ]
}}"""

    def get_random_topic(self) -> tuple:
        """Rastgele konu seÃ§."""
        category = random.choice(list(self.TOPICS.keys()))
        topic = random.choice(self.TOPICS[category])
        return category, topic


class PythonCodeGenerator:
    """Python kod verisi Ã¼retici."""
    
    TOPICS = {
        "temel_kavramlar": [
            "DeÄŸiÅŸken tanÄ±mlama ve veri tipleri",
            "Liste, tuple, dictionary kullanÄ±mÄ±",
            "String iÅŸlemleri",
            "KoÅŸullu ifadeler (if/else)",
            "DÃ¶ngÃ¼ler (for, while)",
            "Fonksiyon tanÄ±mlama",
            "Lambda fonksiyonlarÄ±",
        ],
        "orta_seviye": [
            "List comprehension",
            "Dictionary comprehension",
            "File iÅŸlemleri (okuma/yazma)",
            "Exception handling (try/except)",
            "Class ve OOP temelleri",
            "Decorators",
            "Generators",
        ],
        "algoritmalar": [
            "SÄ±ralama algoritmalarÄ±",
            "Arama algoritmalarÄ±",
            "Recursion Ã¶rnekleri",
            "String manipÃ¼lasyonu",
            "Array/liste problemleri",
            "Matematik problemleri",
        ],
        "debugging": [
            "Hata bulma ve dÃ¼zeltme",
            "Kod optimizasyonu",
            "Best practices",
            "Clean code Ã¶nerileri",
        ],
        "pratik_ornekler": [
            "API istekleri (requests)",
            "JSON iÅŸleme",
            "Tarih/saat iÅŸlemleri",
            "Regex kullanÄ±mÄ±",
            "Unit test yazma",
        ]
    }
    
    def generate_prompt(self, topic_category: str, specific_topic: str) -> str:
        """Python kod veri Ã¼retim promptu - Qwen 2.5 3B iÃ§in optimize."""
        return f"""Sen bir Python programlama eÄŸitim verisi oluÅŸturucususun.

HEDEF MODEL: Qwen 2.5 3B (kÃ¼Ã§Ã¼k dil modeli)
TOKEN LÄ°MÄ°TÄ°: Toplam konuÅŸma ~500-700 token olmalÄ±

Konu: {topic_category} - {specific_topic}

KURALLAR:
1. Soru ve yanÄ±t TÃ¼rkÃ§e olmalÄ±
2. Kod KISA ve Ã–Z olmalÄ± (max 15-20 satÄ±r)
3. AÃ§Ä±klama 2-3 cÃ¼mle ile sÄ±nÄ±rlÄ±
4. Ã‡alÄ±ÅŸÄ±r, doÄŸru Python kodu
5. Asistan "EVO-TR" olarak Python uzmanÄ± bir AI

SADECE aÅŸaÄŸÄ±daki JSON formatÄ±nda yanÄ±t ver:

{{
  "messages": [
    {{"role": "system", "content": "Sen EVO-TR, Python programlama konusunda uzman bir yapay zeka asistanÄ±sÄ±n. KÄ±sa, Ã§alÄ±ÅŸÄ±r kod Ã¶rnekleri ve net aÃ§Ä±klamalar verirsin."}},
    {{"role": "user", "content": "..."}},
    {{"role": "assistant", "content": "..."}}
  ]
}}"""

    def get_random_topic(self) -> tuple:
        """Rastgele konu seÃ§."""
        category = random.choice(list(self.TOPICS.keys()))
        topic = random.choice(self.TOPICS[category])
        return category, topic


class DataGenerator:
    """Ana veri Ã¼retici sÄ±nÄ±fÄ±."""
    
    def __init__(self, config: GenerationConfig, api_key_choice: str = None):
        self.config = config
        self.api_key_choice = api_key_choice
        
        # API key seÃ§imi
        keys = get_api_keys()
        if not keys:
            raise ValueError("GOOGLE_API_KEY bulunamadÄ±! .env dosyasÄ±nÄ± kontrol edin.")
        
        if api_key_choice in ["primary", "1"]:
            self.api_key = keys[0] if len(keys) > 0 else None
            print(f"ğŸ”‘ Primary API key kullanÄ±lÄ±yor")
        elif api_key_choice in ["secondary", "2"]:
            self.api_key = keys[1] if len(keys) > 1 else keys[0]
            print(f"ğŸ”‘ Secondary API key kullanÄ±lÄ±yor")
        else:
            # Rotator kullan
            self.api_key = None
            self.key_rotator = APIKeyRotator()
        
        if api_key_choice and not self.api_key:
            raise ValueError(f"SeÃ§ilen API key bulunamadÄ±: {api_key_choice}")
        
        # Domain'e gÃ¶re generator seÃ§
        if config.domain == "turkish_chat":
            self.generator = TurkishChatGenerator()
        elif config.domain == "python_code":
            self.generator = PythonCodeGenerator()
        else:
            raise ValueError(f"Bilinmeyen domain: {config.domain}")
        
        # Output dizini oluÅŸtur
        self.output_dir = Path(config.output_dir) / config.domain
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # Ä°statistikler
        self.stats = {
            "total_requests": 0,
            "successful": 0,
            "failed": 0,
            "start_time": None,
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "total_tokens": 0
        }
    
    async def generate_single(
        self,
        client: GeminiClient,
        index: int
    ) -> Optional[Dict[str, Any]]:
        """Tek bir Ã¶rnek Ã¼ret."""
        category, topic = self.generator.get_random_topic()
        prompt = self.generator.generate_prompt(category, topic)
        
        self.stats["total_requests"] += 1
        
        for attempt in range(self.config.max_retries):
            response, token_usage = await client.generate(prompt)
            
            # Rate limit kontrolÃ¼
            if token_usage.get("rate_limited"):
                self.key_rotator.mark_rate_limited(client.api_key, wait_seconds=60)
                await asyncio.sleep(5)  # KÄ±sa bekle ve tekrar dene
                continue
            
            # Token kullanÄ±mÄ±nÄ± kaydet
            self.stats["total_input_tokens"] += token_usage["input"]
            self.stats["total_output_tokens"] += token_usage["output"]
            self.stats["total_tokens"] += token_usage["total"]
            
            if response:
                # JSON parse et
                try:
                    # JSON bloÄŸunu bul
                    json_start = response.find("{")
                    json_end = response.rfind("}") + 1
                    
                    if json_start >= 0 and json_end > json_start:
                        json_str = response[json_start:json_end]
                        data = json.loads(json_str)
                        
                        # Validasyon
                        if "messages" in data and len(data["messages"]) >= 2:
                            data["_meta"] = {
                                "category": category,
                                "topic": topic,
                                "index": index,
                                "generated_at": datetime.now().isoformat()
                            }
                            self.stats["successful"] += 1
                            return data
                            
                except json.JSONDecodeError as e:
                    print(f"âš ï¸ JSON parse error ({index}): {e}")
            
            # Retry delay
            if attempt < self.config.max_retries - 1:
                await asyncio.sleep(1)
        
        self.stats["failed"] += 1
        return None
    
    async def generate_batch(
        self,
        client: GeminiClient,
        start_index: int,
        batch_size: int
    ) -> List[Dict[str, Any]]:
        """Bir batch veri Ã¼ret."""
        tasks = [
            self.generate_single(client, start_index + i)
            for i in range(batch_size)
        ]
        
        results = await asyncio.gather(*tasks)
        return [r for r in results if r is not None]
    
    async def generate_all(self) -> List[Dict[str, Any]]:
        """TÃ¼m verileri Ã¼ret."""
        self.stats["start_time"] = datetime.now()
        all_data = []
        consecutive_fails = 0
        max_consecutive_fails = 5
        
        # Tek API key mi yoksa rotator mÄ±?
        using_single_key = self.api_key is not None
        
        print(f"\nğŸš€ Veri Ã¼retimi baÅŸlÄ±yor...")
        print(f"   Domain: {self.config.domain}")
        print(f"   Hedef: {self.config.count} Ã¶rnek")
        print(f"   Batch size: {self.config.batch_size}")
        print(f"   Delay: {self.config.delay_between_batches}s")
        if using_single_key:
            print(f"   API Key: Tek key (sabit)")
        else:
            print(f"   API Keys: {len(self.key_rotator.keys)} adet (rotator)")
        print()
        
        generated = 0
        batch_num = 0
        
        while generated < self.config.count:
            # Aktif API key'i al
            if using_single_key:
                api_key = self.api_key
            else:
                api_key = self.key_rotator.get_current_key()
            
            if api_key is None:
                # TÃ¼m key'ler rate limited
                consecutive_fails += 1
                if consecutive_fails >= max_consecutive_fails:
                    print(f"\nâ³ API key rate limited. 5 dakika bekleniyor...")
                    print(f"   Åu anki ilerleme: {len(all_data)}/{self.config.count}")
                    await asyncio.sleep(300)  # 5 dakika bekle
                    consecutive_fails = 0
                else:
                    await asyncio.sleep(60)
                continue
            
            async with GeminiClient(api_key) as client:
                remaining = self.config.count - generated
                batch_size = min(self.config.batch_size, remaining)
                
                print(f"ğŸ“¦ Batch {batch_num + 1}: {generated}/{self.config.count}", end="", flush=True)
                
                batch_data = await self.generate_batch(client, generated, batch_size)
                
                if batch_data:
                    all_data.extend(batch_data)
                    consecutive_fails = 0
                else:
                    consecutive_fails += 1
                
                generated += batch_size
                batch_num += 1
                
                print(f" -> {len(batch_data)} baÅŸarÄ±lÄ±")
                
                # Her 10 batch'te bir kaydet (checkpoint)
                if len(all_data) > 0 and batch_num % 10 == 0:
                    self._save_checkpoint(all_data)
                
                # Rate limiting
                if generated < self.config.count:
                    await asyncio.sleep(self.config.delay_between_batches)
        
        return all_data
    
    def _save_checkpoint(self, data: List[Dict[str, Any]]):
        """Ara kayÄ±t (checkpoint) oluÅŸtur."""
        filepath = self.output_dir / f"{self.config.domain}_checkpoint.jsonl"
        with open(filepath, 'w', encoding='utf-8') as f:
            for item in data:
                clean_item = {"messages": item["messages"]}
                f.write(json.dumps(clean_item, ensure_ascii=False) + "\n")
        print(f"   ğŸ’¾ Checkpoint: {len(data)} Ã¶rnek kaydedildi")
    
    def save_data(self, data: List[Dict[str, Any]]) -> Path:
        """Veriyi kaydet."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{self.config.domain}_{timestamp}.jsonl"
        filepath = self.output_dir / filename
        
        with open(filepath, 'w', encoding='utf-8') as f:
            for item in data:
                # Meta bilgiyi Ã§Ä±kar, sadece messages'Ä± kaydet
                clean_item = {"messages": item["messages"]}
                f.write(json.dumps(clean_item, ensure_ascii=False) + "\n")
        
        return filepath
    
    def print_stats(self, output_path: Path):
        """Ä°statistikleri yazdÄ±r."""
        elapsed = (datetime.now() - self.stats["start_time"]).total_seconds()
        
        print("\n" + "=" * 50)
        print("ğŸ“Š Ãœretim Ä°statistikleri")
        print("=" * 50)
        print(f"   Domain: {self.config.domain}")
        print(f"   Toplam istek: {self.stats['total_requests']}")
        print(f"   BaÅŸarÄ±lÄ±: {self.stats['successful']}")
        print(f"   BaÅŸarÄ±sÄ±z: {self.stats['failed']}")
        print(f"   BaÅŸarÄ± oranÄ±: {self.stats['successful']/max(1,self.stats['total_requests'])*100:.1f}%")
        print(f"   SÃ¼re: {elapsed:.1f} saniye ({elapsed/60:.1f} dakika)")
        print(f"   HÄ±z: {self.stats['successful']/max(1,elapsed)*60:.1f} Ã¶rnek/dakika")
        print()
        print("   ğŸ“ˆ Token KullanÄ±mÄ±:")
        print(f"      Input tokens: {self.stats['total_input_tokens']:,}")
        print(f"      Output tokens: {self.stats['total_output_tokens']:,}")
        print(f"      Toplam tokens: {self.stats['total_tokens']:,}")
        print(f"\n   ğŸ“ Ã‡Ä±ktÄ±: {output_path}")
        print("=" * 50)


async def main():
    parser = argparse.ArgumentParser(description="Gemini ile eÄŸitim verisi Ã¼ret")
    parser.add_argument(
        "--domain",
        type=str,
        choices=["turkish_chat", "python_code"],
        required=True,
        help="Veri domain'i"
    )
    parser.add_argument(
        "--count",
        type=int,
        default=100,
        help="Ãœretilecek Ã¶rnek sayÄ±sÄ±"
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1,
        help="Batch baÅŸÄ±na istek sayÄ±sÄ± (rate limit iÃ§in 1 Ã¶nerilir)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default="./data/generated",
        help="Ã‡Ä±ktÄ± dizini"
    )
    parser.add_argument(
        "--overnight",
        action="store_true",
        help="Gece modu - rate limit'e uygun yavaÅŸ Ã¼retim"
    )
    parser.add_argument(
        "--delay",
        type=float,
        default=4.0,
        help="Ä°stekler arasÄ± bekleme sÃ¼resi (saniye) - Gemma 3 27B iÃ§in 4s Ã¶nerilir"
    )
    parser.add_argument(
        "--api-key",
        type=str,
        choices=["primary", "secondary", "1", "2"],
        default=None,
        help="KullanÄ±lacak API key: primary/1 veya secondary/2"
    )
    
    args = parser.parse_args()
    
    # Overnight mode: Daha gÃ¼venli delay
    delay = args.delay
    if args.overnight:
        delay = max(5.0, args.delay)
        print(f"ğŸŒ™ Gece modu aktif - {delay} saniye delay ile Ã§alÄ±ÅŸÄ±yor")
    
    config = GenerationConfig(
        domain=args.domain,
        count=args.count,
        batch_size=args.batch_size,
        output_dir=args.output_dir,
        delay_between_batches=delay
    )
    
    # Belirli API key seÃ§ildiyse sadece onu kullan
    generator = DataGenerator(config, api_key_choice=args.api_key)
    
    # Veri Ã¼ret
    data = await generator.generate_all()
    
    if data:
        # Kaydet
        output_path = generator.save_data(data)
        
        # Ä°statistikler
        generator.print_stats(output_path)
    else:
        print("âŒ Veri Ã¼retilemedi!")


if __name__ == "__main__":
    asyncio.run(main())
