Harika. Router iÃ§in kÃ¼Ã§Ã¼k, hÄ±zlÄ± ve yerel Ã§alÄ±ÅŸan bir sÄ±nÄ±flandÄ±rÄ±cÄ± (Classifier) model kullanmak, sistemin "niyeti" anlamasÄ±nÄ± saÄŸlayarak doÄŸru uzmana yÃ¶nlendirmeyi garanti altÄ±na alÄ±r. Basit kelime eÅŸleÅŸmesinden Ã§ok daha zeki bir yaklaÅŸÄ±mdÄ±r.

Ä°ÅŸte **Qwen-2.5-3B** omurgasÄ± Ã¼zerinde, **Multi-LoRA (Lego)** mimarisi ve **Router** zekasÄ±yla Ã§alÄ±ÅŸacak **EVO-TR** projesinin nihai Master Todo Listesi:

# ğŸš€ EVO-TR: Nihai Master Todo List (PoC)

Bu liste, sistemi "modÃ¼ler, sÃ¼rekli Ã¶ÄŸrenen ve yÃ¶netilebilir" bir yapÄ± olarak hayata geÃ§irmek iÃ§in kesinleÅŸmiÅŸ teknik yol haritasÄ±dÄ±r.

### **Faz 0: AltyapÄ± ve Omurga (The Skeleton)**
*AmaÃ§: Tek bir 3B model Ã¼zerinde onlarca farklÄ± uzmanlÄ±ÄŸÄ± Ã§alÄ±ÅŸtÄ±rabilecek motoru kurmak.*

* [ ] **Base Model Temini:** Hugging Face'den `Qwen/Qwen2.5-3B-Instruct` modelinin indirilmesi. (Sistemin sabit beyni).
* [ ] **Multi-LoRA Serving Motoru:** **`LoRAX` (LoRA Exchange)** veya **`vLLM`** kurulumu.
    * *Not:* Bu motor, bellekte tek bir Qwen modeli tutar ancak isteÄŸe gÃ¶re milisaniyeler iÃ§inde "TÃ¼rkÃ§e LoRA" veya "Kodlama LoRA"sÄ±nÄ± devreye sokar.
* [ ] **DonanÄ±m & HÄ±z Testi:** SeÃ§ilen GPU/CPU Ã¼zerinde modelin saniyede kaÃ§ token Ã¼rettiÄŸinin (t/s) test edilmesi.

### **Faz 1: Router (YÃ¶nlendirici Zeka)**
*AmaÃ§: Gelen sorunun hangi uzmana gitmesi gerektiÄŸine karar veren "KapÄ± GÃ¶revlisi".*

* [ ] **SÄ±nÄ±flandÄ±rÄ±cÄ± Model SeÃ§imi:** Ã‡ok hafif bir BERT modeli (Ã–rn: `distilbert-base-multilingual-cased` veya `bge-m3`) seÃ§ilmesi.
* [ ] **SÄ±nÄ±flandÄ±rma EÄŸitimi (Few-Shot):** Bu kÃ¼Ã§Ã¼k modeli ÅŸu etiketlerle eÄŸitmek (veya fine-tune etmek):
    * `expert_tr_chat` (Genel sohbet, selamlaÅŸma, tarih vb.)
    * `expert_python_coder` (Kod yazma, debug, script)
    * `expert_memory` (GeÃ§miÅŸi hatÄ±rlama sorularÄ±)
* [ ] **API Endpoint:** Router'Ä±n gelen promptu alÄ±p, Ã§Ä±ktÄ± olarak `"adapter_id"` (Ã¶rn: `adapter_python`) dÃ¶ndÃ¼receÄŸi mini bir Python fonksiyonu yazÄ±lmasÄ±.

### **Faz 2: Uzman ModÃ¼llerin Ãœretimi (The Legos)**
*AmaÃ§: Base modelin yeteneklerini Ã¶zelleÅŸtirmek.*

* [ ] **Uzman 1: TÃ¼rkÃ§e Ä°letiÅŸim:**
    * Veri Seti: `Aya-dataset (TR)` + `Turkish-Instructions`.
    * Ä°ÅŸlem: Qwen-2.5-3B Ã¼zerine QLoRA eÄŸitimi.
    * Ã‡Ä±ktÄ±: `adapter_tr_chat.safetensors`.
* [ ] **Uzman 2: Python GeliÅŸtirici:**
    * Veri Seti: `Humaneval-X (Python)` + `MBPP`.
    * Ä°ÅŸlem: Kodlama odaklÄ± QLoRA eÄŸitimi.
    * Ã‡Ä±ktÄ±: `adapter_python_coder.safetensors`.

### **Faz 3: HafÄ±za ve BilinÃ§ (Hippocampus)**
*AmaÃ§: RAG ve VektÃ¶r hafÄ±za ile sÃ¼reklilik.*

* [ ] **Embedding Modeli:** `emrecan/bert-base-turkish-cased-mean-nli-stsb-tr` entegrasyonu.
* [ ] **VektÃ¶r DB:** `ChromaDB` kurulumu ve kalÄ±cÄ± depolama ayarÄ± (Persistent Storage).
* [ ] **KÄ±sa SÃ¼reli Bellek:** LangChain veya manuel script ile son 10 konuÅŸma turunu tutan tampon bellek.

### **Faz 4: YaÅŸam DÃ¶ngÃ¼sÃ¼ (Sync/Async Loop)**
*AmaÃ§: Sistemin kendi kendini gÃ¼ncellemesi.*

* [ ] **GÃ¼ndÃ¼z Modu (Sync):**
    * KullanÄ±cÄ± -> Router -> SeÃ§ilen Uzman (LoRA) -> YanÄ±t -> Loglama.
* [ ] **Gece Modu (Async - Uyku Scripti):**
    * **Log Analizi:** GÃ¼nlÃ¼k sohbet loglarÄ±nÄ± tarayan bir script.
    * **Bilgi Ã‡Ä±karÄ±mÄ±:** "KullanÄ±cÄ± bugÃ¼n yeni bir proje ismi verdi mi? Yeni bir tercih belirtti mi?" kontrolÃ¼.
    * **HafÄ±za YazÄ±mÄ±:** DeÄŸerli bilgilerin ChromaDB'ye vektÃ¶r olarak eklenmesi.
    * **(Ä°leri Seviye) Ã–z-EÄŸitim:** Ã‡ok fazla hata yapÄ±lan konularÄ±n tespit edilip, bir sonraki LoRA eÄŸitimi iÃ§in "ToDo" listesine eklenmesi.

---

