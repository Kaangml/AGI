# ğŸ§© EVO-TR BileÅŸen DetaylarÄ±

**Bu dÃ¶kÃ¼man versiyondan baÄŸÄ±msÄ±zdÄ±r ve projenin bileÅŸenlerini detaylÄ± aÃ§Ä±klar.**

---

## ğŸ“‹ Ä°Ã§indekiler

1. [Base Model](#1-base-model)
2. [LoRA AdaptÃ¶rler](#2-lora-adaptÃ¶rler)
3. [Router Sistemi](#3-router-sistemi)
4. [Expert ModÃ¼lleri](#4-expert-modÃ¼lleri)
5. [Memory Sistemi](#5-memory-sistemi)
6. [Inference Engine](#6-inference-engine)
7. [Lifecycle Management](#7-lifecycle-management)
8. [Veri Ãœretim Pipeline](#8-veri-Ã¼retim-pipeline)

---

## 1. Base Model

### Qwen 2.5 3B Instruct


**Konum:** `models/base/qwen-2.5-3b-instruct/`

| Ã–zellik | DeÄŸer |
|---------|-------|
| Parametre | 3 Milyar |
| Context | 32K token |
| Format | Chat/Instruct |
| Boyut | ~6GB |

**Neden Qwen 2.5 3B?**
- âœ… Mac Mini M4 iÃ§in optimize boyut
- âœ… TÃ¼rkÃ§e dil desteÄŸi
- âœ… Kod anlama yeteneÄŸi
- âœ… MLX uyumluluÄŸu

**Chat Format:**
```
<|im_start|>system
Sen EVO-TR, yardÄ±mcÄ± bir TÃ¼rkÃ§e AI asistanÄ±sÄ±n.
<|im_end|>
<|im_start|>user
Merhaba, nasÄ±lsÄ±n?
<|im_end|>
<|im_start|>assistant
Merhaba! Ben bir AI olarak duygularÄ±m yok ama size yardÄ±mcÄ± olmaya hazÄ±rÄ±m!
<|im_end|>
```

---

## 2. LoRA AdaptÃ¶rler

### 2.1 AdaptÃ¶r YapÄ±sÄ±

**Konum:** `adapters/`

```
adapters/
â”œâ”€â”€ tr_chat/           # V1 - TÃ¼rkÃ§e sohbet
â”œâ”€â”€ tr_chat_v2/        # V2 - TÃ¼rkÃ§e sohbet (geliÅŸtirilmiÅŸ)
â”œâ”€â”€ python_coder/      # V1 - Python kod
â”œâ”€â”€ python_coder_v2/   # V2 - Python kod (geliÅŸtirilmiÅŸ)
â”œâ”€â”€ math_expert/       # Matematik
â”œâ”€â”€ history_expert/    # Tarih
â””â”€â”€ science_expert/    # Bilim
```

### 2.2 LoRA Parametreleri

| Parametre | AÃ§Ä±klama | Tipik DeÄŸer |
|-----------|----------|-------------|
| `rank` | AdaptÃ¶r boyutu | 8-16 |
| `alpha` | Ã–ÄŸrenme Ã¶lÃ§eÄŸi | 16-32 |
| `layers` | Uygulanacak katman sayÄ±sÄ± | 8-16 |
| `target_modules` | Hedef modÃ¼ller | q_proj, v_proj |

### 2.3 AdaptÃ¶r PerformanslarÄ±

| AdaptÃ¶r | Val Loss | Ä°yileÅŸme |
|---------|----------|----------|
| tr_chat_v2 | 0.257 | %92 |
| python_coder_v2 | TBD | TBD |

### 2.4 EÄŸitim Config Ã–rneÄŸi

```yaml
# configs/lora_tr_config_v2.yaml
model: models/base/qwen-2.5-3b-instruct
data: data/training/gemma_tr_chat
train: true
adapter_path: adapters/tr_chat_v2

batch_size: 4
learning_rate: 1e-5
iters: 500
val_batches: 10
steps_per_eval: 50
steps_per_save: 250

lora_layers: 8
lora_parameters:
  rank: 8
  alpha: 16
  scale: 1.0
  dropout: 0.0

max_seq_length: 1024
grad_checkpoint: true
```

---

## 3. Router Sistemi

### 3.1 Intent SÄ±nÄ±flandÄ±rma

**Dosya:** `src/router/intent_classifier.py`

```python
class IntentClassifier:
    """KullanÄ±cÄ± girdisini intent'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r."""
    
    INTENTS = {
        "tr_chat": ["merhaba", "nasÄ±lsÄ±n", "teÅŸekkÃ¼r", ...],
        "python_code": ["python", "kod", "fonksiyon", "hata", ...],
        "math": ["hesapla", "toplam", "Ã§arp", "denklem", ...],
        "history": ["tarih", "osmanlÄ±", "savaÅŸ", "antik", ...],
        "science": ["bilim", "fizik", "kimya", "biyoloji", ...]
    }
    
    def classify(self, text: str) -> str:
        """Intent belirle."""
        text_lower = text.lower()
        
        # Keyword matching
        for intent, keywords in self.INTENTS.items():
            if any(kw in text_lower for kw in keywords):
                return intent
        
        # Fallback
        return "tr_chat"
```

### 3.2 Expert Router

**Dosya:** `src/router/expert_router.py`

```python
class ExpertRouter:
    """Intent'e gÃ¶re expert seÃ§er."""
    
    def __init__(self):
        self.classifier = IntentClassifier()
        self.intent_to_expert = {
            "tr_chat": "TrChatExpert",
            "python_code": "PythonExpert",
            "math": "MathExpert",
            "history": "HistoryExpert",
            "science": "ScienceExpert"
        }
    
    def route(self, text: str) -> str:
        """Expert adÄ±nÄ± dÃ¶ndÃ¼r."""
        intent = self.classifier.classify(text)
        return self.intent_to_expert.get(intent, "TrChatExpert")
```

### 3.3 Intent Mapping Config

**Dosya:** `configs/intent_mapping.json`

```json
{
  "tr_chat": {
    "adapter": "adapters/tr_chat_v2",
    "expert": "TrChatExpert",
    "priority": 1
  },
  "python_code": {
    "adapter": "adapters/python_coder_v2",
    "expert": "PythonExpert",
    "priority": 2
  },
  ...
}
```

---

## 4. Expert ModÃ¼lleri

### 4.1 Base Expert

**Dosya:** `src/experts/base_expert.py`

```python
from abc import ABC, abstractmethod

class BaseExpert(ABC):
    """TÃ¼m expertler iÃ§in temel sÄ±nÄ±f."""
    
    def __init__(self, adapter_path: str, inference_engine):
        self.adapter_path = adapter_path
        self.inference = inference_engine
        self.system_prompt = self.get_system_prompt()
    
    @abstractmethod
    def get_system_prompt(self) -> str:
        """Expert'e Ã¶zel sistem promptu."""
        pass
    
    def prepare_messages(self, user_input: str, context: str = "") -> list:
        """Chat format iÃ§in mesaj listesi hazÄ±rla."""
        messages = [
            {"role": "system", "content": self.system_prompt}
        ]
        
        if context:
            messages.append({
                "role": "system", 
                "content": f"Ã–nceki baÄŸlam:\n{context}"
            })
        
        messages.append({"role": "user", "content": user_input})
        return messages
    
    async def generate(self, user_input: str, context: str = "") -> str:
        """YanÄ±t Ã¼ret."""
        messages = self.prepare_messages(user_input, context)
        return await self.inference.generate(messages, self.adapter_path)
```

### 4.2 TÃ¼rkÃ§e Sohbet Expert

**Dosya:** `src/experts/tr_chat_expert.py`

```python
class TrChatExpert(BaseExpert):
    """TÃ¼rkÃ§e genel sohbet uzmanÄ±."""
    
    def __init__(self, inference_engine):
        super().__init__(
            adapter_path="adapters/tr_chat_v2",
            inference_engine=inference_engine
        )
    
    def get_system_prompt(self) -> str:
        return """Sen EVO-TR, samimi ve yardÄ±msever bir TÃ¼rkÃ§e AI asistanÄ±sÄ±n.

Ã–zelliklerin:
- DoÄŸal ve akÄ±cÄ± TÃ¼rkÃ§e kullanÄ±rsÄ±n
- TÃ¼rk kÃ¼ltÃ¼rÃ¼nÃ¼ ve geleneklerini bilirsin
- Empati kurar, duygusal destek verirsin
- AtasÃ¶zleri ve deyimleri yerinde kullanÄ±rsÄ±n
- KÄ±sa, Ã¶z ve anlaÅŸÄ±lÄ±r cevaplar verirsin

YanÄ±t verirken:
- Samimi ama saygÄ±lÄ± ol
- GerektiÄŸinde emoji kullan
- Uzun aÃ§Ä±klamalardan kaÃ§Ä±n"""
```

### 4.3 Python Expert

**Dosya:** `src/experts/python_expert.py`

```python
class PythonExpert(BaseExpert):
    """Python programlama uzmanÄ±."""
    
    def __init__(self, inference_engine):
        super().__init__(
            adapter_path="adapters/python_coder_v2",
            inference_engine=inference_engine
        )
    
    def get_system_prompt(self) -> str:
        return """Sen EVO-TR Python UzmanÄ±, deneyimli bir Python geliÅŸtiricisisin.

Ã–zelliklerin:
- Python 3.10+ syntax bilirsin
- Clean code prensiplerini uygularsÄ±n
- Type hints kullanÄ±rsÄ±n
- Docstring yazarsÄ±n
- Error handling yaparsÄ±n

Kod yazarken:
- PEP 8 standardÄ±na uy
- AnlamlÄ± deÄŸiÅŸken isimleri kullan
- KarmaÅŸÄ±k kodlarÄ± yorumla
- Ã–rneklerle aÃ§Ä±kla"""
```

---

## 5. Memory Sistemi

### 5.1 Conversation Memory

**Dosya:** `src/memory/conversation_memory.py`

```python
from collections import deque
from datetime import datetime

class ConversationMemory:
    """KÄ±sa ve uzun sÃ¼reli konuÅŸma belleÄŸi."""
    
    def __init__(self, short_term_limit: int = 10):
        self.short_term = deque(maxlen=short_term_limit)
        self.long_term = ChromaStore()
    
    def add(self, user_input: str, response: str):
        """Yeni konuÅŸma ekle."""
        entry = {
            "user": user_input,
            "assistant": response,
            "timestamp": datetime.now().isoformat()
        }
        
        # KÄ±sa sÃ¼reli belleÄŸe ekle
        self.short_term.append(entry)
        
        # Uzun sÃ¼reli belleÄŸe kaydet
        self.long_term.store(entry)
    
    def get_context(self, query: str, k: int = 3) -> str:
        """Ä°lgili baÄŸlamÄ± getir."""
        # KÄ±sa sÃ¼reli + RAG birleÅŸtir
        short_context = self._format_short_term()
        rag_context = self.long_term.retrieve(query, k)
        
        return f"{short_context}\n\n{rag_context}"
```

### 5.2 ChromaDB Store

**Dosya:** `src/memory/chroma_store.py`

```python
import chromadb
from chromadb.config import Settings

class ChromaStore:
    """ChromaDB vektÃ¶r deposu."""
    
    def __init__(self, persist_dir: str = "data/chromadb"):
        self.client = chromadb.Client(Settings(
            persist_directory=persist_dir,
            anonymized_telemetry=False
        ))
        self.collection = self.client.get_or_create_collection(
            name="conversations",
            metadata={"hnsw:space": "cosine"}
        )
    
    def store(self, entry: dict):
        """KonuÅŸmayÄ± depola."""
        text = f"User: {entry['user']}\nAssistant: {entry['assistant']}"
        
        self.collection.add(
            documents=[text],
            ids=[entry['timestamp']],
            metadatas=[{"timestamp": entry['timestamp']}]
        )
    
    def retrieve(self, query: str, k: int = 3) -> str:
        """Benzer konuÅŸmalarÄ± getir."""
        results = self.collection.query(
            query_texts=[query],
            n_results=k
        )
        
        return "\n---\n".join(results['documents'][0])
```

### 5.3 RAG Retriever

**Dosya:** `src/memory/rag_retriever.py`

```python
class RAGRetriever:
    """Retrieval-Augmented Generation."""
    
    def __init__(self, chroma_store: ChromaStore):
        self.store = chroma_store
    
    def retrieve_and_format(self, query: str, k: int = 3) -> str:
        """BaÄŸlam getir ve formatla."""
        docs = self.store.retrieve(query, k)
        
        if not docs:
            return ""
        
        formatted = "ğŸ“š Ä°lgili Ã¶nceki konuÅŸmalar:\n\n"
        for i, doc in enumerate(docs, 1):
            formatted += f"{i}. {doc}\n\n"
        
        return formatted
```

---

## 6. Inference Engine

### 6.1 MLX Inference

**Dosya:** `src/inference/mlx_inference.py`

```python
import mlx.core as mx
from mlx_lm import load, generate

class MLXInference:
    """MLX tabanlÄ± model Ã§Ä±karÄ±mÄ±."""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = None
        self.tokenizer = None
        self._load_model()
    
    def _load_model(self):
        """Base modeli yÃ¼kle."""
        self.model, self.tokenizer = load(self.model_path)
    
    async def generate(
        self, 
        messages: list, 
        adapter_path: str = None,
        max_tokens: int = 512,
        temperature: float = 0.7
    ) -> str:
        """YanÄ±t Ã¼ret."""
        
        # Prompt hazÄ±rla
        prompt = self.tokenizer.apply_chat_template(
            messages, 
            add_generation_prompt=True,
            tokenize=False
        )
        
        # LoRA adaptÃ¶r varsa fuse et
        model = self.model
        if adapter_path:
            model = self._fuse_adapter(adapter_path)
        
        # Generate
        response = generate(
            model,
            self.tokenizer,
            prompt=prompt,
            max_tokens=max_tokens,
            temp=temperature
        )
        
        return response
    
    def _fuse_adapter(self, adapter_path: str):
        """LoRA adaptÃ¶rÃ¼ modele fuse et."""
        from mlx_lm import load
        
        model, _ = load(
            self.model_path,
            adapter_path=adapter_path
        )
        return model
```

### 6.2 Adapter Manager

**Dosya:** `src/inference/adapter_manager.py`

```python
from pathlib import Path

class AdapterManager:
    """LoRA adaptÃ¶r yÃ¶netimi."""
    
    def __init__(self, adapters_dir: str = "adapters"):
        self.adapters_dir = Path(adapters_dir)
        self.loaded_adapters = {}
    
    def list_adapters(self) -> list:
        """Mevcut adaptÃ¶rleri listele."""
        return [d.name for d in self.adapters_dir.iterdir() if d.is_dir()]
    
    def get_latest(self, adapter_name: str) -> str:
        """En son checkpoint'i getir."""
        adapter_dir = self.adapters_dir / adapter_name
        
        checkpoints = sorted(adapter_dir.glob("*_adapters.safetensors"))
        if checkpoints:
            return str(checkpoints[-1].parent)
        
        return str(adapter_dir)
    
    def validate(self, adapter_path: str) -> bool:
        """AdaptÃ¶r geÃ§erliliÄŸini kontrol et."""
        path = Path(adapter_path)
        
        required_files = [
            "adapter_config.json",
            # checkpoint files
        ]
        
        return path.exists() and any(path.glob("*.safetensors"))
```

---

## 7. Lifecycle Management

### 7.1 Active Learning

**Dosya:** `src/lifecycle/active_learning.py`

```python
class ActiveLearning:
    """Aktif Ã¶ÄŸrenme yÃ¶netimi."""
    
    def __init__(self, data_dir: str = "data/active_learning"):
        self.data_dir = Path(data_dir)
        self.feedback_queue = []
    
    def collect_feedback(self, conversation: dict, rating: int, comment: str = ""):
        """KullanÄ±cÄ± geri bildirimi topla."""
        feedback = {
            "conversation": conversation,
            "rating": rating,  # 1-5
            "comment": comment,
            "timestamp": datetime.now().isoformat()
        }
        self.feedback_queue.append(feedback)
        self._save_feedback(feedback)
    
    def identify_weak_areas(self) -> list:
        """DÃ¼ÅŸÃ¼k puanlÄ± alanlarÄ± belirle."""
        # Rating < 3 olan konuÅŸmalarÄ± analiz et
        weak = []
        for fb in self.feedback_queue:
            if fb["rating"] < 3:
                weak.append(fb)
        return weak
    
    def prepare_training_data(self) -> list:
        """EÄŸitim verisi hazÄ±rla."""
        # YÃ¼ksek puanlÄ± Ã¶rneklerden veri oluÅŸtur
        training_data = []
        for fb in self.feedback_queue:
            if fb["rating"] >= 4:
                training_data.append(fb["conversation"])
        return training_data
```

### 7.2 Incremental Trainer

**Dosya:** `src/lifecycle/incremental_trainer.py`

```python
class IncrementalTrainer:
    """ArtÄ±mlÄ± LoRA eÄŸitimi."""
    
    def __init__(self, base_model_path: str):
        self.base_model_path = base_model_path
    
    async def train(
        self,
        training_data: list,
        adapter_path: str,
        epochs: int = 1,
        learning_rate: float = 1e-5
    ):
        """ArtÄ±mlÄ± eÄŸitim yap."""
        
        # Veriyi hazÄ±rla
        prepared_data = self._prepare_data(training_data)
        
        # EÄŸitim config
        config = {
            "model": self.base_model_path,
            "data": prepared_data,
            "adapter_path": adapter_path,
            "epochs": epochs,
            "learning_rate": learning_rate,
            "batch_size": 2,
            "grad_checkpoint": True
        }
        
        # mlx_lm lora komutu Ã§alÄ±ÅŸtÄ±r
        await self._run_training(config)
    
    def _prepare_data(self, data: list) -> str:
        """Veriyi eÄŸitim formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r."""
        # JSONL formatÄ±nda kaydet
        output_path = "data/incremental/train.jsonl"
        
        with open(output_path, "w") as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        
        return "data/incremental"
```

### 7.3 Preference Learning

**Dosya:** `src/lifecycle/preference_learning.py`

```python
class PreferenceLearning:
    """Tercih tabanlÄ± Ã¶ÄŸrenme (RLHF benzeri)."""
    
    def __init__(self):
        self.preferences = []
    
    def add_preference(self, prompt: str, chosen: str, rejected: str):
        """Tercih Ã§ifti ekle."""
        self.preferences.append({
            "prompt": prompt,
            "chosen": chosen,
            "rejected": rejected
        })
    
    def prepare_dpo_data(self) -> list:
        """DPO formatÄ±nda veri hazÄ±rla."""
        dpo_data = []
        
        for pref in self.preferences:
            dpo_data.append({
                "prompt": pref["prompt"],
                "chosen": pref["chosen"],
                "rejected": pref["rejected"]
            })
        
        return dpo_data
```

---

## 8. Veri Ãœretim Pipeline

### 8.1 Gemini/Gemma Veri Ãœretici

**Dosya:** `scripts/gemini_data_generator.py`

```python
class GeminiClient:
    """Gemma 3 27B API istemcisi."""
    
    def __init__(self, api_keys: list):
        self.key_rotator = APIKeyRotator(api_keys)
        self.model = "gemma-3-27b-it"
    
    async def generate(self, prompt: str) -> str:
        """YanÄ±t Ã¼ret."""
        api_key = self.key_rotator.get_next_key()
        
        # Rate limit kontrolÃ¼
        await self._wait_for_rate_limit()
        
        # API Ã§aÄŸrÄ±sÄ±
        response = await self._call_api(prompt, api_key)
        
        return response
```

### 8.2 Veri HazÄ±rlama

**Dosya:** `scripts/prepare_gemma_data.py`

```python
def prepare_mlx_format(input_path: str, output_path: str):
    """Ham veriyi MLX formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r."""
    
    data = []
    with open(input_path) as f:
        for line in f:
            item = json.loads(line)
            
            # Messages formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
            formatted = {
                "messages": [
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": item["input"]},
                    {"role": "assistant", "content": item["output"]}
                ]
            }
            data.append(formatted)
    
    # Train/valid split
    train, valid = train_test_split(data, test_size=0.1)
    
    # Kaydet
    save_jsonl(f"{output_path}/train.jsonl", train)
    save_jsonl(f"{output_path}/valid.jsonl", valid)
```

---

## ğŸ“š Ä°lgili DÃ¶kÃ¼manlar

- [PROJECT-STRUCTURE.md](./PROJECT-STRUCTURE.md) - Dizin yapÄ±sÄ±
- [ARCHITECTURE.md](./ARCHITECTURE.md) - Sistem mimarisi
- [v2/TODO.md](../v2/TODO.md) - GÃ¼ncel gÃ¶revler
- [v2/MEMORY.md](../v2/MEMORY.md) - GÃ¼ncel durum
