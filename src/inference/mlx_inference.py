"""
EVO-TR: MLX Inference Engine

MLX-LM ile text generation yÃ¶netimi.
"""

from typing import Dict, List, Optional, Any, Generator
from dataclasses import dataclass
import time
from mlx_lm import generate, stream_generate


@dataclass
class GenerationConfig:
    """Generation parametreleri."""
    max_tokens: int = 512


@dataclass
class GenerationResult:
    """Generation sonucu."""
    text: str
    tokens_generated: int
    generation_time: float
    tokens_per_second: float
    prompt_tokens: int


class MLXInference:
    """
    MLX tabanlÄ± inference engine.
    
    Ã–zellikler:
    - Chat format desteÄŸi
    - Configurable generation parametreleri
    - Performance metrikleri
    - System prompt yÃ¶netimi
    """
    
    # Intent'e gÃ¶re system prompt'lar
    SYSTEM_PROMPTS = {
        "general_chat": """Sen EVO-TR, dostÃ§a ve yardÄ±msever bir TÃ¼rkÃ§e asistansÄ±n. 
KullanÄ±cÄ±yla samimi ama saygÄ±lÄ± bir ÅŸekilde iletiÅŸim kur. 
KÄ±sa ve Ã¶z yanÄ±tlar ver.""",
        
        "turkish_culture": """Sen EVO-TR, TÃ¼rk kÃ¼ltÃ¼rÃ¼ ve dili konusunda uzman bir asistansÄ±n.
TÃ¼rkÃ§e deyimler, atasÃ¶zleri, gelenekler ve kÃ¼ltÃ¼rel konularda detaylÄ± bilgi ver.
TÃ¼rkÃ§e'nin inceliklerini aÃ§Ä±kla.""",
        
        "code_python": """Sen EVO-TR, deneyimli bir Python geliÅŸtiricisisin.
Temiz, okunabilir ve iyi belgelenmiÅŸ kod yaz.
KodlarÄ± TÃ¼rkÃ§e aÃ§Ä±klamalarla destekle.
Best practice'leri takip et.""",
        
        "code_debug": """Sen EVO-TR, hata ayÄ±klama uzmanÄ± bir Python geliÅŸtiricisisin.
Kodlardaki hatalarÄ± tespit et ve dÃ¼zelt.
HatanÄ±n nedenini aÃ§Ä±kla ve Ã§Ã¶zÃ¼mÃ¼ gÃ¶ster.
TÃ¼rkÃ§e aÃ§Ä±klamalar kullan.""",
        
        "code_explain": """Sen EVO-TR, kod aÃ§Ä±klama uzmanÄ± bir Python geliÅŸtiricisisin.
KodlarÄ± satÄ±r satÄ±r veya blok blok aÃ§Ä±kla.
KarmaÅŸÄ±k kavramlarÄ± basit TÃ¼rkÃ§e ile anlat.
Ã–rneklerle destekle.""",
        
        "code_math": """Sen EVO-TR, matematik konusunda uzman bir asistansÄ±n.
Matematik problemlerini adÄ±m adÄ±m Ã§Ã¶z.
Her adÄ±mÄ± TÃ¼rkÃ§e aÃ§Ä±kla.
FormÃ¼lleri ve hesaplamalarÄ± gÃ¶ster.
Cebir, geometri, istatistik ve sÃ¶zel problemlerde yardÄ±mcÄ± ol.""",
        
        "science": """Sen EVO-TR, fizik, kimya ve biyoloji konularÄ±nda uzman bir bilim asistanÄ±sÄ±n.
Bilimsel kavramlarÄ± aÃ§Ä±k ve anlaÅŸÄ±lÄ±r ÅŸekilde aÃ§Ä±kla.
Ã–rnekler ve benzetmeler kullan.
FormÃ¼lleri ve denklemleri gÃ¶ster.
Hem TÃ¼rkÃ§e hem Ä°ngilizce bilim terimlerini kullan.""",
        
        "history": """Sen EVO-TR, TÃ¼rk tarihi ve dÃ¼nya tarihi konusunda uzman bir tarihÃ§isin.
Tarihi olaylarÄ±, dÃ¶nemleri ve Ã¶nemli kiÅŸileri detaylÄ± ÅŸekilde anlat.
Kronolojik sÄ±ralama yap.
Neden-sonuÃ§ iliÅŸkilerini aÃ§Ä±kla.
OsmanlÄ±, Cumhuriyet, SelÃ§uklu ve TÃ¼rk tarihi konularÄ±nda Ã¶zellikle bilgilisin.
AtatÃ¼rk, KurtuluÅŸ SavaÅŸÄ± ve TÃ¼rk devrimleri hakkÄ±nda kapsamlÄ± bilgi ver.""",
        
        "memory_recall": """Sen EVO-TR, iyi bir hafÄ±zaya sahip bir asistansÄ±n.
KullanÄ±cÄ± hakkÄ±nda Ã¶ÄŸrendiÄŸin bilgileri hatÄ±rla ve kullan.
GeÃ§miÅŸ konuÅŸmalara referans ver.""",
        
        "general_knowledge": """Sen EVO-TR, geniÅŸ bilgi birikimine sahip bir asistansÄ±n.
Genel kÃ¼ltÃ¼r sorularÄ±na doÄŸru ve gÃ¼ncel yanÄ±tlar ver.
BilmediÄŸin konularda dÃ¼rÃ¼st ol."""
    }
    
    def __init__(
        self,
        default_config: Optional[GenerationConfig] = None
    ):
        """
        MLXInference baÅŸlat.
        
        Args:
            default_config: VarsayÄ±lan generation config
        """
        self.default_config = default_config or GenerationConfig()
        self._generation_count = 0
        self._total_tokens = 0
        self._total_time = 0.0
        
        print("âœ… MLXInference hazÄ±r")
    
    def get_system_prompt(self, intent: str) -> str:
        """Intent iÃ§in system prompt dÃ¶ndÃ¼r."""
        return self.SYSTEM_PROMPTS.get(intent, self.SYSTEM_PROMPTS["general_chat"])
    
    def build_chat_prompt(
        self,
        tokenizer: Any,
        user_message: str,
        intent: str = "general_chat",
        chat_history: Optional[List[Dict[str, str]]] = None,
        context: Optional[str] = None,
        custom_system_prompt: Optional[str] = None
    ) -> str:
        """
        Chat formatÄ±nda prompt oluÅŸtur.
        
        Args:
            tokenizer: Tokenizer
            user_message: KullanÄ±cÄ± mesajÄ±
            intent: Intent kategorisi
            chat_history: Ã–nceki mesajlar [{"role": "user/assistant", "content": "..."}]
            context: RAG context (ek bilgi)
            custom_system_prompt: Ã–zel system prompt
        
        Returns:
            FormatlanmÄ±ÅŸ prompt string
        """
        # System prompt
        system_prompt = custom_system_prompt or self.get_system_prompt(intent)
        
        # Context varsa system prompt'a ekle
        if context:
            system_prompt += f"\n\nğŸ“š Ä°lgili Bilgiler:\n{context}"
        
        # MesajlarÄ± hazÄ±rla
        messages = [{"role": "system", "content": system_prompt}]
        
        # Chat history ekle
        if chat_history:
            messages.extend(chat_history)
        
        # User mesajÄ±nÄ± ekle
        messages.append({"role": "user", "content": user_message})
        
        # Tokenizer ile format
        prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        return prompt
    
    def generate(
        self,
        model: Any,
        tokenizer: Any,
        prompt: str,
        config: Optional[GenerationConfig] = None
    ) -> GenerationResult:
        """
        Text generation yap.
        
        Args:
            model: MLX model
            tokenizer: Tokenizer
            prompt: Input prompt
            config: Generation config (optional)
        
        Returns:
            GenerationResult
        """
        cfg = config or self.default_config
        
        # Prompt token sayÄ±sÄ± (yaklaÅŸÄ±k)
        prompt_tokens = len(prompt) // 4
        
        start_time = time.time()
        
        # Generate
        response = generate(
            model,
            tokenizer,
            prompt=prompt,
            max_tokens=cfg.max_tokens,
            verbose=False
        )
        
        generation_time = time.time() - start_time
        
        # Token sayÄ±sÄ± (yaklaÅŸÄ±k)
        tokens_generated = len(response) // 4
        tokens_per_second = tokens_generated / generation_time if generation_time > 0 else 0
        
        # Ä°statistikleri gÃ¼ncelle
        self._generation_count += 1
        self._total_tokens += tokens_generated
        self._total_time += generation_time
        
        return GenerationResult(
            text=response,
            tokens_generated=tokens_generated,
            generation_time=generation_time,
            tokens_per_second=tokens_per_second,
            prompt_tokens=prompt_tokens
        )
    
    def generate_response(
        self,
        model: Any,
        tokenizer: Any,
        user_message: str,
        intent: str = "general_chat",
        chat_history: Optional[List[Dict[str, str]]] = None,
        context: Optional[str] = None,
        config: Optional[GenerationConfig] = None
    ) -> GenerationResult:
        """
        Tam yanÄ±t oluÅŸtur (prompt building + generation).
        
        Args:
            model: MLX model
            tokenizer: Tokenizer
            user_message: KullanÄ±cÄ± mesajÄ±
            intent: Intent kategorisi
            chat_history: Ã–nceki mesajlar
            context: RAG context
            config: Generation config
        
        Returns:
            GenerationResult
        """
        # Prompt oluÅŸtur
        prompt = self.build_chat_prompt(
            tokenizer=tokenizer,
            user_message=user_message,
            intent=intent,
            chat_history=chat_history,
            context=context
        )
        
        # Generate
        return self.generate(model, tokenizer, prompt, config)
    
    def generate_stream(
        self,
        model: Any,
        tokenizer: Any,
        prompt: str,
        config: Optional[GenerationConfig] = None
    ) -> Generator[str, None, None]:
        """
        Streaming text generation.
        
        Args:
            model: MLX model
            tokenizer: Tokenizer
            prompt: Input prompt
            config: Generation config (optional)
        
        Yields:
            Token strings one by one
        """
        cfg = config or self.default_config
        
        for response in stream_generate(
            model,
            tokenizer,
            prompt=prompt,
            max_tokens=cfg.max_tokens
        ):
            yield response.text
    
    def generate_response_stream(
        self,
        model: Any,
        tokenizer: Any,
        user_message: str,
        intent: str = "general_chat",
        chat_history: Optional[List[Dict[str, str]]] = None,
        context: Optional[str] = None,
        config: Optional[GenerationConfig] = None
    ) -> Generator[str, None, None]:
        """
        Streaming yanÄ±t oluÅŸtur (prompt building + streaming generation).
        
        Args:
            model: MLX model
            tokenizer: Tokenizer
            user_message: KullanÄ±cÄ± mesajÄ±
            intent: Intent kategorisi
            chat_history: Ã–nceki mesajlar
            context: RAG context
            config: Generation config
        
        Yields:
            Token strings one by one
        """
        # Prompt oluÅŸtur
        prompt = self.build_chat_prompt(
            tokenizer=tokenizer,
            user_message=user_message,
            intent=intent,
            chat_history=chat_history,
            context=context
        )
        
        # Stream generate
        yield from self.generate_stream(model, tokenizer, prompt, config)
    
    def get_stats(self) -> Dict[str, Any]:
        """Inference istatistikleri."""
        avg_tokens_per_sec = self._total_tokens / self._total_time if self._total_time > 0 else 0
        
        return {
            "total_generations": self._generation_count,
            "total_tokens": self._total_tokens,
            "total_time": round(self._total_time, 2),
            "avg_tokens_per_second": round(avg_tokens_per_sec, 1)
        }
    
    def reset_stats(self) -> None:
        """Ä°statistikleri sÄ±fÄ±rla."""
        self._generation_count = 0
        self._total_tokens = 0
        self._total_time = 0.0


# Test
if __name__ == "__main__":
    print("ğŸ§ª MLXInference Testi\n")
    
    from mlx_lm import load
    
    inference = MLXInference()
    
    # Model yÃ¼kle
    print("ğŸ“¥ Model yÃ¼kleniyor...")
    model, tokenizer = load("./models/base/qwen-2.5-3b-instruct")
    print("âœ… Model hazÄ±r!\n")
    
    # Test 1: Genel sohbet
    print("=" * 50)
    print("Test 1: Genel Sohbet")
    print("=" * 50)
    
    result = inference.generate_response(
        model=model,
        tokenizer=tokenizer,
        user_message="Merhaba! BugÃ¼n nasÄ±lsÄ±n?",
        intent="general_chat"
    )
    
    print(f"YanÄ±t: {result.text}")
    print(f"Tokens: {result.tokens_generated}, Time: {result.generation_time:.2f}s")
    print(f"Speed: {result.tokens_per_second:.1f} tok/s")
    
    # Test 2: Kod yazma
    print("\n" + "=" * 50)
    print("Test 2: Python Kodu")
    print("=" * 50)
    
    result = inference.generate_response(
        model=model,
        tokenizer=tokenizer,
        user_message="Fibonacci dizisinin ilk 10 elemanÄ±nÄ± yazdÄ±ran Python kodu yaz.",
        intent="code_python"
    )
    
    print(f"YanÄ±t: {result.text}")
    print(f"Tokens: {result.tokens_generated}, Time: {result.generation_time:.2f}s")
    
    # Test 3: Context ile
    print("\n" + "=" * 50)
    print("Test 3: RAG Context ile")
    print("=" * 50)
    
    context = "KullanÄ±cÄ±nÄ±n adÄ± Kaan. Python'u Ã§ok sever."
    
    result = inference.generate_response(
        model=model,
        tokenizer=tokenizer,
        user_message="Benim hakkÄ±mda ne biliyorsun?",
        intent="memory_recall",
        context=context
    )
    
    print(f"YanÄ±t: {result.text}")
    
    # Ä°statistikler
    print("\n" + "=" * 50)
    print("ğŸ“Š Inference Ä°statistikleri")
    print("=" * 50)
    stats = inference.get_stats()
    for k, v in stats.items():
        print(f"  {k}: {v}")
    
    print("\nâœ… Test tamamlandÄ±!")
