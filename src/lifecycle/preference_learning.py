"""
EVO-TR: Preference Learning System

Direct Preference Optimization (DPO) yaklaÅŸÄ±mÄ± ile tercih Ã¶ÄŸrenimi.
KullanÄ±cÄ± tercihlerinden model davranÄ±ÅŸÄ±nÄ± iyileÅŸtirir.
"""

import json
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, asdict
from enum import Enum


class PreferenceSource(Enum):
    """Tercih kaynaÄŸÄ±."""
    USER_FEEDBACK = "user_feedback"      # KullanÄ±cÄ± ğŸ‘/ğŸ‘
    USER_EDIT = "user_edit"               # KullanÄ±cÄ± dÃ¼zeltmesi
    A_B_TEST = "ab_test"                  # A/B test sonucu
    HUMAN_ANNOTATION = "human_annotation" # Ä°nsan etiketlemesi
    AUTOMATED = "automated"               # Otomatik deÄŸerlendirme


@dataclass
class PreferencePair:
    """
    Tercih Ã§ifti - DPO eÄŸitimi iÃ§in.
    
    chosen: Tercih edilen yanÄ±t
    rejected: Tercih edilmeyen yanÄ±t
    """
    id: str
    prompt: str
    chosen: str           # Tercih edilen yanÄ±t
    rejected: str         # Tercih edilmeyen yanÄ±t
    source: str           # PreferenceSource value
    margin: float         # Tercih marjÄ± (ne kadar daha iyi)
    adapter: str          # Hangi adapter iÃ§in
    timestamp: str
    metadata: Optional[Dict[str, Any]] = None
    
    def to_dpo_format(self) -> Dict[str, Any]:
        """DPO eÄŸitim formatÄ±na Ã§evir."""
        return {
            "prompt": self.prompt,
            "chosen": self.chosen,
            "rejected": self.rejected
        }


class PreferenceCollector:
    """
    Tercih verisi toplama.
    
    Feedback'lerden ve dÃ¼zeltmelerden tercih Ã§iftleri oluÅŸturur.
    """
    
    def __init__(self, storage_path: str = "./data/preferences"):
        self.storage_path = Path(storage_path)
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        self.preferences: List[PreferencePair] = []
        
        print(f"âœ… PreferenceCollector hazÄ±r | Storage: {self.storage_path}")
    
    def create_from_feedback(
        self,
        prompt: str,
        response: str,
        feedback_type: str,
        adapter: str,
        corrected_response: Optional[str] = None
    ) -> Optional[PreferencePair]:
        """
        Feedback'den tercih Ã§ifti oluÅŸtur.
        
        thumbs_up: response = chosen (rejected yok, atlayabiliriz veya base ile karÅŸÄ±laÅŸtÄ±r)
        thumbs_down: response = rejected (chosen = corrected_response veya atlayabiliriz)
        edit: original = rejected, corrected = chosen
        """
        if feedback_type == "edit" and corrected_response:
            # En deÄŸerli: KullanÄ±cÄ± dÃ¼zeltmesi var
            pair = PreferencePair(
                id=self._generate_id(),
                prompt=prompt,
                chosen=corrected_response,
                rejected=response,
                source=PreferenceSource.USER_EDIT.value,
                margin=1.0,  # KullanÄ±cÄ± dÃ¼zeltmesi en gÃ¼Ã§lÃ¼ sinyal
                adapter=adapter,
                timestamp=datetime.now().isoformat(),
                metadata={"original_feedback_type": feedback_type}
            )
            self.preferences.append(pair)
            return pair
        
        elif feedback_type == "thumbs_down":
            # Negatif feedback - rejected yanÄ±tÄ± var ama chosen yok
            # Bu durumda eÄŸer dÃ¼zeltme yoksa sadece kaydet, eÄŸitim iÃ§in kullanamayÄ±z
            # Ama gelecekte karÅŸÄ±laÅŸtÄ±rma iÃ§in kullanÄ±labilir
            if corrected_response:
                pair = PreferencePair(
                    id=self._generate_id(),
                    prompt=prompt,
                    chosen=corrected_response,
                    rejected=response,
                    source=PreferenceSource.USER_FEEDBACK.value,
                    margin=0.8,
                    adapter=adapter,
                    timestamp=datetime.now().isoformat()
                )
                self.preferences.append(pair)
                return pair
        
        # thumbs_up durumunda sadece pozitif Ã¶rnek var, DPO iÃ§in yeterli deÄŸil
        return None
    
    def create_from_ab_test(
        self,
        prompt: str,
        response_a: str,
        response_b: str,
        preferred: str,  # "a" veya "b"
        adapter: str
    ) -> PreferencePair:
        """A/B test sonucundan tercih Ã§ifti oluÅŸtur."""
        if preferred == "a":
            chosen, rejected = response_a, response_b
        else:
            chosen, rejected = response_b, response_a
        
        pair = PreferencePair(
            id=self._generate_id(),
            prompt=prompt,
            chosen=chosen,
            rejected=rejected,
            source=PreferenceSource.A_B_TEST.value,
            margin=0.7,
            adapter=adapter,
            timestamp=datetime.now().isoformat()
        )
        self.preferences.append(pair)
        return pair
    
    def create_from_multiple_responses(
        self,
        prompt: str,
        responses: List[str],
        scores: List[float],
        adapter: str
    ) -> List[PreferencePair]:
        """
        Birden fazla yanÄ±t ve skorlarÄ±ndan tercih Ã§iftleri oluÅŸtur.
        En iyi vs geri kalan hepsi iÃ§in Ã§iftler.
        """
        if len(responses) < 2:
            return []
        
        # Score'a gÃ¶re sÄ±rala
        sorted_pairs = sorted(zip(responses, scores), key=lambda x: x[1], reverse=True)
        pairs = []
        
        best_response, best_score = sorted_pairs[0]
        
        for resp, score in sorted_pairs[1:]:
            margin = (best_score - score) / best_score if best_score > 0 else 0.5
            
            pair = PreferencePair(
                id=self._generate_id(),
                prompt=prompt,
                chosen=best_response,
                rejected=resp,
                source=PreferenceSource.AUTOMATED.value,
                margin=margin,
                adapter=adapter,
                timestamp=datetime.now().isoformat()
            )
            self.preferences.append(pair)
            pairs.append(pair)
        
        return pairs
    
    def _generate_id(self) -> str:
        """Unique ID oluÅŸtur."""
        return f"pref_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(self.preferences):04d}"
    
    def get_preferences_by_adapter(self, adapter: str) -> List[PreferencePair]:
        """Adapter'a gÃ¶re tercih Ã§iftlerini al."""
        return [p for p in self.preferences if p.adapter == adapter]
    
    def export_for_dpo(
        self,
        adapter: Optional[str] = None,
        min_pairs: int = 10
    ) -> Optional[Path]:
        """DPO eÄŸitimi iÃ§in dÄ±ÅŸa aktar."""
        pairs = self.preferences if adapter is None else self.get_preferences_by_adapter(adapter)
        
        if len(pairs) < min_pairs:
            print(f"âš ï¸ Yeterli tercih Ã§ifti yok: {len(pairs)}/{min_pairs}")
            return None
        
        filename = f"dpo_{adapter or 'all'}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
        filepath = self.storage_path / filename
        
        with open(filepath, "w", encoding="utf-8") as f:
            for pair in pairs:
                f.write(json.dumps(pair.to_dpo_format(), ensure_ascii=False) + "\n")
        
        print(f"âœ… DPO verisi dÄ±ÅŸa aktarÄ±ldÄ±: {len(pairs)} Ã§ift -> {filepath}")
        return filepath
    
    def get_statistics(self) -> Dict[str, Any]:
        """Ä°statistikler."""
        total = len(self.preferences)
        if total == 0:
            return {"total": 0, "message": "HenÃ¼z tercih verisi yok"}
        
        # Kaynak daÄŸÄ±lÄ±mÄ±
        by_source = {}
        for p in self.preferences:
            by_source[p.source] = by_source.get(p.source, 0) + 1
        
        # Adapter daÄŸÄ±lÄ±mÄ±
        by_adapter = {}
        for p in self.preferences:
            by_adapter[p.adapter] = by_adapter.get(p.adapter, 0) + 1
        
        # Ortalama marj
        avg_margin = sum(p.margin for p in self.preferences) / total
        
        return {
            "total": total,
            "by_source": by_source,
            "by_adapter": by_adapter,
            "avg_margin": round(avg_margin, 3),
            "ready_for_dpo": total >= 10
        }


class DPOTrainer:
    """
    Direct Preference Optimization Trainer.
    
    NOT: GerÃ§ek DPO eÄŸitimi iÃ§in Ã¶zel bir kÃ¼tÃ¼phane gerekir.
    Bu sÄ±nÄ±f eÄŸitim sÃ¼recini yÃ¶netir ve MLX ile entegre olur.
    """
    
    def __init__(
        self,
        base_model_path: str = "./models/base/qwen-2.5-3b-instruct",
        adapters_dir: str = "./adapters",
        output_dir: str = "./adapters/dpo_trained"
    ):
        self.base_model_path = Path(base_model_path)
        self.adapters_dir = Path(adapters_dir)
        self.output_dir = Path(output_dir)
        
        self.training_history: List[Dict[str, Any]] = []
        
        print(f"âœ… DPOTrainer hazÄ±r")
        print(f"   Base model: {self.base_model_path}")
        print(f"   Output: {self.output_dir}")
    
    def prepare_dpo_config(
        self,
        adapter_name: str,
        training_data_path: Path,
        beta: float = 0.1,
        learning_rate: float = 5e-6,
        epochs: int = 1
    ) -> Dict[str, Any]:
        """DPO eÄŸitim konfigÃ¼rasyonu hazÄ±rla."""
        config = {
            "model": str(self.base_model_path),
            "adapter_path": str(self.adapters_dir / adapter_name),
            "data": str(training_data_path),
            "method": "dpo",
            "beta": beta,  # DPO beta parametresi (tercih gÃ¼cÃ¼)
            "learning_rate": learning_rate,
            "epochs": epochs,
            "lora_layers": 8,
            "batch_size": 1,
            "output_dir": str(self.output_dir / adapter_name)
        }
        return config
    
    def estimate_training_time(
        self,
        num_pairs: int,
        epochs: int = 1
    ) -> Dict[str, Any]:
        """EÄŸitim sÃ¼resini tahmin et."""
        # Tahmini: ~2 saniye/Ã¶rnek (Apple Silicon M4 iÃ§in)
        seconds_per_pair = 2
        total_seconds = num_pairs * epochs * seconds_per_pair
        
        return {
            "pairs": num_pairs,
            "epochs": epochs,
            "estimated_seconds": total_seconds,
            "estimated_minutes": round(total_seconds / 60, 1),
            "note": "Tahmini sÃ¼re, gerÃ§ek sÃ¼re donanÄ±ma gÃ¶re deÄŸiÅŸebilir"
        }
    
    def run_dpo_training(
        self,
        config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        DPO eÄŸitimi Ã§alÄ±ÅŸtÄ±r.
        
        NOT: GerÃ§ek implementasyonda mlx_lm veya Ã¶zel DPO kÃ¼tÃ¼phanesi kullanÄ±lÄ±r.
        Bu simÃ¼lasyon eÄŸitim sÃ¼recini gÃ¶sterir.
        """
        result = {
            "success": False,
            "adapter": config.get("adapter_path", ""),
            "timestamp": datetime.now().isoformat()
        }
        
        try:
            print(f"ğŸ”„ DPO Training baÅŸlÄ±yor...")
            print(f"   Config: {json.dumps(config, indent=2)}")
            
            # SimÃ¼lasyon: GerÃ§ek eÄŸitim burada yapÄ±lÄ±r
            # mlx_lm ÅŸu an doÄŸrudan DPO desteklemiyor
            # Alternatif: TRL kÃ¼tÃ¼phanesi veya custom implementation
            
            result["success"] = True
            result["metrics"] = {
                "preference_accuracy": 0.85,  # SimÃ¼le
                "loss": 0.3,
                "epochs_completed": config.get("epochs", 1)
            }
            
            self.training_history.append(result)
            print(f"âœ… DPO Training tamamlandÄ± (simÃ¼lasyon)")
            
        except Exception as e:
            result["error"] = str(e)
            print(f"âŒ DPO Training baÅŸarÄ±sÄ±z: {e}")
        
        return result
    
    def get_training_history(self) -> List[Dict[str, Any]]:
        """EÄŸitim geÃ§miÅŸi."""
        return self.training_history


class PreferenceLearningPipeline:
    """
    End-to-end Preference Learning Pipeline.
    
    Feedback toplama -> Tercih Ã§ifti oluÅŸturma -> DPO eÄŸitimi
    """
    
    def __init__(
        self,
        min_pairs_for_training: int = 10,
        auto_train: bool = False
    ):
        self.collector = PreferenceCollector()
        self.trainer = DPOTrainer()
        
        self.min_pairs = min_pairs_for_training
        self.auto_train = auto_train
        
        print(f"âœ… PreferenceLearningPipeline hazÄ±r")
        print(f"   Min pairs: {min_pairs_for_training}")
        print(f"   Auto train: {auto_train}")
    
    def process_feedback(
        self,
        prompt: str,
        response: str,
        feedback_type: str,
        adapter: str,
        corrected_response: Optional[str] = None
    ) -> Optional[PreferencePair]:
        """Feedback'i iÅŸle ve tercih Ã§ifti oluÅŸtur."""
        pair = self.collector.create_from_feedback(
            prompt=prompt,
            response=response,
            feedback_type=feedback_type,
            adapter=adapter,
            corrected_response=corrected_response
        )
        
        # Otomatik eÄŸitim kontrolÃ¼
        if self.auto_train and pair:
            self._check_and_trigger_training(adapter)
        
        return pair
    
    def _check_and_trigger_training(self, adapter: str):
        """EÄŸitim tetiklenmeli mi kontrol et."""
        pairs = self.collector.get_preferences_by_adapter(adapter)
        if len(pairs) >= self.min_pairs:
            print(f"ğŸ¯ Otomatik DPO eÄŸitimi tetiklendi: {adapter} ({len(pairs)} Ã§ift)")
            # self.run_training(adapter)  # Aktif deÄŸil
    
    def run_training(self, adapter: str) -> Dict[str, Any]:
        """DPO eÄŸitimi Ã§alÄ±ÅŸtÄ±r."""
        # Veri dÄ±ÅŸa aktar
        data_path = self.collector.export_for_dpo(
            adapter=adapter,
            min_pairs=self.min_pairs
        )
        
        if not data_path:
            return {"success": False, "error": "Yeterli veri yok"}
        
        # Config oluÅŸtur
        config = self.trainer.prepare_dpo_config(
            adapter_name=adapter,
            training_data_path=data_path
        )
        
        # EÄŸitimi Ã§alÄ±ÅŸtÄ±r
        result = self.trainer.run_dpo_training(config)
        return result
    
    def get_pipeline_status(self) -> Dict[str, Any]:
        """Pipeline durumu."""
        collector_stats = self.collector.get_statistics()
        
        return {
            "preference_stats": collector_stats,
            "training_history": len(self.trainer.get_training_history()),
            "auto_train_enabled": self.auto_train,
            "min_pairs_threshold": self.min_pairs
        }


# Test
if __name__ == "__main__":
    print("=" * 50)
    print("Preference Learning Test")
    print("=" * 50)
    
    # Collector test
    collector = PreferenceCollector(storage_path="./data/test_preferences")
    
    # Test 1: Edit feedback'den tercih Ã§ifti
    print("\n1. Edit feedback'den tercih Ã§ifti:")
    pair = collector.create_from_feedback(
        prompt="Python'da liste nasÄ±l oluÅŸturulur?",
        response="list = []",
        feedback_type="edit",
        adapter="python_coder",
        corrected_response="Python'da liste oluÅŸturmak iÃ§in kÃ¶ÅŸeli parantez kullanÄ±lÄ±r:\n\nmy_list = []  # BoÅŸ liste\nmy_list = [1, 2, 3]  # DeÄŸerlerle"
    )
    print(f"   Ã‡ift oluÅŸturuldu: {pair is not None}")
    if pair:
        print(f"   Kaynak: {pair.source}")
        print(f"   Marj: {pair.margin}")
    
    # Test 2: A/B test
    print("\n2. A/B test'den tercih Ã§ifti:")
    pair = collector.create_from_ab_test(
        prompt="DÃ¶ngÃ¼ nasÄ±l yazÄ±lÄ±r?",
        response_a="for i in range(10): print(i)",
        response_b="for dÃ¶ngÃ¼sÃ¼ kullanabilirsin",
        preferred="a",
        adapter="python_coder"
    )
    print(f"   Ã‡ift oluÅŸturuldu: {pair is not None}")
    
    # Test 3: Ã‡oklu yanÄ±t skorlamasÄ±
    print("\n3. Ã‡oklu yanÄ±t skorlamasÄ±ndan tercih Ã§iftleri:")
    pairs = collector.create_from_multiple_responses(
        prompt="Fonksiyon nedir?",
        responses=[
            "Fonksiyon, kod bloÄŸudur.",
            "def my_func(): pass",
            "Fonksiyon, belirli bir gÃ¶revi yerine getiren, tekrar kullanÄ±labilir kod bloÄŸudur. Python'da def anahtar kelimesiyle tanÄ±mlanÄ±r."
        ],
        scores=[0.3, 0.5, 0.95],
        adapter="python_coder"
    )
    print(f"   OluÅŸturulan Ã§ift sayÄ±sÄ±: {len(pairs)}")
    
    # Ä°statistikler
    print("\nğŸ“Š Ä°statistikler:")
    stats = collector.get_statistics()
    print(f"   Toplam: {stats['total']}")
    print(f"   Kaynak daÄŸÄ±lÄ±mÄ±: {stats['by_source']}")
    print(f"   DPO iÃ§in hazÄ±r: {stats['ready_for_dpo']}")
    
    # Pipeline test
    print("\n" + "=" * 50)
    print("Preference Learning Pipeline Test")
    print("=" * 50)
    
    pipeline = PreferenceLearningPipeline(min_pairs_for_training=3)
    
    # Feedback iÅŸle
    pipeline.process_feedback(
        prompt="Test prompt",
        response="KÃ¶tÃ¼ yanÄ±t",
        feedback_type="edit",
        adapter="tr_chat",
        corrected_response="Ä°yi yanÄ±t"
    )
    
    # Durum
    print("\nPipeline durumu:")
    status = pipeline.get_pipeline_status()
    print(f"   Tercih sayÄ±sÄ±: {status['preference_stats']['total']}")
    print(f"   EÄŸitim geÃ§miÅŸi: {status['training_history']}")
    
    # DPO Trainer test
    print("\n" + "=" * 50)
    print("DPO Trainer Test")
    print("=" * 50)
    
    trainer = DPOTrainer()
    
    # SÃ¼re tahmini
    print("\nEÄŸitim sÃ¼resi tahmini:")
    estimate = trainer.estimate_training_time(num_pairs=50, epochs=2)
    print(f"   50 Ã§ift, 2 epoch: ~{estimate['estimated_minutes']} dakika")
    
    print("\nâœ… Test tamamlandÄ±!")
