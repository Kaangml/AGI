# ğŸš€ EVO-TR: Master Todo List (Mac Mini M4 Edition)

**Tarih:** 02 AralÄ±k 2025  
**DonanÄ±m:** Mac Mini M4 (Apple Silicon)  
**Durum:** PoC (Proof of Concept)

---

## ğŸ“‹ Genel BakÄ±ÅŸ

| Faz | Ä°sim | Durum | Tahmini SÃ¼re |
|-----|------|-------|--------------|
| 0 | AltyapÄ± ve Kurulum | âœ… TamamlandÄ± | 1-2 gÃ¼n |
| 1 | Router (YÃ¶nlendirici) | âœ… TamamlandÄ± | 2-3 gÃ¼n |
| 2 | TÃ¼rkÃ§e Uzman (LoRA #1) | âœ… TamamlandÄ± | 3-4 gÃ¼n |
| 3 | Python Uzman (LoRA #2) | âœ… TamamlandÄ± | 2-3 gÃ¼n |
| 4 | HafÄ±za ve RAG | â¬œ BaÅŸlanmadÄ± | 2-3 gÃ¼n |
| 5 | Entegrasyon | â¬œ BaÅŸlanmadÄ± | 2-3 gÃ¼n |
| 6 | YaÅŸam DÃ¶ngÃ¼sÃ¼ | â¬œ BaÅŸlanmadÄ± | 2-3 gÃ¼n |

---

## â¬œ Faz 0: AltyapÄ± ve Kurulum (The Skeleton)

*AmaÃ§: Mac M4 Ã¼zerinde Ã§alÄ±ÅŸacak temel ortamÄ± hazÄ±rlamak*

### 0.1 Sistem Gereksinimleri KontrolÃ¼
- [ ] macOS sÃ¼rÃ¼mÃ¼ kontrolÃ¼ (Sonoma 14+ Ã¶nerilir)
- [ ] Python 3.10+ kurulu mu kontrol et
- [ ] Xcode Command Line Tools kurulu mu kontrol et
- [ ] Homebrew kurulu mu kontrol et

### 0.2 Python OrtamÄ± Kurulumu
- [ ] Proje dizininde virtual environment oluÅŸtur
  ```bash
  python3 -m venv .venv
  source .venv/bin/activate
  ```
- [ ] pip gÃ¼ncelle
- [ ] requirements.txt oluÅŸtur

### 0.3 Temel BaÄŸÄ±mlÄ±lÄ±klar
- [ ] `mlx` kurulumu (Apple ML Framework)
- [ ] `mlx-lm` kurulumu (LLM desteÄŸi)
- [ ] `transformers` kurulumu
- [ ] `huggingface_hub` kurulumu
- [ ] `torch` kurulumu (CPU-only, MLX ile uyumluluk iÃ§in)

### 0.4 Hugging Face AyarlarÄ±
- [ ] `.env` dosyasÄ±nda HF_TOKEN kontrolÃ¼
- [ ] `huggingface-cli login` ile giriÅŸ yap
- [ ] Token'Ä± test et: Model eriÅŸimi var mÄ±?

### 0.5 Base Model Ä°ndirme
- [ ] Qwen-2.5-3B-Instruct MLX formatÄ±nda indir
  ```bash
  mlx_lm.convert --hf-path Qwen/Qwen2.5-3B-Instruct --mlx-path ./models/qwen-2.5-3b-instruct -q
  ```
- [ ] Model boyutunu kontrol et (~2GB olmalÄ±)
- [ ] "Hello World" testi yap

### 0.6 Dizin YapÄ±sÄ± OluÅŸturma
- [ ] `src/` ana kaynak dizini
- [ ] `models/` model dizini
- [ ] `adapters/` LoRA adaptÃ¶r dizini
- [ ] `data/` veri dizini
- [ ] `logs/` log dizini
- [ ] `scripts/` yardÄ±mcÄ± script dizini

---

## â¬œ Faz 1: Router (YÃ¶nlendirici Zeka)

*AmaÃ§: Gelen sorunun hangi uzmana gitmesi gerektiÄŸine karar veren "KapÄ± GÃ¶revlisi"*

### 1.1 Intent Veri Seti HazÄ±rlama
- [ ] Intent kategorilerini belirle:
  - `general_chat` - Genel sohbet, selamlaÅŸma
  - `turkish_culture` - TÃ¼rkÃ§e kÃ¼ltÃ¼r, deyimler
  - `code_python` - Python kodu yazma
  - `code_debug` - Hata ayÄ±klama
  - `memory_recall` - GeÃ§miÅŸi hatÄ±rlama
- [ ] Her kategori iÃ§in 20+ Ã¶rnek yaz
- [ ] `data/intents/intent_dataset.json` oluÅŸtur
- [ ] Veri setini train/val olarak bÃ¶l (80/20)

### 1.2 SÄ±nÄ±flandÄ±rÄ±cÄ± Model SeÃ§imi
- [ ] `distilbert-base-multilingual-cased` indir
- [ ] Alternatif: `sentence-transformers` + cosine similarity
- [ ] Bellek kullanÄ±mÄ± test et (<500MB olmalÄ±)

### 1.3 Router EÄŸitimi
- [ ] `src/router/train_classifier.py` yaz
- [ ] Few-shot learning veya fine-tuning seÃ§
- [ ] EÄŸitimi baÅŸlat
- [ ] Accuracy kontrolÃ¼ (%90+ hedef)
- [ ] Model'i kaydet: `models/router/`

### 1.4 Router API YazÄ±mÄ±
- [ ] `src/router/classifier.py` oluÅŸtur
- [ ] `RouterClassifier` sÄ±nÄ±fÄ± yaz
- [ ] `predict(text) -> {"intent": str, "confidence": float}` metodu
- [ ] Confidence threshold ayarla (0.7 Ã¶nerilen)
- [ ] DÃ¼ÅŸÃ¼k confidence iÃ§in fallback stratejisi

### 1.5 Router Testleri
- [ ] `tests/test_router.py` yaz
- [ ] Edge case'leri test et
- [ ] Latency testi (<50ms hedef)

---

## â¬œ Faz 2: TÃ¼rkÃ§e Uzman (LoRA #1)

*AmaÃ§: Base modelin TÃ¼rkÃ§e iletiÅŸim yeteneklerini geliÅŸtirmek*

### 2.1 Veri Seti HazÄ±rlama
- [ ] `CohereForAI/aya_dataset` TÃ¼rkÃ§e subset'ini indir
- [ ] `Turkish-Instructions` veri setini incele
- [ ] Veri setlerini birleÅŸtir
- [ ] Temizlik yap:
  - Duplice kayÄ±tlarÄ± kaldÄ±r
  - Ã‡ok kÄ±sa/uzun Ã¶rnekleri filtrele
  - Format kontrolÃ¼
- [ ] Alpaca formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r:
  ```json
  {"instruction": "...", "input": "...", "output": "..."}
  ```
- [ ] `data/training/tr_chat_dataset.jsonl` oluÅŸtur

### 2.2 QLoRA EÄŸitim KonfigÃ¼rasyonu
- [ ] `scripts/train_lora_tr.py` oluÅŸtur
- [ ] MLX LoRA parametreleri:
  ```python
  lora_config = {
      "r": 8,                    # LoRA rank
      "lora_alpha": 16,          # Scaling factor
      "lora_dropout": 0.05,
      "target_modules": ["q_proj", "v_proj"]
  }
  ```
- [ ] Training parametreleri:
  ```python
  training_args = {
      "learning_rate": 1e-4,
      "batch_size": 1,           # M4 iÃ§in gÃ¼venli
      "epochs": 3,
      "gradient_accumulation": 4
  }
  ```

### 2.3 EÄŸitim SÃ¼reci
- [ ] EÄŸitimi baÅŸlat
- [ ] Loss deÄŸerlerini logla
- [ ] Checkpoint'ler kaydet (her epoch)
- [ ] EÄŸitim bitince final model kaydet
- [ ] `adapters/adapter_tr_chat/` dizinine taÅŸÄ±

### 2.4 TÃ¼rkÃ§e Adapter Testi
- [ ] Base + Adapter yÃ¼kle
- [ ] Test promptlarÄ± hazÄ±rla:
  - SelamlaÅŸma
  - GÃ¼nlÃ¼k sohbet
  - TÃ¼rk kÃ¼ltÃ¼rÃ¼ sorularÄ±
  - Deyim/atasÃ¶zÃ¼ aÃ§Ä±klamalarÄ±
- [ ] YanÄ±t kalitesini deÄŸerlendir
- [ ] Base model ile karÅŸÄ±laÅŸtÄ±r

---

## âœ… Faz 3: Python Uzman (LoRA #2) - TAMAMLANDI!

*AmaÃ§: Kod yazma ve debugging yeteneklerini geliÅŸtirmek*

### âœ… 3.1 Veri Seti HazÄ±rlama
- [x] `openai/humaneval` indir (164 Ã¶rnek)
- [x] `mbpp` (Mostly Basic Programming Problems) indir (964 Ã¶rnek)
- [x] `sahil2801/CodeAlpaca-20k` indir (9208 Ã¶rnek)
- [x] `iamtarun/code_instructions_120k_alpaca` indir (5000 Ã¶rnek)
- [x] Manuel Ã¶rnekler (58 Ã¶rnek)
- [x] Veriler temizlendi: 15390 â†’ 13334 geÃ§erli Ã¶rnek
- [x] `data/training/python_coder_mlx/train.jsonl` (12,000 Ã¶rnek)
- [x] `data/training/python_coder_mlx/valid.jsonl` (1,334 Ã¶rnek)

### âœ… 3.2 QLoRA EÄŸitim KonfigÃ¼rasyonu
- [x] `configs/lora_python_config.yaml` oluÅŸturuldu
- [x] LoRA: rank=16, alpha=32, num_layers=16
- [x] Training: lr=1e-5, iters=3000, batch=2, max_seq=512

### âœ… 3.3 EÄŸitim SÃ¼reci
- [x] EÄŸitim tamamlandÄ± (3000 iterasyon)
- [x] En iyi val_loss: 0.551 (iter 2800)
- [x] Final val_loss: 0.634
- [x] Peak memory: 6.64 GB
- [x] `adapters/python_coder/adapters.safetensors` kaydedildi

### âœ… 3.4 Python Adapter Testi
- [x] is_prime() - âœ… doÄŸru
- [x] binary_search() - âœ… doÄŸru
- [x] fibonacci() (TÃ¼rkÃ§e prompt) - âœ… doÄŸru
- [x] Bug fix (a-b â†’ a+b) - âœ… doÄŸru
- [ ] Algoritma implementasyonu
- [ ] Bug fixing senaryolarÄ±
- [ ] Kod aÃ§Ä±klama yetenekleri

---

## âœ… Faz 4: HafÄ±za ve RAG Sistemi - TAMAMLANDI!

*AmaÃ§: SÃ¼rekli hatÄ±rlayan bir sistem oluÅŸturmak*

### âœ… 4.1 ChromaDB Kurulumu
- [x] `chromadb` paketini kur (1.3.5)
- [x] Persistent storage ayarla
- [x] `data/chromadb/` dizini oluÅŸtur
- [x] Connection testi yap

### âœ… 4.2 Embedding Model Entegrasyonu
- [x] `paraphrase-multilingual-MiniLM-L12-v2` kullanÄ±ldÄ± (TÃ¼rkÃ§e destekli)
- [x] Embedding boyutu: 384 dimension
- [x] TÃ¼rkÃ§e benzerlik testi: %82 ("Merhaba" vs "Selam")

### âœ… 4.3 ChromaDB Handler
- [x] `src/memory/chromadb_handler.py` oluÅŸtur
- [x] `MemoryHandler` sÄ±nÄ±fÄ±:
  - `add_memory(text, metadata)`
  - `add_conversation(user, assistant, intent)`
  - `search(query, top_k, memory_type)`
  - `get_relevant_context(query)` - RAG iÃ§in
  - `delete(id)`, `clear_all()`
  - `get_stats()`

### âœ… 4.4 KÄ±sa SÃ¼reli HafÄ±za
- [x] `src/memory/context_buffer.py` oluÅŸtur
- [x] Son N mesajÄ± tutan buffer
- [x] Token limiti kontrolÃ¼
- [x] Sliding window mantÄ±ÄŸÄ±
- [x] Chat format export

### âœ… 4.5 Unified Memory Manager
- [x] `src/memory/memory_manager.py` oluÅŸtur
- [x] KÄ±sa + Uzun sÃ¼reli hafÄ±za birleÅŸimi
- [x] Auto-save Ã¶zelliÄŸi
- [x] RAG context oluÅŸturma

### âœ… 4.6 Unit Testler
- [x] `tests/test_memory.py` - **25/25 test geÃ§ti**

### âœ… 4.7 Demo
- [x] `scripts/memory_rag_demo.py` - LLM entegrasyonu demo

---

## â¬œ Faz 5: Sistem Entegrasyonu

*AmaÃ§: TÃ¼m parÃ§alarÄ± birleÅŸtirmek*

### 5.1 LoRA Manager
- [ ] `src/experts/lora_manager.py` oluÅŸtur
- [ ] Adapter yÃ¼kleme/deÄŸiÅŸtirme
- [ ] Adapter caching
- [ ] Hot-swap desteÄŸi

### 5.2 Inference Engine
- [ ] `src/inference/mlx_inference.py` oluÅŸtur
- [ ] MLX-LM ile generation
- [ ] Streaming desteÄŸi
- [ ] Token limiti yÃ¶netimi

### 5.3 Ana Orkestrasyon
- [ ] `src/main.py` veya `src/orchestrator.py` oluÅŸtur
- [ ] Flow:
  ```
  1. User Input
  2. Router -> Intent Classification
  3. LoRA Manager -> Load Adapter
  4. Memory -> Retrieve Context
  5. Inference -> Generate Response
  6. Logger -> Save Conversation
  ```
- [ ] Error handling
- [ ] Graceful degradation

### 5.4 CLI Interface
- [ ] Basit terminal chat interface
- [ ] `/help`, `/clear`, `/switch` komutlarÄ±
- [ ] GÃ¼zel output formatting

### 5.5 Entegrasyon Testleri
- [ ] UÃ§tan uca test senaryolarÄ±
- [ ] TÃ¼rkÃ§e sohbet -> Kod yazma geÃ§iÅŸi
- [ ] HafÄ±za hatÄ±rlama
- [ ] Performance metrikleri

---

## â¬œ Faz 6: YaÅŸam DÃ¶ngÃ¼sÃ¼ (Sync/Async)

*AmaÃ§: Sistemin kendi kendini gÃ¼ncellemesi*

### 6.1 Loglama Sistemi
- [ ] `src/lifecycle/logger.py` oluÅŸtur
- [ ] Structured logging (JSON format)
- [ ] Log rotasyonu
- [ ] Conversation tracking

### 6.2 GÃ¼ndÃ¼z Modu (Sync Handler)
- [ ] `src/lifecycle/sync_handler.py` oluÅŸtur
- [ ] Real-time chat loop
- [ ] AnlÄ±k yanÄ±t Ã¼retimi
- [ ] Session yÃ¶netimi

### 6.3 Gece Modu (Async Processor)
- [ ] `src/lifecycle/async_processor.py` oluÅŸtur
- [ ] Log analiz fonksiyonlarÄ±:
  ```python
  def analyze_daily_logs(date):
      # BaÅŸarÄ±sÄ±z yanÄ±tlarÄ± bul
      # Yeni bilgi Ã§Ä±kar
      # Patterns tespit et
  ```
- [ ] Bilgi Ã§Ä±karÄ±mÄ± (NER, keyword extraction)
- [ ] ChromaDB'ye yeni bilgi yazÄ±mÄ±

### 6.4 Scheduler
- [ ] Gece script'i iÃ§in cron job veya launchd
- [ ] Manuel tetikleme seÃ§eneÄŸi
- [ ] Rapor Ã¼retimi

### 6.5 Self-Improvement Pipeline (Ä°leri Seviye)
- [ ] Hata pattern analizi
- [ ] Yeni eÄŸitim verisi Ã¶nerileri
- [ ] LoRA re-training trigger'larÄ±

---

## ğŸ“ Notlar

### Bellek YÃ¶netimi Ä°puÃ§larÄ± (Mac M4)
- Batch size=1 kullan, memory overflow Ã¶nle
- Gradient checkpointing aktif et
- Model'leri lazy load yap
- KullanÄ±lmayan adapter'larÄ± unload et

### Performans Hedefleri
- Router latency: <50ms
- Inference latency: <500ms (ilk token)
- Memory search: <100ms
- Token/saniye: 30+ (streaming)

### Debugging
- `MPS_FALLBACK_TO_CPU=1` (fallback iÃ§in)
- `PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0` (memory)

---

## ğŸ”— FaydalÄ± Linkler

- [MLX Documentation](https://ml-explore.github.io/mlx/)
- [MLX-LM Examples](https://github.com/ml-explore/mlx-examples)
- [Qwen2.5 Model Card](https://huggingface.co/Qwen/Qwen2.5-3B-Instruct)
- [ChromaDB Docs](https://docs.trychroma.com/)
- [HuggingFace Hub](https://huggingface.co/)

---

*Bu liste projemizin yol haritasÄ±dÄ±r. Her tamamlanan gÃ¶rev iÃ§in â¬œ yerine âœ… koyun.*
